[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Sharing as I learn statistics with R and Quarto."
  },
  {
    "objectID": "posts/discovr_08_the_glm/index.html",
    "href": "posts/discovr_08_the_glm/index.html",
    "title": "discover_08 - General Linear Model",
    "section": "",
    "text": "library(tidyverse, ggplot2)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.0     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nalbum_tib &lt;- here::here(\"data/album_sales.csv\") |&gt; readr::read_csv()\n\nRows: 200 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): album_id\ndbl (4): adverts, sales, airplay, image\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nsoc_anx_tib &lt;- here::here(\"data/social_anxiety.csv\") |&gt; readr::read_csv()\n\nRows: 134 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (4): spai, iii, obq, tosca\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nmetal_tib &lt;- here::here(\"data/metal_health.csv\")  |&gt; readr::read_csv()\n\nRows: 2506 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (2): hm, suicide\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message."
  },
  {
    "objectID": "posts/discovr_08_the_glm/index.html#interpretation",
    "href": "posts/discovr_08_the_glm/index.html#interpretation",
    "title": "discover_08 - General Linear Model",
    "section": "Interpretation",
    "text": "Interpretation\n\n3 predictors have reasonably linear relationships with album sales and no obvious outliers (except bottom left of ‘band image’ scatterplots)\nacross the diagonal (distributions)\n\nadvertising is very skewed\nairplay and sales look heavy-tailed\n\ncorrelations in the plot give us an idea of the relationships between predictors and outcome\nif we ignore album sales, the highest correlation is between image ratings and amount of airplay, which is significant and the 0.01 level (\\(r=0.18\\))\nfocussing on the outcome variable, adverts and airplay correlate best with the outcome (\\(r=0.58\\) and \\(r=0.6\\))"
  },
  {
    "objectID": "posts/discovr_08_the_glm/index.html#one-predictor",
    "href": "posts/discovr_08_the_glm/index.html#one-predictor",
    "title": "discover_08 - General Linear Model",
    "section": "One predictor",
    "text": "One predictor\n\nFitting the model\n\npredicting sales from advertising alone\n\n\\[Y_i=b_0+{b_1}{X_i}+\\epsilon_i\\]\n\\[\\text{Sales}_i=b_0+{b_1}{\\text{Advertising}_i}+\\epsilon_i\\]\n\nit is clear from the bottom left scatterplot and the correlation (\\(r=0.58\\)) that a positive relation exists. More advertising money spent leads to greater album sales.\nsome albums sell well regardless of advertising (top-left of scatterplot)\nno albums sell badly when adverts are high (bottom-right of scatterplot)\nto fit a linear model, we use lm() function my_model &lt;- lm(outcome ~ predictor(s), data = tibble, na.action = an action)\n\nmy_model is the name of the model\noutcome is the name of the outcome variable (sales)\npredictor is the name of the predictor variable (adverts) or, a list of variables separated by + symbols\ntibble is the name of the tibble containing the data (album_tib)\n\nthis function maps directly to the equation for the model\n\nadverts ~ sales maps to \\(\\text{Sales}_i=b_0+{b_1}{\\text{Advertising}_i}+\\epsilon_i\\) except we ignore the error term and parameter estimates and we replace the = with ~ (which means ‘predicted from’)\n\n\n\nalbum_lm &lt;- lm(sales ~ adverts, data = album_tib, na.action = na.exclude)\nsummary(album_lm)\n\n\nCall:\nlm(formula = sales ~ adverts, data = album_tib, na.action = na.exclude)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-152.949  -43.796   -0.393   37.040  211.866 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 1.341e+02  7.537e+00  17.799   &lt;2e-16 ***\nadverts     9.612e-02  9.632e-03   9.979   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 65.99 on 198 degrees of freedom\nMultiple R-squared:  0.3346,    Adjusted R-squared:  0.3313 \nF-statistic: 99.59 on 1 and 198 DF,  p-value: &lt; 2.2e-16\n\nbroom::glance(album_lm)  |&gt; \n  knitr::kable(digits = 3)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nr.squared\nadj.r.squared\nsigma\nstatistic\np.value\ndf\nlogLik\nAIC\nBIC\ndeviance\ndf.residual\nnobs\n\n\n\n\n0.335\n0.331\n65.991\n99.587\n0\n1\n-1120.688\n2247.375\n2257.27\n862264.2\n198\n200\n\n\n\n\n\n\nnote \\(df=1\\) and \\(df.residual=198\\) therefore we can say that adding the predictor of advertising significantly improved the fit of the model to the data compared to having no predictors in the model\n\\(F(1,198)=99.59, p&lt;.001\\)\n\n\n\nModel Parameters\nTo see model parameters, use broom::tidy()\nbroom::tidy(model_name, conf.int = FALSE, conf.level = 0.95)\n\nput the model name into the function, then two optional arguments\n\nconfidence intervals conf.int=TRUE (confidence intervals are not included by default)\ndefault is 95%, but you can change it with conf.level=.99 for 99% confidence interval\n\n\n\nbroom::tidy(album_lm, conf.int = TRUE)\n\n# A tibble: 2 × 7\n  term        estimate std.error statistic  p.value conf.low conf.high\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept) 134.       7.54        17.8  5.97e-43 119.       149.   \n2 adverts       0.0961   0.00963      9.98 2.94e-19   0.0771     0.115\n\n\n\noutput provides estimates of the model parameters (\\(\\hat{b}\\)-values)\n\\(Y\\) intercept (\\(b_0\\)) is 134.14 (when \\(X\\) is 0, sales will be 134140)\n\\(X\\) (\\(b_1\\)) is 0.096.\n\nrepresents the change in outcome associated with a unit change in predictor.\nwhen the predictor increases by 1, the outcome increases by .096, but since the units of measurement was thousands of pounds and thousands of sales, an increase in $1000 will lead to 96 more albums sold\nnot very good return\nBUT, we know that advertising only accounts for 1/3 of the variance\n\nIf a predictor is having a significant impact on our ability to predict the outcome, then \\(\\hat{b}\\) should be different from 0 and large relative to its standard error\nthe \\(t\\)-test (labelled statistic) and the associated \\(p\\)-value tell us whether the \\(\\hat{b}\\) is significantly different from 0\nthe column p.value contains the exact probability that a value of \\(t\\) at least as big as the one in the table would occur if the value of \\(b\\) in the population were 0\nif this propbability is less than 0.05, then we interpret that as the predictor being a significant predictor of hte outcome.\nfor both \\(t\\)s, the probabilities are in scientific notation\n\n2.91e-19 means \\(2.91*10^{-19}\\), or move the decimal 19 places to the left or 0.000000000000000000291\n2.91e+19 means \\(2.91*10^{19}\\), or move the decimal 19 places to the right or 29100000000000000000\n\n\nboth values are 0 at 3 decimal places\n\n\n\nExploring the standard error of \\(\\hat{b}\\)\nhttps://www.youtube-nocookie.com/embed/3L9ZMdzJyyI?si=ET90VDYq3RVKnKDq\n\n\nConfidence intervals for \\(\\hat{b}\\)\nImagine we collect 100 samples of data measuring the same variables as the current model, then estimate the same model, including confidence intervals for unstandardized values.The boundaries are constructed such that 95% of our 100 samples contain the population value of \\(b\\). 95 of 100 sample will yield confidence intervals for \\(b\\) that contain the population value, but we don’t know if our sample is one of the 95.\nWe might just assume that it does, but if the confidence interval contains 0, then there is a possibility that there is no relationship, or the relationship might be negative. The trouble is that we would be wrong 5% of the time.\nIf the interval does not contain 0, we might conclude there is a genuine positive relationship.\n\n\nUsing the model\n\\[\\text{Sales}_i=\\hat{b_0}+\\hat{b_1}{\\text{Advertising}_i}\\]\n\\[\\text{Sales}_i=134.14+.096*{Advertising}_i\\]\nNow we can make a prediction by entering a value for the advertising budget, say 100 (equal to 100,000 gbp)\n\\[\\text{Sales}_i=134.14+.096*{100}_i\\]\n\\[\\text{Sales}_i=143.74\\] or 143,740 sales"
  },
  {
    "objectID": "posts/discovr_08_the_glm/index.html#several-predictors",
    "href": "posts/discovr_08_the_glm/index.html#several-predictors",
    "title": "discover_08 - General Linear Model",
    "section": "Several Predictors",
    "text": "Several Predictors\nadd multiple predictors hierarchically, after advertising is shown to be significant\n\\[Y_i=b_0+{b_1}{X_1i}+{b_2}{X_2i}+{b_3}{X_3i}+ ... +{b_n}{X_ni}\\epsilon_i\\]\n\\[Y_i=b_0+{b_1}{advertising_i}+{b_2}{airplay_i}+{b_3}{image_i}+\\epsilon_i\\]\n\nBuilding the model\n\nadd predictors to the R code the same way as we do in the equation, by adding +\n\n\nalbum_full_lm &lt;- lm(sales ~ adverts + airplay + image, data = album_tib, na.action = na.exclude)\nsummary(album_full_lm)\n\n\nCall:\nlm(formula = sales ~ adverts + airplay + image, data = album_tib, \n    na.action = na.exclude)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-121.324  -28.336   -0.451   28.967  144.132 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -26.612958  17.350001  -1.534    0.127    \nadverts       0.084885   0.006923  12.261  &lt; 2e-16 ***\nairplay       3.367425   0.277771  12.123  &lt; 2e-16 ***\nimage        11.086335   2.437849   4.548 9.49e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 47.09 on 196 degrees of freedom\nMultiple R-squared:  0.6647,    Adjusted R-squared:  0.6595 \nF-statistic: 129.5 on 3 and 196 DF,  p-value: &lt; 2.2e-16\n\nbroom::glance(album_full_lm)  |&gt; \n  knitr::kable(digits = 3)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nr.squared\nadj.r.squared\nsigma\nstatistic\np.value\ndf\nlogLik\nAIC\nBIC\ndeviance\ndf.residual\nnobs\n\n\n\n\n0.665\n0.66\n47.087\n129.498\n0\n3\n-1052.168\n2114.337\n2130.828\n434574.6\n196\n200\n\n\n\n\n\n\nr.squared \\(R^2\\) tells us the variance in album sales -&gt; .665, or 66.5%.\n\\(R^2\\) for adverts was 0.335, so the difference -&gt; .665-.335=.33 means that airplay and image account for a further 33% of the variance\nadjusted \\(R^2\\) adj.r.squared tells us how well the model generalizes, and the value should be close to r.squared.\n\nin this case it is .66, or only .005 away\n\n\\(F\\)-statistic is the ratio of the improvementin prediction that results from fitting the model, relative to the inaccuracy that sitll exists in the model\nthe variable p.value contains the p-value associated with \\(F\\) -&gt; in this case is \\(2.88×10^{−46}\\) -&gt; much smaller than .001\ndegrees of freedom are df and df.residual\nwe can interpret the result as meaning that the model significantly improves our ability to predict the outcome variable ocmpared to not fitting the model.\nreported as \\(F(3, 196)=129.5, p=&lt;0.001\\)"
  },
  {
    "objectID": "posts/discovr_08_the_glm/index.html#comparing-models",
    "href": "posts/discovr_08_the_glm/index.html#comparing-models",
    "title": "discover_08 - General Linear Model",
    "section": "Comparing Models",
    "text": "Comparing Models\n\nwe can compare hierarchical models using an \\(F\\)-statisticusing the anova() function -&gt; anova(model_1, model_2, … , model_n)\nlist the models in order that we want to compare\n\n\nanova(album_lm, album_full_lm) |&gt;\nbroom::tidy()\n\n# A tibble: 2 × 7\n  term                      df.residual    rss    df   sumsq statistic   p.value\n  &lt;chr&gt;                           &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 sales ~ adverts                   198 8.62e5    NA     NA       NA   NA       \n2 sales ~ adverts + airpla…         196 4.35e5     2 427690.      96.4  6.88e-30\n\n\n\n\\(F\\) (statistic) is 96.5\n\\(p\\)-value (p.value) is \\(6.8793949^{-30}\\)\ndf is 2 (difference between df in 2 models)\ndf.residual is 196\nConclusion -&gt; adding airplay and image to the model significantly improved model fit\n\n\\(F(2, 196) = 96.45, p &lt; 0.001\\)\n\n\n\n\n\n\n\n\nTip\n\n\n\nWe can only compare hierarchical models; that is to say that the second model must contain everything that was in the first model plus something new, and the third model must contain everything in the second model plus something new, and so on.\n\n\n\nModel parameter estimates (\\(\\hat{b}\\))\n\nbroom::tidy(album_full_lm, conf.int = TRUE)  |&gt; \n  knitr::kable(digits = 3)\n\n\n\n\n\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n\n(Intercept)\n-26.613\n17.350\n-1.534\n0.127\n-60.830\n7.604\n\n\nadverts\n0.085\n0.007\n12.261\n0.000\n0.071\n0.099\n\n\nairplay\n3.367\n0.278\n12.123\n0.000\n2.820\n3.915\n\n\nimage\n11.086\n2.438\n4.548\n0.000\n6.279\n15.894\n\n\n\n\n\n\noutput gives estimates of the \\(b\\)-balues (column labelled estimate) and statistics that indicate the individual contribution of each predictor in the model.\nall three predictors have positive \\(\\hat{b}\\)values, indicating positive relationships\n\nas all three predictors increase, so do album sales\n\n\\(\\hat{b}\\) vlaues also tell us to what degree each predictor affects the outcome if the effects of all other predictors are held constant\n\nadvertising budget -&gt; \\(\\hat{b}=0.085\\) -&gt; as advertising increases by 1 unit (1000gbp), sales increases by 0.085 units (85 albums), but this is only true if the other two predictors are held constant\nairplay \\(\\hat{b}=3.367\\) -&gt; as airplay prior to release increases by one unit (1 play), album sales increase by 3.367 units (3367 album sales, but only if the other two predictors are held constant)\n\n\n\n\n\n\n\n\nHow would we interpret the 𝑏̂ (11.086) for band image?\n\n\n\n\n\nIf a band can increase their image rating by 1 unit they can expect additional album sales of 11,086 units\n\n\n\n\n\nStandardized \\(\\hat{b}\\)s\nThe lm() function does not provide standardized betas, so use model_parameters()\nparameters::model_parameters(my_model, standardize = refit)\n\nparameters::model_parameters(album_full_lm, standardize = \"refit\") |&gt; \n  knitr::kable(digits = 3)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nParameter\nCoefficient\nSE\nCI\nCI_low\nCI_high\nt\ndf_error\np\n\n\n\n\n(Intercept)\n0.000\n0.041\n0.95\n-0.081\n0.081\n0.000\n196\n1\n\n\nadverts\n0.511\n0.042\n0.95\n0.429\n0.593\n12.261\n196\n0\n\n\nairplay\n0.512\n0.042\n0.95\n0.429\n0.595\n12.123\n196\n0\n\n\nimage\n0.192\n0.042\n0.95\n0.109\n0.275\n4.548\n196\n0\n\n\n\n\n\n\nadvertising budget -&gt; Standardized \\(\\hat{\\beta}=0.511\\)\n\nas the advertising budgetr increases by one standard deviation (485,655gbp), album sales increase by .511 standard deviations.\nthe standard deviation for album sales is 80,699, so .511 SD is 41,240 sales\nfor every increase in advert budget of 485655gbp, album sales will increase by 41,240 IF the other predictors are held constant\n\nImage -&gt; Standardized \\(\\hat{\\beta}=0.192\\)\n\na band rated 1 SD on the image scale (1.4 units) will sell .192 SD more units.\nthis is a change of 15,490 sales IF the other predictors are held constant\n\n\n\n\n\n\n\n\nHow would we interpret the Standardized B (0.512) for airplay?\n\n\n\n\n\nAs the number of plays on radio in the week before release increases by 1 standard deviation, album sales increase by 0.512 standard deviations\n\n\n\n\n\nConfidence Intervals\n\n\n\n\n\n\nThe confidence interval for airplay ranges from 2.82 to 3.92. What does this tell us?\n\n\n\n\n\n\nThe probability of this confidence interval containing the population value is 0.95.\nIf this confidence interval is one of the 95% that contains the population value then the population value of b lies between 2.82 and 3.92.\nI can be 95% confident that the population value of b lies between 2.82 and 3.92\nThere is a 95% chance that the population value of b lies between 2.82 and 3.92\n\n\n\n\nAssuming that each confidence interval is one of the 95% that contains the population parameter: - the true size of the relationship between advertising budget and album sales lies somewhere between 0.071 and 0.099 - the true size of the relationship between band image and album sales lies somewhere between 6.279 and 15.894\n\nthe two best predictors have tight confidence intervals (airplay and adverts) indicating that the estimates are likely representative of the true population values\nthe interval for band image is much wider, but does not cross zero indicating this parameter is less representative of the population, but is still significant.\n\n\n\nSignificance tests\nThe values in statistic are the values of \\(t\\) associated with each \\(\\hat{b}\\) and p.value is the associated significance of the \\(t\\)-statistic. For every predictor, the \\(\\hat{b}\\) is significantly different from 0 (\\(p&lt;.001\\)) meaning that all predictors significantly predict album sales.\n\n\n\n\n\n\nHow might we interpret the statistic and p.value for the three predictors?\n\n\n\n\n\n\nThe probability of the null hypothesis is less than 0.001 in all cases\nThe probability that each b is a chance result is less than 0.001\nThey tell us that the probability of getting a value of t at least as big as these values if the value of b were, in fact, zero is smaller than 0.001 for all predictors.\n\n\n\n\n\n\n\n\n\n\n\\(p\\)-values\n\n\n\nMany students and researchers think of p-values in terms of the ‘probability of a chance result’ or ‘the probability of a hypothesis being true’ but they are neither of these things. They are the long-run probability that you would get a test-statistic (in this case t) at least as large as the one you have if the null hypothesis were true. In other words, if there really were no relationship between advertising budget and album sales (the null hypothesis) then the population value of b would be zero.\nImagine we sampled from this null population and computed t, and then repeated this process 1000 times. We’d have 1000 values of t from a population in which there was no effect. We could plot these values as a histogram. This would tell us how often certain values of t occur. From it we could work out the probability of getting a particular value of t. If we then took another sample, and computed t (because we’re kind of obsessed with this sort of thing) we would be able to compare this value of t to the distribution of all the previous 1000 samples. Is the t in our current sample large of small compared to the others? Let’s say it was larger than 999 of the previous values. That would be quite an unlikely value of t whereas if it was larger than 500 of them this would not surprise us. This is what a p-value is: it is the long run probability of getting test statistic at least as large as the one you have if the null hypothesis were true. If the value is less than 0.05, people typically take this as supporting the idea that the null hypothesis isn’t true.\n\n\n\n\n\n\n\n\nReport\n\n\n\n\nthe model that included the band’s image and airplay was significantly better fit than the model that included advertising budget alone \\(f(2, 196)=96.45, p&lt;0.001\\)\nthe final model explained 66.5% of the variance in album sales\nadvertising budget significantly predicted album sales \\(\\hat{b}=0.08[0.07, 0.10], t(196)=12.26, p&gt;.001\\)\nairplay significantly predicted album sales \\(\\hat{b}=3.37[2.82, 3.92], t(196)=12.12, p&lt;.001\\)\nband image significantly predicted album sales -&gt; \\(\\hat{b}=11.09[6.28, 15.89], t=4.55, p&lt;.001\\)"
  },
  {
    "objectID": "posts/discovr_08_the_glm/index.html#unguided-example",
    "href": "posts/discovr_08_the_glm/index.html#unguided-example",
    "title": "discover_08 - General Linear Model",
    "section": "Unguided Example",
    "text": "Unguided Example\n\nMetal and mental health\n\nmetal_tib\n\n# A tibble: 2,506 × 2\n      hm suicide\n   &lt;dbl&gt;   &lt;dbl&gt;\n 1    10       7\n 2    10      11\n 3    10       8\n 4     4      14\n 5    10      13\n 6     8      12\n 7     8       7\n 8     5      15\n 9     8       7\n10     9      10\n# ℹ 2,496 more rows\n\n\n\nGGally::ggscatmat(metal_tib, columns=c(\"hm\", \"suicide\")) + theme_minimal()\n\nWarning: Removed 200 rows containing non-finite outside the scale range\n(`stat_density()`).\n\n\nWarning: Removed 284 rows containing non-finite outside the scale range\n(`stat_density()`).\n\n\n\n\n\n\n\n\n\n\nmetal_lm &lt;- lm(suicide ~ hm, data = metal_tib, na.action = na.exclude)\nbroom::glance(metal_lm) |&gt;\nknitr::kable(digits=3)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nr.squared\nadj.r.squared\nsigma\nstatistic\np.value\ndf\nlogLik\nAIC\nBIC\ndeviance\ndf.residual\nnobs\n\n\n\n\n0.125\n0.125\n4.842\n304.784\n0\n1\n-6401.851\n12809.7\n12826.7\n50047.05\n2135\n2137\n\n\n\n\n\n\nbroom::tidy(metal_lm, conf.int = TRUE, conf.level=0.95)\n\n# A tibble: 2 × 7\n  term        estimate std.error statistic  p.value conf.low conf.high\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)   16.0      0.261       61.4 0          15.5      16.5  \n2 hm            -0.612    0.0350     -17.5 6.64e-64   -0.680    -0.543\n\n\n\n\n\n\n\n\nHow much variance does the final model explain?\n\n\n\n\n\n12.5%\n\n\n\n\n\n\n\n\n\nWhat is the nature of the relationship between listening to heavy metal and suicide risk?\n\n\n\n\n\n\nAs love of heavy metal increases, suicide risk decreases\nbecause the \\(\\hat{b}\\) value is negative (-.0612)\n\n\n\n\n\n\n\n\n\n\nAs listening to heavy metal increases by 1 unit, by how much does suicide risk change?\n\n\n\n\n\n\n-0.612 units\n\n\n\n\n\n\nPredicting Social Anxiety\n\nsoc_anx_tib\n\n# A tibble: 134 × 4\n    spai   iii   obq tosca\n   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1    26 56.5   4.43  4.18\n 2    51 16.1   2.03  4.20\n 3    33 37.4   2.59  3.25\n 4   106 16.7   4.84  4.07\n 5    25  5.16  1.76  3.80\n 6   109 36.0   2.13  4.25\n 7    39 34.5   2.01  4.95\n 8   134 18.0   1.76  4.52\n 9    43 12.9   2.29  3.59\n10    57  7.10  3.20  3.64\n# ℹ 124 more rows\n\n\n\nGGally::ggscatmat(soc_anx_tib, columns = c(\"spai\", \"tosca\"))  +\ntheme_minimal()\n\n\n\n\n\n\n\n\n\nsoc_anx_lm &lt;- lm(spai ~ tosca, data = soc_anx_tib, na.action = na.exclude)\nbroom::glance(soc_anx_lm)  |&gt; \n  knitr::kable(digits = 3)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nr.squared\nadj.r.squared\nsigma\nstatistic\np.value\ndf\nlogLik\nAIC\nBIC\ndeviance\ndf.residual\nnobs\n\n\n\n\n0.102\n0.095\n29.918\n14.989\n0\n1\n-644.525\n1295.05\n1303.743\n118153.3\n132\n134\n\n\n\n\nbroom::tidy(soc_anx_lm, conf.int = TRUE, conf.level = 0.95)\n\n# A tibble: 2 × 7\n  term        estimate std.error statistic  p.value conf.low conf.high\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)    -54.8     30.0      -1.82 0.0703     -114.       4.60\n2 tosca           27.4      7.08      3.87 0.000169     13.4     41.4 \n\n\n\nsoc_anx_obq_lm &lt;- lm(spai ~ tosca + obq, data = soc_anx_tib, na.action = na.exclude)\nbroom::glance(soc_anx_obq_lm)  |&gt;\n  knitr::kable(digits = 3)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nr.squared\nadj.r.squared\nsigma\nstatistic\np.value\ndf\nlogLik\nAIC\nBIC\ndeviance\ndf.residual\nnobs\n\n\n\n\n0.148\n0.135\n29.015\n11.218\n0\n2\n-630.333\n1268.666\n1280.197\n108599.6\n129\n132\n\n\n\n\nbroom::tidy(soc_anx_obq_lm, conf.int = TRUE, conf.level = 0.95)\n\n# A tibble: 3 × 7\n  term        estimate std.error statistic p.value conf.low conf.high\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)   -53.3      29.2      -1.82 0.0704   -111.        4.50\n2 tosca          22.1       7.24      3.05 0.00276     7.77     36.4 \n3 obq             7.25      2.91      2.49 0.0141      1.49     13.0 \n\n\n\nparameters::model_parameters(soc_anx_obq_lm, standardize = \"refit\") |&gt; \n  knitr::kable(digits = 3)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nParameter\nCoefficient\nSE\nCI\nCI_low\nCI_high\nt\ndf_error\np\n\n\n\n\n(Intercept)\n0.000\n0.081\n0.95\n-0.160\n0.160\n0.000\n129\n1.000\n\n\ntosca\n0.261\n0.086\n0.95\n0.092\n0.430\n3.052\n129\n0.003\n\n\nobq\n0.213\n0.086\n0.95\n0.044\n0.382\n2.489\n129\n0.014\n\n\n\n\n\nanova(soc_anx_lm, soc_anx_obq_lm) |&gt;\nbroom::tidy()\n\n\n\n\n\n\nError message\n\n\n\nError in anova.lmlist(object, …) : models were not all fitted to the same size of dataset\n\nas of this writing (Mar-04-24), there is a solution listed in the discovr_08 tutorial to start by finding the error with soc_anx_tib |&gt; dplyr::summarise( across(.fns = list(valid = ~sum(!is.na(.x)), missing = ~sum(is.na(.x))), .names = \"{.col}_{.fn}\") )\n\nThe output from running this function in the tutorial is a table showing that there are missing values in obq. this means that spai and tosca have 134 values and obq has 132 values, so they are not the same size.\n\nHowever, the output above in my own .qmd file throws a deprecated error in dplyr 1.1.4 that the use of across without .cols is deprecated since dplyr 1.1.1\nthis means that I can’t currently generate the table showing the sizes of the datasets with the code provided\nthe workaround to the problem is normally to use multiple imputation to estimate the missing values, which is beyond the scope of this tutorial, so Field recommends a very bad practice, which is to omit the missing values.\nI’ve filed an issue in the discover.rocks repo\nFix’t"
  },
  {
    "objectID": "posts/variance-std-dev/index.html",
    "href": "posts/variance-std-dev/index.html",
    "title": "Variance and Standard Deviation",
    "section": "",
    "text": "require(UsingR)\n\nLoading required package: UsingR\n\n\nLoading required package: MASS\n\n\nLoading required package: HistData\n\n\nLoading required package: Hmisc\n\n\n\nAttaching package: 'Hmisc'\n\n\nThe following objects are masked from 'package:base':\n\n    format.pval, units\n\nrequire(HistData)\n\n\\(sample variance=s^2=\\frac{1}{n-1}‎‎\\sum_i(x_i - \\bar{x})^2\\)\n\nwts &lt;- c(38, 43, 48, 61, 47, 24, 29, 48, 59, 24, 40, 27)\n\n\nsort(wts)\n\n [1] 24 24 27 29 38 40 43 47 48 48 59 61"
  },
  {
    "objectID": "posts/simple-regression/index.html",
    "href": "posts/simple-regression/index.html",
    "title": "Simple Regression",
    "section": "",
    "text": "Linear regression expresses the relationship between two variables, \\(X_i\\) and \\(Y_i\\).\nThe \\(_i\\) refers to the cases in the dataset, so if there are 1000 cases, then \\(_i\\) is 1-1000. Each \\(_i\\) refers to a ‘student’.\n\\[Y_i=b_0+{b_1}{X_i}+\\epsilon_i\\]\nWhere:\n\n\\(b_1\\) is the slope or regression coefficient (how much \\(Y\\) will change when \\(X\\) increases by 1)\n\n\\(b_0\\) is the \\(Y\\)-intercept, where the regression line crosses the \\(Y\\) axis\n\\(\\epsilon_i\\) is the error term\n\nor\n\\[\\hat{Y}=b_0+{b_1}{X_i}\\]\nWhere:\n\n\\(\\hat{Y}\\) is the predicted value of \\(Y\\)\n\nNotes:\nLarger values of \\(b_1\\) indicate a steeper regression line\n\n\n\nexamples of slope and intercept\n\n\n\nleft image, red is positive slope and green is negative slope (negative correlation; when predictor increases, outcome decreases)\nright image, all slopes positive"
  },
  {
    "objectID": "posts/simple-regression/index.html#regression-equation",
    "href": "posts/simple-regression/index.html#regression-equation",
    "title": "Simple Regression",
    "section": "",
    "text": "Linear regression expresses the relationship between two variables, \\(X_i\\) and \\(Y_i\\).\nThe \\(_i\\) refers to the cases in the dataset, so if there are 1000 cases, then \\(_i\\) is 1-1000. Each \\(_i\\) refers to a ‘student’.\n\\[Y_i=b_0+{b_1}{X_i}+\\epsilon_i\\]\nWhere:\n\n\\(b_1\\) is the slope or regression coefficient (how much \\(Y\\) will change when \\(X\\) increases by 1)\n\n\\(b_0\\) is the \\(Y\\)-intercept, where the regression line crosses the \\(Y\\) axis\n\\(\\epsilon_i\\) is the error term\n\nor\n\\[\\hat{Y}=b_0+{b_1}{X_i}\\]\nWhere:\n\n\\(\\hat{Y}\\) is the predicted value of \\(Y\\)\n\nNotes:\nLarger values of \\(b_1\\) indicate a steeper regression line\n\n\n\nexamples of slope and intercept\n\n\n\nleft image, red is positive slope and green is negative slope (negative correlation; when predictor increases, outcome decreases)\nright image, all slopes positive"
  },
  {
    "objectID": "posts/simple-regression/index.html#calculating-regression",
    "href": "posts/simple-regression/index.html#calculating-regression",
    "title": "Simple Regression",
    "section": "Calculating Regression",
    "text": "Calculating Regression\n\nany dataset is made up of points, and regression finds the best fitting straight line for the dataset"
  },
  {
    "objectID": "posts/simple-regression/index.html#best-fit",
    "href": "posts/simple-regression/index.html#best-fit",
    "title": "Simple Regression",
    "section": "Best Fit",
    "text": "Best Fit\n\nneed to define mathematically the distance between each data point and the regression line\n\nfor every \\(X\\) value in the data, the linear equation can determine the \\(Y\\) value on the line - called predicted \\(Y\\), or \\(\\hat{Y}\\)\nthis distance measures the error between the line and the data\nif all data points were on the line, then the line ‘fits’ the data perfectly\n\\(\\hat{Y}\\) should be close to actual \\(Y\\)\n\n\n\n\ngraph showing the difference between predicted and actual values of \\(Y\\)\n\n\n\ngreen dots = \\(\\hat{Y}\\)\nblue dots = \\(Y\\)\ndistance between the green and blue dots is the prediction error - need to minimize\nsome are positive (above the line) and some are negative (below the line)\n\nsince some are positive and some are negative, if we want to add the error values together, we need to get rid of the signs by squaring the distance and adding the squared error\n\n\\[total \\space squared \\space error ={\\sum^n_{i=1}}({Y_i}-{\\hat{Y_i}})^2\\]\n\nfor each poissible line, we can calculate the total squared error, then choose the line with the lowest total squared error, which is the regression line\nthis minimizes the difference between the line and the observed data\nthis line is called the least squared error solution\nthis method is inefficient due to the infinite number of lines possible\ncalculus is a mathematical technique to find the maxima or minima of a mathematical expression.\nUsing this method, the regression coefficients (slope (\\(b_1\\)) and intercept (\\(b_0\\))) that produce the minimum errors can be calculated."
  },
  {
    "objectID": "posts/simple-regression/index.html#calculating-regression-coefficients",
    "href": "posts/simple-regression/index.html#calculating-regression-coefficients",
    "title": "Simple Regression",
    "section": "Calculating Regression Coefficients",
    "text": "Calculating Regression Coefficients\n\\[\\hat{Y}_i=b_0+b_1X_i\\]\n\\[\\hat{b_1}=r\\frac{S_Y}{S_X}\\]\n\\[\\hat{b_0}=M_Y-\\hat{b_1}M_x\\]\nWhere\n\\(r\\) is the correlation between \\(X\\) and \\(Y\\)\n\\(S_X\\) and \\(S_Y\\) are sample standard deviations for \\(X\\) and \\(Y\\)\n\\(M_X\\) and \\(M_Y\\) are sample means for \\(X\\) and \\(Y\\)\nThese are raw regression coefficients (\\(b_1\\)): the change in outcome is associated with a change in the predictor\n\nStandardized regression coefficients\n\ntell us the same thing, except expressed as standard deviations\nif both variables \\(X\\) and \\(Y\\) have been standardized by transforming tehm into Z-scores before calculating the regression coefficients, then the regression coefficients become *standardized coefficients\n\n\\[\\hat{Z_{Y_i}}=\\beta{Z_{X_i}}\\]\n\\[\\beta=r\\]\nWhere\n\\(\\hat{Z_{Y_i}}\\) is the standard deviation of \\(Y\\)\n\\(\\hat{Z_{X_i}}\\) is the styandard deviation of \\(X\\)\n\\(\\beta\\) is the correlation coefficient\nStandardizing \\(X\\) and \\(Y\\) puts the variables on the same scale - when \\(X\\) increases by 1SD, \\(Y\\) increases by 1SD"
  },
  {
    "objectID": "posts/simple-regression/index.html#analysis-of-regression",
    "href": "posts/simple-regression/index.html#analysis-of-regression",
    "title": "Simple Regression",
    "section": "Analysis of regression",
    "text": "Analysis of regression\n\nregression line is only a model based on the data\nprocess of testing the significance of a regression equation is called analysis of regression\nnull hypothesis in the analysis of regression states that the equation does not account for a significant proportion of the variance in \\(Y\\) scores\nanalysis of regression is similar to ANOVA\n\n\nSum Square Total (\\(SS_T\\) (left plot)) is \\(\\sum(Y-\\bar{Y})^2\\)\nSum Square Residual (\\(SS_R\\) (right plot)) is \\(\\sum(Y-\\hat{Y})^2\\)\n\nprediction error - squared distance between the data and the regression line\nneeds to be minimized\nthis is the variability that is not explained by the regression equation\n\n\n\n\nAlt text\n\n\nSum Square model (\\(SS_M\\)) is \\(\\sum(\\hat{Y}-\\bar{Y})^2\\)\n\nin this equation, \\(\\hat{Y}=b_0+b_1X\\)\n\\(SS_T\\): total variability is the variability between the scores and the mean\n\n\\(SS_R\\): sum squared residual is the residual or error variability between the regression model and the data\n\n\\(SS_M\\) or sum square regression: Model variability - difference in variability between the model and the mean.\\(Y\\) are determined by the value of \\(X\\)\n\n\\(SS_T\\) is the sum of \\(SS_R\\) and \\(SS_M\\)\n\npart of the variability is residual - we don’t know what is causing\n\npart of it is due to the model - the relationship between \\(Y\\) and \\(X\\)\n\n\nThe proportion of variability in \\(Y\\) that is predicted by its relationship with \\(X\\):\n\\[\\frac{SS_{regression}}{SS_T}=r^2\\]\n\\(r^2\\) is an effect size that tells us the proportion of variability that is accounted for by the model\nThe proportion of the variability that is not accounted for is\n\\[\\frac{SS_{residual}}{SS_T}=1-r^2\\]\n\\(r^2\\) is very mieaningful\nif \\(R^2\\) is 20%, then the amount of the variability in \\(Y\\) that is accounted for by the model is 20%, which is quite large.\nTherefore\n\\[SS_{regression}=r^2SS_T\\]\n\\[SS_{residual}=(1-r^2)SS_Y\\]\n\\(SS_Y\\) is the same thing as \\(SS_T\\)\n\\(r^2\\) is the Pearson Correlation Coefficient squared\n\n\n\nAlt text\n\n\nTotal variability is partitioned into two - sum squared regression (explained by the model) and sum squared residual (not explained by the model).\nSame for degrees of freedom\nSums of squares are total values and can be expressed as averages called Mean Squares, or \\(MS\\) AKA variance\n\\[MS_{regression}=\\frac{SS_{regression}}{df_{regression}}\\]\n\nmean square variance is the variance accounted for in the model\n\n\\[MS_{residual}=\\frac{SS_{residual}}{df_{residual}}\\]\n\nmean square residual is the variance not accounted for in the model\n\nAn \\(F\\)-ratio can be used to compare these two mean squares\n\\[F=\\frac{MS_{regression}}{MS_{residual}} \\text{with}\\: df=1,n-2\\]\n\nif the mean square regression is small compared to the mean square residual, then the model does not account for much variance\n\nif the mean square regression is larger than the mean square residual, then the model accounts for more variance"
  },
  {
    "objectID": "posts/simple-regression/index.html#an-example-fields",
    "href": "posts/simple-regression/index.html#an-example-fields",
    "title": "Simple Regression",
    "section": "An Example (Fields)",
    "text": "An Example (Fields)\n\nrecord company boss interested in predicting album sales from advertising bsed on 200 different album releases\n\noutcome variable: sales of CDs and downloads in the week after release\n\npredictor variable: amount in GBP spent promoting the album before release\n\n\n\n\nscatterplot of album sales by advertising budget\n\n\n\\(r^2\\) = 0.335, so 33.5% of the variability is shared by both variables (cutoffs, 9% - small, 25% - large effect size)\n\n\n\nAlt text\n\n\n\n\n\nAlt text\n\n\n\n\n\nAlt text"
  },
  {
    "objectID": "posts/simple-regression/index.html#activities",
    "href": "posts/simple-regression/index.html#activities",
    "title": "Simple Regression",
    "section": "Activities",
    "text": "Activities\nIn a survey that included assessment of husband and wife heights, Hodges, Krech and Crutchfield (1975) reported the following results. Let’s treat wife height as the predictor (X) variable and husband as the outcome (Y) variable:\n\\(r_XY\\) = .32, \\(N\\) = 1, 296\nWife height: \\(M_X\\) = 64.42, \\(S_X\\) = 2.56\nHusband height: \\(M_Y\\) = 70.46, \\(S_Y\\) = 2.87\na. Calculates the values of \\(b_0\\) and \\(b_1\\) to predict husband height in inches (Y) from wife height in inches (X), and write out this raw score predictive equation\n\\[\\hat{b_1}=r\\frac{S_Y}{S_X}\\]\n\\[\\hat{b_1}=.32\\frac{2.87}{2.56}\\]\n\\[\\hat{b_1}=0.35\\]\n\\[b_0=M_Y-\\hat{b_1}M_X\\]\n\\[b_0=70.46-(0.35*64.42)\\]\n\\[b_0=70.46-22.54\\]\n\\[b_0=47.91\\]\nPredictive Equation \\[\\hat{Y}=b_0+{b_1}{X_i}\\] \\[\\hat{Y}=47.91+(0.35*{X_i})\\]\n\nFor women: what is your own height? Substitute your own height into the equation from step a, and calculate the predicted height of your present or future spouse.\n\n\\[\\hat{Y}=47.91+(0.35*67)\\]\n\\[\\hat{Y}=71.36\\]\n\nNow reverse the roles of the variables (i.e., use husband height as the predictor and wife height as the outcome variable). Calculates the values of bb0 and bb1.\n\n\\[\\hat{b_1}=r\\frac{S_Y}{S_X}\\]\n\\[\\hat{b_1}=.32\\frac{2.56}{2.87}\\]\n\\[\\hat{b_1}=.32*.89\\]\n\\[\\hat{b_1}=.28\\]\n\\[b_0=M_Y-\\hat{b_1}M_X\\]\n\\[b_0=64.42-(0.28*70.46)\\]\n\\[b_0=64.42-19.72\\]\n\\[b_0=44.7\\]\n\\[\\hat{Y}=44.7+(0.28*{X_i})\\]\n\nFor men: what is your height in inches? Substitute your own height into the equation from step c, and calculate the predicted height of your present or future spouse.\n\n\\[\\hat{Y}=44.7+(0.28*70)\\]\n\\[\\hat{Y}=64.3\\]\n\nWhat proportion of variance in husband height is predictable from wife height? Test the significance of the regression equation.\n\n\\[r^2=.10\\]\n\nIf both X and Y have been standardized by transforming into Z scores before calculating the regression coefficients, what would be the values of bb0 and bb1 to predict husband height in inches (Y) from wife height in inches (X)? How does this standardized version of the prediction equation tell us about “regression toward the mean” for predictions?\n\n\\[\\hat{Z_{Y_i}}=\\beta{Z_{X_i}}\\]\n\\[\\hat{2.87}=0.32{2.56}\\]"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "Hey!\n\nI plan to use this space to record my learning and review of stats using R and this blog built with Quarto. The last time I was semi-serious about a blog was in Grav, and WordPress before that, so I’m curious to see how this works out.\nI’m using a few books in this journey, including\nDesjardins, C. D., & Bulut, O. (2018). Handbook of Educational Measurement and Psychometrics Using R (1st ed.). Chapman and Hall/CRC. https://doi.org/10.1201/b20498\nField, A. P. (2018). Discovering statistics using IBM SPSS statistics (5th edition, North American edition). Sage Publications Inc.\nNavarro, D. (n.d.). Learning statistics with R: A tutorial for psychology students and other beginners. (Version 0.6.1). Retrieved January 8, 2020, from https://learningstatisticswithr.com/book/"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Stats Blog",
    "section": "",
    "text": "discover_08 - General Linear Model\n\n\n\n\n\n\nregression\n\n\nR\n\n\ndiscovr\n\n\n\n\n\n\n\n\n\nFeb 26, 2024\n\n\nColin Madland\n\n\n\n\n\n\n\n\n\n\n\n\nSimple Regression\n\n\n\n\n\n\n505\n\n\n\n\n\n\n\n\n\nFeb 18, 2024\n\n\nColin Madland\n\n\n\n\n\n\n\n\n\n\n\n\nVariance and Standard Deviation\n\n\n\n\n\n\nintro-stats\n\n\nvariance\n\n\nstandard-deviation\n\n\n\n\n\n\n\n\n\nFeb 18, 2024\n\n\nColin Madland\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nFeb 14, 2024\n\n\nColin Madland\n\n\n\n\n\n\nNo matching items"
  }
]