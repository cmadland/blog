[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Sharing as I learn statistics with R and Quarto."
  },
  {
    "objectID": "posts/discovr_06_bias/index.html",
    "href": "posts/discovr_06_bias/index.html",
    "title": "discovr_06 - The Beast of Bias",
    "section": "",
    "text": "library(tidyverse, ggplot2)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\ndownload_tib &lt;- here::here(\"data/download_festival.csv\") |&gt; readr::read_csv()\n\nRows: 810 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): gender\ndbl (4): ticket_no, day_1, day_2, day_3\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ndownload_tib &lt;- download_tib |&gt; \ndplyr::mutate(\n    ticket_no = as.character(ticket_no),\n    gender = forcats::as_factor(gender) |&gt;\n      forcats::fct_relevel(\"Male\", \"Female\", \"Non-binary\")\n  )\ndownload_tib\n\n# A tibble: 810 × 5\n   ticket_no gender day_1 day_2 day_3\n   &lt;chr&gt;     &lt;fct&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 2111      Male    2.64  1.35  1.61\n 2 2229      Female  0.97  1.41  0.29\n 3 2338      Male    0.84 NA    NA   \n 4 2384      Female  3.03 NA    NA   \n 5 2401      Female  0.88  0.08 NA   \n 6 2405      Male    0.85 NA    NA   \n 7 2467      Female  1.56 NA    NA   \n 8 2478      Female  3.02 NA    NA   \n 9 2490      Male    2.29 NA    NA   \n10 2504      Female  1.11  0.44  0.55\n# ℹ 800 more rows\ntidyr has two functions for converting data from messy to tidy. - pivot_longer() takes columns and puts them into rosw to make messy data tidy - pivot_wider() takes rows and puts them in columns to make tidy data messy"
  },
  {
    "objectID": "posts/discovr_06_bias/index.html#making-messy-data-tidy",
    "href": "posts/discovr_06_bias/index.html#making-messy-data-tidy",
    "title": "discovr_06 - The Beast of Bias",
    "section": "Making messy data tidy",
    "text": "Making messy data tidy\ntidyr::pivot_longer(\n  data = tibble,\n  cols = column_names,\n  names_to = \"name_of_column_to_contain_variable_names\",\n  values_to = \"name_of_column_to_contain_values\",\n)\n\ntibble\n\nName of the messy tibble\n\ncolumn_names\n\nlist of columns to be restructured into rows\n\nnames_to\n\nname for the new variable that contains names of the original columns\n\nvalue_to\n\nname for the new variable that will contain the values."
  },
  {
    "objectID": "posts/discovr_06_bias/index.html#code-example",
    "href": "posts/discovr_06_bias/index.html#code-example",
    "title": "discovr_06 - The Beast of Bias",
    "section": "Code example",
    "text": "Code example\n\nin download_tib, there are three columns/variables that need to be restructured into rows\nspecify the variables using day_1:day_3\nscores in these columns represent hygiene scores, so we could use hygiene as the variable to contain values after restructuring\ncolumns we are transforming represent different days at the festival, so we can use day as the name of the variable created to contain column names\n\n\ndownload_tidy_tib &lt;- download_tib |&gt;  # create a new object called `download_tidy_tib`\n  tidyr::pivot_longer(                              # use the `pivot_longer()` function from `tidyr`\n  cols = day_1:day_3,                           # specify columns `day_1:day_3` for restructuring\n  names_to = \"day\",                             # names of the columns be placed in a variable called `day`\n  values_to = \"hygiene\",                        # values of the columns placed in a variable called `hygiene`\n)\ndownload_tidy_tib                               # display the new object\n\n# A tibble: 2,430 × 4\n   ticket_no gender day   hygiene\n   &lt;chr&gt;     &lt;fct&gt;  &lt;chr&gt;   &lt;dbl&gt;\n 1 2111      Male   day_1    2.64\n 2 2111      Male   day_2    1.35\n 3 2111      Male   day_3    1.61\n 4 2229      Female day_1    0.97\n 5 2229      Female day_2    1.41\n 6 2229      Female day_3    0.29\n 7 2338      Male   day_1    0.84\n 8 2338      Male   day_2   NA   \n 9 2338      Male   day_3   NA   \n10 2384      Female day_1    3.03\n# ℹ 2,420 more rows"
  },
  {
    "objectID": "posts/discovr_06_bias/index.html#tidying-labels",
    "href": "posts/discovr_06_bias/index.html#tidying-labels",
    "title": "discovr_06 - The Beast of Bias",
    "section": "Tidying labels",
    "text": "Tidying labels\n\nthe values in day match the original column names exactly (day_1)\nwe want sentence case (Day 1)\nuse stringr\n\ndownload_tidy_tib &lt;- download_tidy_tib |&gt;   # recreates `download_tidy_tib` from itself\n   dplyr::mutate(                                             # uses `dplyr::mutate` to recreate the variable `day`\n    day = stringr::str_to_sentence(day) |&gt; stringr::str_replace(\"_\", \" \")                                                # uses `stringr::str_to_sentence` to capitalize the d, then `str_replace()` to find the underscore and replace it with a space\n  )\n\ndownload_tidy_tib &lt;- download_tidy_tib |&gt; \n   dplyr::mutate(\n    day = stringr::str_to_sentence(day) |&gt; stringr::str_replace(\"_\", \" \")\n  )\n  download_tidy_tib\n\n# A tibble: 2,430 × 4\n   ticket_no gender day   hygiene\n   &lt;chr&gt;     &lt;fct&gt;  &lt;chr&gt;   &lt;dbl&gt;\n 1 2111      Male   Day 1    2.64\n 2 2111      Male   Day 2    1.35\n 3 2111      Male   Day 3    1.61\n 4 2229      Female Day 1    0.97\n 5 2229      Female Day 2    1.41\n 6 2229      Female Day 3    0.29\n 7 2338      Male   Day 1    0.84\n 8 2338      Male   Day 2   NA   \n 9 2338      Male   Day 3   NA   \n10 2384      Female Day 1    3.03\n# ℹ 2,420 more rows\n\n\n## Making tidy data messy\n\npivot_wider() reverses the process above\n\ntidyr::pivot_wider(\n  data = tibble,  # tibble to be restructured\n  id_cols = variables_that_you_do_not_want_to_restructure,\n  names_from = \"variable_containing_the_names_of_columns\",\n  values_from = \" variable_containing_the_scores\",\n)\n\ndownload_tib &lt;- download_tidy_tib |&gt; \n  tidyr::pivot_wider(\n  id_cols = c(ticket_no, gender),\n  names_from = \"day\",\n  values_from = \"hygiene\",\n)\ndownload_tib\n\n# A tibble: 810 × 5\n   ticket_no gender `Day 1` `Day 2` `Day 3`\n   &lt;chr&gt;     &lt;fct&gt;    &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n 1 2111      Male      2.64    1.35    1.61\n 2 2229      Female    0.97    1.41    0.29\n 3 2338      Male      0.84   NA      NA   \n 4 2384      Female    3.03   NA      NA   \n 5 2401      Female    0.88    0.08   NA   \n 6 2405      Male      0.85   NA      NA   \n 7 2467      Female    1.56   NA      NA   \n 8 2478      Female    3.02   NA      NA   \n 9 2490      Male      2.29   NA      NA   \n10 2504      Female    1.11    0.44    0.55\n# ℹ 800 more rows\n\n\n\nin this case, having the variable names in sentence case (Day 1), is inconvenient because we will always have to put them in backticks\nrename using dplyr::rename_with\n\n\ndownload_tib &lt;- download_tib |&gt; \n  dplyr::rename_with(.cols = starts_with(\"Day\"), # finds all columns w/i download_tib that begin with the word `Day'\n     .fn = \\(column) stringr::str_replace(string = column, # creates a lambda or anonymous function that will be applied to the variables that begin with Day\n        pattern = \"Day \",  # with next line, tells the function what to do\n        replacement = \"day_\")\n)\n\ndownload_tib\n\n# A tibble: 810 × 5\n   ticket_no gender day_1 day_2 day_3\n   &lt;chr&gt;     &lt;fct&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 2111      Male    2.64  1.35  1.61\n 2 2229      Female  0.97  1.41  0.29\n 3 2338      Male    0.84 NA    NA   \n 4 2384      Female  3.03 NA    NA   \n 5 2401      Female  0.88  0.08 NA   \n 6 2405      Male    0.85 NA    NA   \n 7 2467      Female  1.56 NA    NA   \n 8 2478      Female  3.02 NA    NA   \n 9 2490      Male    2.29 NA    NA   \n10 2504      Female  1.11  0.44  0.55\n# ℹ 800 more rows"
  },
  {
    "objectID": "posts/discovr_06_bias/index.html#spotting-outliers",
    "href": "posts/discovr_06_bias/index.html#spotting-outliers",
    "title": "discovr_06 - The Beast of Bias",
    "section": "Spotting outliers",
    "text": "Spotting outliers\nTwo ways:\n\nvisualize the data and look for unusual cases\nlook for values that are poorly predicted by the model, using model residuals as described in DSUR\n\n\n\n\n\n\n\nWhat are the model residuals?\n\n\n\n\n\n\nThe differences between the values a model predicts and the values observed in the data on which the model is based"
  },
  {
    "objectID": "posts/discovr_06_bias/index.html#histograms-and-boxplots",
    "href": "posts/discovr_06_bias/index.html#histograms-and-boxplots",
    "title": "discovr_06 - The Beast of Bias",
    "section": "Histograms and Boxplots",
    "text": "Histograms and Boxplots"
  },
  {
    "objectID": "posts/fellowship/index.html",
    "href": "posts/fellowship/index.html",
    "title": "Technology-Integrated Assessment in BC Higher Education",
    "section": "",
    "text": "In the fall of 2021, I embarked on the BCcampus Student Research Fellowship, a project funded by BCcampus intended to promote research on teaching and learning in BC. My project aimed at technology-integrated assessment practices in higher education. Assessment is already a contentious and high-stakes topic in higher education; however, concern around assessment escalated due to the COVID-19 pandemic, and also in light of generative AI models. The fellowship provided funding and support for a component of my PhD dissertation research at the University of Victoria, where I am supervised by Dr. Valerie Irvine.\nMy research is ongoing, and I am pleased to report that I have hit a milestone in completing and publishing the first two of three papers that will form my dissertation. The first paper is a literature review exploring how higher education instructors integrate technology and assessment (Madland et al., 2024b). We purposely explored papers written by those who do not study assessment as a disciplinary area, but who are instructors in a wide variety of disciplines and who are compelled for one reason or another to use technology when they assess learning. We explored these articles through the lens of the assessment design in a digital world framework published by Bearman et al. (2022). While there were areas of overlap between the literature and the Bearman et al. framework (primarily the importance of understanding the different purposes of assessment), there were also themes that emerged in the literature that are not modeled in the framework (e.g., instructor workload, academic integrity), and there were components of the framework that were not evident in the literature (e.g., digital literacies). These points of incongruity suggested a need for revisions to the model which led to a process of envisioning what a model of technology-integrated assessment might look like.\nThe development of the technology-integrated assessment framework, described in the second paper (Madland et al., 2024a), is the result of this envisioning process and serves as a good anchor point in beginning to understand how to improve technology-integrated assessment practices in higher education in British Columbia and beyond. The technology-integrated assessment framework consists of four components, which might be considered to be four factors that instructors consider when planning assessment. The four factors and their sub-factors are:\nFigure 1 is a theoretical conceptualization of the model and its sub-components.\nNote: Licensed (CC-BY), from Madland, C., Irvine, V., DeLuca, C., & Bulut, O. (2024). Developing the Technology-Integrated Assessment Framework. The Open/Technology in Education, Society, and Scholarship Association Journal, 4(1), 1–19. https://doi.org/10.18357/otessaj.2024.4.1.63.\nWe were intentional about considering the framework in light of the 5Rs of Indigenous education: respect, relevance, reciprocity, responsibility, and relationships (Kirkness & Barnhardt, 1991; Tessaro et al., 2018). Respect and responsibility are embedded in the ‘Duty of Care’ component while relationships, reciprocity, and relevance are explicit in the framework. Further work is coming and will focus on exploring the relationships between the factors and sub-factors in the framework.\nI am grateful to my supervisor, Dr. Valerie Irvine, for her keen insight and expert contributions, as well as my committee members, Dr. Chris DeLuca (Queen’s University), and Dr. Okan Bulut (University of Alberta) whose diverse wisdom around assessment were invaluable.\nI am also appreciative to BCcampus, especially Leva Lee, Britt Dzioba, Rebecca Shortt, and Gwen Nguyen for their support through the BCcampus Student Fellowship program, which provides B.C. post-secondary educators and students with funding to conduct research on teaching and learning, as well as explore evidence-based teaching practices that focus on student success and learning."
  },
  {
    "objectID": "posts/dsur_ch18_efa/index.html",
    "href": "posts/dsur_ch18_efa/index.html",
    "title": "dsus_18 - Exploratory Factor Analysis",
    "section": "",
    "text": "library(tidyverse, knitr)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(ggplot2, ggfortify, robust)\nlibrary(viridis)\n\nLoading required package: viridisLite\n\nlibrary(GPArotation)\nraq_tib &lt;- here::here(\"data/raq.csv\") |&gt;\n  readr::read_csv()\n\nRows: 2571 Columns: 24\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (1): id\ndbl (23): raq_01, raq_02, raq_03, raq_04, raq_05, raq_06, raq_07, raq_08, ra...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nNotes here are from\nField, A. P. (2018). Discovering statistics using IBM SPSS statistics (5th edition, North American edition). Sage Publications Inc. -&gt; DSuS\nCode examples will be from:\nField, A. (2023). discovr: Interactive tutorials and data for “Discovering Statistics Using R and RStudio” [Manual]. -&gt; discovr\nR Core Team. (2023). R: A language and environment for statistical computing [Manual]. R Foundation for Statistical Computing. https://www.R-project.org/"
  },
  {
    "objectID": "posts/dsur_ch18_efa/index.html#when-to-use-factor-analysis-p.-571",
    "href": "posts/dsur_ch18_efa/index.html#when-to-use-factor-analysis-p.-571",
    "title": "dsus_18 - Exploratory Factor Analysis",
    "section": "When to use factor analysis (p. 571)",
    "text": "When to use factor analysis (p. 571)\n\nwhen attempting to measure latent variables\nfactor analysis and principal component analysis are used to identify clusters of variables\n3 main uses\n\nunderstand the structure of a set of variables\nto construct a questionnaire to measure an underlying variable\nreduce the size of a data set while retaining as much of the original information as possible\n\n(FA can be used to solve the problem of multicollinearity by combining factors that are collinear)\n\n\n\n\n\n\n\n\n\n\nmulticollinearity\n\nexists when there is a strong correlation between two factors\n\n\nmulticollinearity makes it difficult or impossible to determine the amount of variance accounted for by one of two correlated factors\n\n\ne.g., if a factor predicts the outcome variable with \\(R=0.80\\) and a second variable accounts for the same variance (i.e., it is highly correlated to the first variable) then it is only contributing to a very small amount of the unique variance in outcome and we might see \\(R=0.82\\). If the two predictor variables are uncorrelated, then the second variable is contributing more unique variance and we might see \\(R=0.95\\)\n\n\nmulticollinearity leads to interchangable predictors\n\n\n\n\n\n\nExamples of factor analysis\n\nextroversion, introversion, neuroticism traits\npersonality questionnaires\nin economics to see whether productivity, profits, and workforce contribute to the underlying dimension of company growth\n\n\n\nEFA and PCA\n\nthey are not the same thing\nbut they both are used to reduce a set of variables into a smaller set of dimensions (factors in EFA, and components in PCA)"
  },
  {
    "objectID": "posts/dsur_ch18_efa/index.html#factors-and-components",
    "href": "posts/dsur_ch18_efa/index.html#factors-and-components",
    "title": "dsus_18 - Exploratory Factor Analysis",
    "section": "Factors and Components",
    "text": "Factors and Components\n\nmeasuring several variables with several questions gives us data that can be arranged in a correlation matrix (R matrix) as below. \nNote: Created with the ggplot2 R package (Wickham 2016) with data from the discovr package (Field 2023.)\nfactor analysis tries to explain the maximum amount of common variance in the matrix using the least number of explanatory constructs (latent variables), which represent clusters of variables that correlate highly with each other.\nPCA differs in that it tries to explain the maximum amount of total variance in a correlation matrix by transforming the original variables into linear components"
  },
  {
    "objectID": "posts/dsur_ch18_efa/index.html#example---induced-anxiety-discovr_18-field2023",
    "href": "posts/dsur_ch18_efa/index.html#example---induced-anxiety-discovr_18-field2023",
    "title": "dsus_18 - Exploratory Factor Analysis",
    "section": "Example - Induced Anxiety (discovr_18 Field (2023))",
    "text": "Example - Induced Anxiety (discovr_18 Field (2023))\n\nquestionnaire developed to measure anxiety related to using R\nquestions developed from interviews with anxious and non-anxious students\n23 questions; 5-point likert (strongly disagree -&gt; strongly agree)\n\nraq_01: Statistics make me cry\nraq_02: My friends will think I’m stupid for not being able to cope with R\nraq_03: Standard deviations excite me\nraq_04: I dream that Pearson is attacking me with correlation coefficients\nraq_05: I don’t understand statistics\nraq_06: I have little experience of computers\nraq_07: All computers hate me\nraq_08: I have never been good at mathematics\nraq_09: My friends are better at statistics than me\nraq_10: Computers are useful only for playing games\nraq_11: I did badly at mathematics at school\nraq_12: People try to tell you that R makes statistics easier to understand but it doesn’t\nraq_13: I worry that I will cause irreparable damage because of my incompetence with computers\nraq_14: Computers have minds of their own and deliberately go wrong whenever I use them\nraq_15: Computers are out to get me\nraq_16: I weep openly at the mention of central tendency\nraq_17: I slip into a coma whenever I see an equation\nraq_18: R always crashes when I try to use it\nraq_19: Everybody looks at me when I use R\nraq_20: I can’t sleep for thoughts of eigenvectors\nraq_21: I wake up under my duvet thinking that I am trapped under a normal distribution\nraq_22: My friends are better at R than I am\nraq_23: If I am good at statistics people will think I am a nerd\n\n\n\nraq_tib\n\n# A tibble: 2,571 × 24\n   id    raq_01 raq_02 raq_03 raq_04 raq_05 raq_06 raq_07 raq_08 raq_09 raq_10\n   &lt;chr&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n 1 157lk      3      1      3      4      2      4      3      3      3      2\n 2 k3xjh      2      5      3      3      4      4      5      4      4      3\n 3 832x6      4      5      3      3      3      4      5      4      3      4\n 4 6dyt5      4      4      4      3      4      4      4      3      2      2\n 5 w4upu      4      2      5      4      2      3      2      5      4      5\n 6 9432i      2      3      4      3      4      2      3      2      3      2\n 7 x03sh      2      4      5      5      3      3      4      3      3      3\n 8 12v15      4      3      4      3      5      4      5      4      3      5\n 9 46y22      3      5      5      1      3      4      4      3      5      3\n10 1iqc4      2      5      3      2      4      5      5      5      4      4\n# ℹ 2,561 more rows\n# ℹ 13 more variables: raq_11 &lt;dbl&gt;, raq_12 &lt;dbl&gt;, raq_13 &lt;dbl&gt;, raq_14 &lt;dbl&gt;,\n#   raq_15 &lt;dbl&gt;, raq_16 &lt;dbl&gt;, raq_17 &lt;dbl&gt;, raq_18 &lt;dbl&gt;, raq_19 &lt;dbl&gt;,\n#   raq_20 &lt;dbl&gt;, raq_21 &lt;dbl&gt;, raq_22 &lt;dbl&gt;, raq_23 &lt;dbl&gt;\n\n\n\n24 variables including the ID\nwe don’t need the id in analyses so create a new tib without it\n\n\nraq_items_tib &lt;- raq_tib |&gt; \n  dplyr::select(-id)\nraq_items_tib\n\n# A tibble: 2,571 × 23\n   raq_01 raq_02 raq_03 raq_04 raq_05 raq_06 raq_07 raq_08 raq_09 raq_10 raq_11\n    &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n 1      3      1      3      4      2      4      3      3      3      2      2\n 2      2      5      3      3      4      4      5      4      4      3      4\n 3      4      5      3      3      3      4      5      4      3      4      4\n 4      4      4      4      3      4      4      4      3      2      2      2\n 5      4      2      5      4      2      3      2      5      4      5      4\n 6      2      3      4      3      4      2      3      2      3      2      1\n 7      2      4      5      5      3      3      4      3      3      3      3\n 8      4      3      4      3      5      4      5      4      3      5      3\n 9      3      5      5      1      3      4      4      3      5      3      3\n10      2      5      3      2      4      5      5      5      4      4      4\n# ℹ 2,561 more rows\n# ℹ 12 more variables: raq_12 &lt;dbl&gt;, raq_13 &lt;dbl&gt;, raq_14 &lt;dbl&gt;, raq_15 &lt;dbl&gt;,\n#   raq_16 &lt;dbl&gt;, raq_17 &lt;dbl&gt;, raq_18 &lt;dbl&gt;, raq_19 &lt;dbl&gt;, raq_20 &lt;dbl&gt;,\n#   raq_21 &lt;dbl&gt;, raq_22 &lt;dbl&gt;, raq_23 &lt;dbl&gt;\n\n\n\nCorrelation matrix\n\n# feed the tibble with only the RAQ items into the correlation() function\ncorrelation::correlation(raq_items_tib)\n\n# Correlation Matrix (pearson-method)\n\nParameter1 | Parameter2 |     r |         95% CI | t(2569) |         p\n----------------------------------------------------------------------\nraq_01     |     raq_02 |  0.11 | [ 0.07,  0.15] |    5.47 | &lt; .001***\nraq_01     |     raq_03 | -0.18 | [-0.22, -0.14] |   -9.21 | &lt; .001***\nraq_01     |     raq_04 |  0.22 | [ 0.18,  0.25] |   11.26 | &lt; .001***\nraq_01     |     raq_05 |  0.22 | [ 0.19,  0.26] |   11.60 | &lt; .001***\nraq_01     |     raq_06 |  0.16 | [ 0.12,  0.20] |    8.17 | &lt; .001***\nraq_01     |     raq_07 |  0.11 | [ 0.07,  0.14] |    5.41 | &lt; .001***\nraq_01     |     raq_08 |  0.22 | [ 0.18,  0.26] |   11.53 | &lt; .001***\nraq_01     |     raq_09 |  0.08 | [ 0.05,  0.12] |    4.26 | &lt; .001***\nraq_01     |     raq_10 |  0.11 | [ 0.07,  0.15] |    5.70 | &lt; .001***\nraq_01     |     raq_11 |  0.19 | [ 0.15,  0.23] |    9.76 | &lt; .001***\nraq_01     |     raq_12 |  0.15 | [ 0.11,  0.19] |    7.67 | &lt; .001***\nraq_01     |     raq_13 |  0.11 | [ 0.07,  0.15] |    5.55 | &lt; .001***\nraq_01     |     raq_14 |  0.12 | [ 0.08,  0.15] |    5.87 | &lt; .001***\nraq_01     |     raq_15 |  0.11 | [ 0.07,  0.15] |    5.61 | &lt; .001***\nraq_01     |     raq_16 |  0.18 | [ 0.14,  0.21] |    9.04 | &lt; .001***\nraq_01     |     raq_17 |  0.17 | [ 0.13,  0.20] |    8.52 | &lt; .001***\nraq_01     |     raq_18 |  0.12 | [ 0.08,  0.16] |    6.06 | &lt; .001***\nraq_01     |     raq_19 |  0.11 | [ 0.07,  0.14] |    5.45 | &lt; .001***\nraq_01     |     raq_20 |  0.21 | [ 0.17,  0.24] |   10.73 | &lt; .001***\nraq_01     |     raq_21 |  0.23 | [ 0.19,  0.26] |   11.81 | &lt; .001***\nraq_01     |     raq_22 |  0.12 | [ 0.09,  0.16] |    6.30 | &lt; .001***\nraq_01     |     raq_23 |  0.08 | [ 0.04,  0.12] |    3.93 | &lt; .001***\nraq_02     |     raq_03 | -0.12 | [-0.16, -0.08] |   -6.14 | &lt; .001***\nraq_02     |     raq_04 |  0.12 | [ 0.09,  0.16] |    6.31 | &lt; .001***\nraq_02     |     raq_05 |  0.28 | [ 0.24,  0.31] |   14.71 | &lt; .001***\nraq_02     |     raq_06 |  0.34 | [ 0.30,  0.37] |   18.19 | &lt; .001***\nraq_02     |     raq_07 |  0.26 | [ 0.22,  0.29] |   13.44 | &lt; .001***\nraq_02     |     raq_08 |  0.18 | [ 0.14,  0.22] |    9.16 | &lt; .001***\nraq_02     |     raq_09 |  0.39 | [ 0.36,  0.42] |   21.50 | &lt; .001***\nraq_02     |     raq_10 |  0.18 | [ 0.15,  0.22] |    9.48 | &lt; .001***\nraq_02     |     raq_11 |  0.15 | [ 0.11,  0.19] |    7.82 | &lt; .001***\nraq_02     |     raq_12 |  0.09 | [ 0.05,  0.13] |    4.58 | &lt; .001***\nraq_02     |     raq_13 |  0.23 | [ 0.19,  0.27] |   11.96 | &lt; .001***\nraq_02     |     raq_14 |  0.20 | [ 0.16,  0.23] |   10.21 | &lt; .001***\nraq_02     |     raq_15 |  0.19 | [ 0.15,  0.22] |    9.64 | &lt; .001***\nraq_02     |     raq_16 |  0.10 | [ 0.06,  0.13] |    4.92 | &lt; .001***\nraq_02     |     raq_17 |  0.17 | [ 0.14,  0.21] |    8.94 | &lt; .001***\nraq_02     |     raq_18 |  0.26 | [ 0.23,  0.30] |   13.87 | &lt; .001***\nraq_02     |     raq_19 |  0.39 | [ 0.36,  0.42] |   21.50 | &lt; .001***\nraq_02     |     raq_20 |  0.13 | [ 0.09,  0.17] |    6.55 | &lt; .001***\nraq_02     |     raq_21 |  0.16 | [ 0.12,  0.20] |    8.26 | &lt; .001***\nraq_02     |     raq_22 |  0.34 | [ 0.30,  0.37] |   18.28 | &lt; .001***\nraq_02     |     raq_23 |  0.39 | [ 0.36,  0.42] |   21.56 | &lt; .001***\nraq_03     |     raq_04 | -0.19 | [-0.23, -0.16] |  -10.01 | &lt; .001***\nraq_03     |     raq_05 | -0.25 | [-0.29, -0.22] |  -13.34 | &lt; .001***\nraq_03     |     raq_06 | -0.17 | [-0.20, -0.13] |   -8.51 | &lt; .001***\nraq_03     |     raq_07 | -0.13 | [-0.16, -0.09] |   -6.45 | &lt; .001***\nraq_03     |     raq_08 | -0.22 | [-0.25, -0.18] |  -11.33 | &lt; .001***\nraq_03     |     raq_09 | -0.08 | [-0.12, -0.04] |   -4.14 | &lt; .001***\nraq_03     |     raq_10 | -0.10 | [-0.14, -0.06] |   -5.18 | &lt; .001***\nraq_03     |     raq_11 | -0.19 | [-0.23, -0.15] |   -9.81 | &lt; .001***\nraq_03     |     raq_12 | -0.15 | [-0.19, -0.11] |   -7.58 | &lt; .001***\nraq_03     |     raq_13 | -0.13 | [-0.17, -0.09] |   -6.71 | &lt; .001***\nraq_03     |     raq_14 | -0.09 | [-0.13, -0.05] |   -4.45 | &lt; .001***\nraq_03     |     raq_15 | -0.15 | [-0.19, -0.11] |   -7.57 | &lt; .001***\nraq_03     |     raq_16 | -0.20 | [-0.24, -0.17] |  -10.56 | &lt; .001***\nraq_03     |     raq_17 | -0.18 | [-0.21, -0.14] |   -9.01 | &lt; .001***\nraq_03     |     raq_18 | -0.15 | [-0.19, -0.11] |   -7.78 | &lt; .001***\nraq_03     |     raq_19 | -0.12 | [-0.15, -0.08] |   -5.96 | &lt; .001***\nraq_03     |     raq_20 | -0.23 | [-0.27, -0.20] |  -12.20 | &lt; .001***\nraq_03     |     raq_21 | -0.26 | [-0.30, -0.22] |  -13.62 | &lt; .001***\nraq_03     |     raq_22 | -0.12 | [-0.16, -0.08] |   -6.00 | &lt; .001***\nraq_03     |     raq_23 | -0.05 | [-0.09, -0.01] |   -2.65 | 0.014*   \nraq_04     |     raq_05 |  0.32 | [ 0.28,  0.35] |   17.08 | &lt; .001***\nraq_04     |     raq_06 |  0.23 | [ 0.20,  0.27] |   12.21 | &lt; .001***\nraq_04     |     raq_07 |  0.17 | [ 0.13,  0.20] |    8.56 | &lt; .001***\nraq_04     |     raq_08 |  0.25 | [ 0.21,  0.28] |   12.96 | &lt; .001***\nraq_04     |     raq_09 |  0.10 | [ 0.06,  0.14] |    4.98 | &lt; .001***\nraq_04     |     raq_10 |  0.18 | [ 0.14,  0.21] |    9.11 | &lt; .001***\nraq_04     |     raq_11 |  0.22 | [ 0.18,  0.26] |   11.54 | &lt; .001***\nraq_04     |     raq_12 |  0.20 | [ 0.16,  0.23] |   10.12 | &lt; .001***\nraq_04     |     raq_13 |  0.16 | [ 0.13,  0.20] |    8.42 | &lt; .001***\nraq_04     |     raq_14 |  0.15 | [ 0.11,  0.18] |    7.55 | &lt; .001***\nraq_04     |     raq_15 |  0.18 | [ 0.15,  0.22] |    9.50 | &lt; .001***\nraq_04     |     raq_16 |  0.27 | [ 0.23,  0.31] |   14.25 | &lt; .001***\nraq_04     |     raq_17 |  0.24 | [ 0.20,  0.27] |   12.36 | &lt; .001***\nraq_04     |     raq_18 |  0.17 | [ 0.13,  0.21] |    8.81 | &lt; .001***\nraq_04     |     raq_19 |  0.14 | [ 0.10,  0.18] |    7.24 | &lt; .001***\nraq_04     |     raq_20 |  0.28 | [ 0.25,  0.32] |   15.00 | &lt; .001***\nraq_04     |     raq_21 |  0.32 | [ 0.28,  0.35] |   17.12 | &lt; .001***\nraq_04     |     raq_22 |  0.16 | [ 0.12,  0.19] |    8.07 | &lt; .001***\nraq_04     |     raq_23 |  0.08 | [ 0.04,  0.12] |    4.22 | &lt; .001***\nraq_05     |     raq_06 |  0.51 | [ 0.48,  0.54] |   30.26 | &lt; .001***\nraq_05     |     raq_07 |  0.36 | [ 0.33,  0.40] |   19.79 | &lt; .001***\nraq_05     |     raq_08 |  0.36 | [ 0.32,  0.39] |   19.39 | &lt; .001***\nraq_05     |     raq_09 |  0.16 | [ 0.12,  0.20] |    8.28 | &lt; .001***\nraq_05     |     raq_10 |  0.31 | [ 0.28,  0.35] |   16.67 | &lt; .001***\nraq_05     |     raq_11 |  0.31 | [ 0.28,  0.35] |   16.61 | &lt; .001***\nraq_05     |     raq_12 |  0.19 | [ 0.15,  0.22] |    9.66 | &lt; .001***\nraq_05     |     raq_13 |  0.35 | [ 0.32,  0.38] |   19.00 | &lt; .001***\nraq_05     |     raq_14 |  0.27 | [ 0.23,  0.30] |   14.05 | &lt; .001***\nraq_05     |     raq_15 |  0.33 | [ 0.29,  0.36] |   17.54 | &lt; .001***\nraq_05     |     raq_16 |  0.24 | [ 0.20,  0.28] |   12.53 | &lt; .001***\nraq_05     |     raq_17 |  0.30 | [ 0.27,  0.34] |   16.01 | &lt; .001***\nraq_05     |     raq_18 |  0.38 | [ 0.35,  0.41] |   20.97 | &lt; .001***\nraq_05     |     raq_19 |  0.29 | [ 0.26,  0.33] |   15.50 | &lt; .001***\nraq_05     |     raq_20 |  0.34 | [ 0.31,  0.37] |   18.36 | &lt; .001***\nraq_05     |     raq_21 |  0.37 | [ 0.34,  0.40] |   20.24 | &lt; .001***\nraq_05     |     raq_22 |  0.27 | [ 0.23,  0.31] |   14.26 | &lt; .001***\nraq_05     |     raq_23 |  0.17 | [ 0.13,  0.21] |    8.70 | &lt; .001***\nraq_06     |     raq_07 |  0.45 | [ 0.42,  0.48] |   25.68 | &lt; .001***\nraq_06     |     raq_08 |  0.34 | [ 0.31,  0.37] |   18.36 | &lt; .001***\nraq_06     |     raq_09 |  0.20 | [ 0.16,  0.24] |   10.40 | &lt; .001***\nraq_06     |     raq_10 |  0.40 | [ 0.36,  0.43] |   21.93 | &lt; .001***\nraq_06     |     raq_11 |  0.29 | [ 0.26,  0.33] |   15.55 | &lt; .001***\nraq_06     |     raq_12 |  0.11 | [ 0.07,  0.15] |    5.74 | &lt; .001***\nraq_06     |     raq_13 |  0.46 | [ 0.42,  0.49] |   25.90 | &lt; .001***\nraq_06     |     raq_14 |  0.36 | [ 0.32,  0.39] |   19.25 | &lt; .001***\nraq_06     |     raq_15 |  0.40 | [ 0.37,  0.43] |   22.20 | &lt; .001***\nraq_06     |     raq_16 |  0.16 | [ 0.12,  0.20] |    8.21 | &lt; .001***\nraq_06     |     raq_17 |  0.28 | [ 0.24,  0.32] |   14.83 | &lt; .001***\nraq_06     |     raq_18 |  0.51 | [ 0.48,  0.54] |   30.01 | &lt; .001***\nraq_06     |     raq_19 |  0.38 | [ 0.34,  0.41] |   20.64 | &lt; .001***\nraq_06     |     raq_20 |  0.23 | [ 0.19,  0.26] |   11.74 | &lt; .001***\nraq_06     |     raq_21 |  0.26 | [ 0.23,  0.30] |   13.86 | &lt; .001***\nraq_06     |     raq_22 |  0.33 | [ 0.29,  0.36] |   17.48 | &lt; .001***\nraq_06     |     raq_23 |  0.17 | [ 0.13,  0.21] |    8.85 | &lt; .001***\nraq_07     |     raq_08 |  0.25 | [ 0.21,  0.28] |   13.03 | &lt; .001***\nraq_07     |     raq_09 |  0.17 | [ 0.13,  0.20] |    8.48 | &lt; .001***\nraq_07     |     raq_10 |  0.28 | [ 0.25,  0.32] |   14.94 | &lt; .001***\nraq_07     |     raq_11 |  0.21 | [ 0.18,  0.25] |   11.02 | &lt; .001***\nraq_07     |     raq_12 |  0.10 | [ 0.06,  0.13] |    4.89 | &lt; .001***\nraq_07     |     raq_13 |  0.31 | [ 0.28,  0.35] |   16.58 | &lt; .001***\nraq_07     |     raq_14 |  0.25 | [ 0.21,  0.29] |   13.16 | &lt; .001***\nraq_07     |     raq_15 |  0.29 | [ 0.25,  0.32] |   15.28 | &lt; .001***\nraq_07     |     raq_16 |  0.12 | [ 0.08,  0.16] |    6.21 | &lt; .001***\nraq_07     |     raq_17 |  0.21 | [ 0.17,  0.25] |   10.94 | &lt; .001***\nraq_07     |     raq_18 |  0.35 | [ 0.32,  0.38] |   18.93 | &lt; .001***\nraq_07     |     raq_19 |  0.26 | [ 0.22,  0.29] |   13.41 | &lt; .001***\nraq_07     |     raq_20 |  0.14 | [ 0.10,  0.18] |    7.17 | &lt; .001***\nraq_07     |     raq_21 |  0.20 | [ 0.16,  0.23] |   10.19 | &lt; .001***\nraq_07     |     raq_22 |  0.22 | [ 0.19,  0.26] |   11.69 | &lt; .001***\nraq_07     |     raq_23 |  0.16 | [ 0.12,  0.20] |    8.24 | &lt; .001***\nraq_08     |     raq_09 |  0.16 | [ 0.13,  0.20] |    8.45 | &lt; .001***\nraq_08     |     raq_10 |  0.19 | [ 0.15,  0.22] |    9.58 | &lt; .001***\nraq_08     |     raq_11 |  0.58 | [ 0.56,  0.61] |   36.28 | &lt; .001***\nraq_08     |     raq_12 |  0.13 | [ 0.09,  0.17] |    6.58 | &lt; .001***\nraq_08     |     raq_13 |  0.20 | [ 0.16,  0.24] |   10.38 | &lt; .001***\nraq_08     |     raq_14 |  0.23 | [ 0.19,  0.27] |   12.01 | &lt; .001***\nraq_08     |     raq_15 |  0.23 | [ 0.19,  0.27] |   11.97 | &lt; .001***\nraq_08     |     raq_16 |  0.21 | [ 0.17,  0.24] |   10.69 | &lt; .001***\nraq_08     |     raq_17 |  0.55 | [ 0.52,  0.57] |   33.20 | &lt; .001***\nraq_08     |     raq_18 |  0.28 | [ 0.24,  0.31] |   14.68 | &lt; .001***\nraq_08     |     raq_19 |  0.21 | [ 0.17,  0.25] |   10.86 | &lt; .001***\nraq_08     |     raq_20 |  0.26 | [ 0.23,  0.30] |   13.76 | &lt; .001***\nraq_08     |     raq_21 |  0.30 | [ 0.27,  0.34] |   16.15 | &lt; .001***\nraq_08     |     raq_22 |  0.22 | [ 0.19,  0.26] |   11.68 | &lt; .001***\nraq_08     |     raq_23 |  0.14 | [ 0.10,  0.18] |    7.25 | &lt; .001***\nraq_09     |     raq_10 |  0.11 | [ 0.07,  0.14] |    5.38 | &lt; .001***\nraq_09     |     raq_11 |  0.17 | [ 0.14,  0.21] |    9.01 | &lt; .001***\nraq_09     |     raq_12 |  0.06 | [ 0.02,  0.10] |    3.07 | 0.007**  \nraq_09     |     raq_13 |  0.15 | [ 0.11,  0.19] |    7.84 | &lt; .001***\nraq_09     |     raq_14 |  0.12 | [ 0.09,  0.16] |    6.30 | &lt; .001***\nraq_09     |     raq_15 |  0.15 | [ 0.11,  0.19] |    7.60 | &lt; .001***\nraq_09     |     raq_16 |  0.08 | [ 0.04,  0.12] |    4.19 | &lt; .001***\nraq_09     |     raq_17 |  0.14 | [ 0.10,  0.18] |    7.32 | &lt; .001***\nraq_09     |     raq_18 |  0.15 | [ 0.11,  0.18] |    7.52 | &lt; .001***\nraq_09     |     raq_19 |  0.46 | [ 0.43,  0.49] |   26.59 | &lt; .001***\nraq_09     |     raq_20 |  0.10 | [ 0.06,  0.14] |    5.08 | &lt; .001***\nraq_09     |     raq_21 |  0.17 | [ 0.13,  0.20] |    8.61 | &lt; .001***\nraq_09     |     raq_22 |  0.43 | [ 0.40,  0.46] |   23.95 | &lt; .001***\nraq_09     |     raq_23 |  0.55 | [ 0.52,  0.57] |   33.09 | &lt; .001***\nraq_10     |     raq_11 |  0.17 | [ 0.13,  0.21] |    8.68 | &lt; .001***\nraq_10     |     raq_12 |  0.08 | [ 0.04,  0.11] |    3.85 | 0.001**  \nraq_10     |     raq_13 |  0.25 | [ 0.21,  0.28] |   12.99 | &lt; .001***\nraq_10     |     raq_14 |  0.22 | [ 0.18,  0.26] |   11.38 | &lt; .001***\nraq_10     |     raq_15 |  0.24 | [ 0.21,  0.28] |   12.68 | &lt; .001***\nraq_10     |     raq_16 |  0.13 | [ 0.09,  0.17] |    6.59 | &lt; .001***\nraq_10     |     raq_17 |  0.18 | [ 0.14,  0.21] |    9.12 | &lt; .001***\nraq_10     |     raq_18 |  0.29 | [ 0.26,  0.33] |   15.43 | &lt; .001***\nraq_10     |     raq_19 |  0.21 | [ 0.17,  0.25] |   10.97 | &lt; .001***\nraq_10     |     raq_20 |  0.18 | [ 0.14,  0.22] |    9.23 | &lt; .001***\nraq_10     |     raq_21 |  0.16 | [ 0.12,  0.20] |    8.34 | &lt; .001***\nraq_10     |     raq_22 |  0.16 | [ 0.12,  0.19] |    7.98 | &lt; .001***\nraq_10     |     raq_23 |  0.07 | [ 0.03,  0.11] |    3.53 | 0.003**  \nraq_11     |     raq_12 |  0.10 | [ 0.07,  0.14] |    5.27 | &lt; .001***\nraq_11     |     raq_13 |  0.20 | [ 0.16,  0.24] |   10.40 | &lt; .001***\nraq_11     |     raq_14 |  0.19 | [ 0.15,  0.22] |    9.59 | &lt; .001***\nraq_11     |     raq_15 |  0.20 | [ 0.16,  0.24] |   10.42 | &lt; .001***\nraq_11     |     raq_16 |  0.17 | [ 0.14,  0.21] |    9.00 | &lt; .001***\nraq_11     |     raq_17 |  0.47 | [ 0.43,  0.49] |   26.63 | &lt; .001***\nraq_11     |     raq_18 |  0.24 | [ 0.20,  0.28] |   12.48 | &lt; .001***\nraq_11     |     raq_19 |  0.19 | [ 0.16,  0.23] |   10.04 | &lt; .001***\nraq_11     |     raq_20 |  0.27 | [ 0.23,  0.31] |   14.19 | &lt; .001***\nraq_11     |     raq_21 |  0.29 | [ 0.25,  0.32] |   15.15 | &lt; .001***\nraq_11     |     raq_22 |  0.20 | [ 0.16,  0.24] |   10.44 | &lt; .001***\nraq_11     |     raq_23 |  0.13 | [ 0.09,  0.17] |    6.56 | &lt; .001***\nraq_12     |     raq_13 |  0.07 | [ 0.03,  0.11] |    3.48 | 0.003**  \nraq_12     |     raq_14 |  0.07 | [ 0.03,  0.11] |    3.50 | 0.003**  \nraq_12     |     raq_15 |  0.07 | [ 0.03,  0.11] |    3.63 | 0.002**  \nraq_12     |     raq_16 |  0.15 | [ 0.12,  0.19] |    7.87 | &lt; .001***\nraq_12     |     raq_17 |  0.07 | [ 0.03,  0.11] |    3.71 | 0.002**  \nraq_12     |     raq_18 |  0.10 | [ 0.06,  0.14] |    5.00 | &lt; .001***\nraq_12     |     raq_19 |  0.07 | [ 0.03,  0.11] |    3.40 | 0.003**  \nraq_12     |     raq_20 |  0.12 | [ 0.08,  0.16] |    6.27 | &lt; .001***\nraq_12     |     raq_21 |  0.16 | [ 0.13,  0.20] |    8.42 | &lt; .001***\nraq_12     |     raq_22 |  0.08 | [ 0.04,  0.12] |    4.05 | &lt; .001***\nraq_12     |     raq_23 |  0.05 | [ 0.01,  0.09] |    2.70 | 0.014*   \nraq_13     |     raq_14 |  0.27 | [ 0.24,  0.31] |   14.44 | &lt; .001***\nraq_13     |     raq_15 |  0.29 | [ 0.25,  0.32] |   15.31 | &lt; .001***\nraq_13     |     raq_16 |  0.15 | [ 0.11,  0.19] |    7.66 | &lt; .001***\nraq_13     |     raq_17 |  0.18 | [ 0.14,  0.21] |    9.12 | &lt; .001***\nraq_13     |     raq_18 |  0.34 | [ 0.31,  0.37] |   18.35 | &lt; .001***\nraq_13     |     raq_19 |  0.26 | [ 0.22,  0.30] |   13.64 | &lt; .001***\nraq_13     |     raq_20 |  0.16 | [ 0.12,  0.20] |    8.32 | &lt; .001***\nraq_13     |     raq_21 |  0.17 | [ 0.14,  0.21] |    8.98 | &lt; .001***\nraq_13     |     raq_22 |  0.23 | [ 0.19,  0.27] |   11.97 | &lt; .001***\nraq_13     |     raq_23 |  0.14 | [ 0.10,  0.18] |    7.15 | &lt; .001***\nraq_14     |     raq_15 |  0.23 | [ 0.20,  0.27] |   12.20 | &lt; .001***\nraq_14     |     raq_16 |  0.08 | [ 0.05,  0.12] |    4.29 | &lt; .001***\nraq_14     |     raq_17 |  0.17 | [ 0.13,  0.20] |    8.61 | &lt; .001***\nraq_14     |     raq_18 |  0.26 | [ 0.22,  0.29] |   13.40 | &lt; .001***\nraq_14     |     raq_19 |  0.22 | [ 0.18,  0.26] |   11.49 | &lt; .001***\nraq_14     |     raq_20 |  0.15 | [ 0.11,  0.19] |    7.60 | &lt; .001***\nraq_14     |     raq_21 |  0.17 | [ 0.13,  0.20] |    8.57 | &lt; .001***\nraq_14     |     raq_22 |  0.22 | [ 0.18,  0.26] |   11.43 | &lt; .001***\nraq_14     |     raq_23 |  0.13 | [ 0.09,  0.16] |    6.47 | &lt; .001***\nraq_15     |     raq_16 |  0.11 | [ 0.07,  0.15] |    5.59 | &lt; .001***\nraq_15     |     raq_17 |  0.21 | [ 0.18,  0.25] |   11.06 | &lt; .001***\nraq_15     |     raq_18 |  0.32 | [ 0.29,  0.35] |   17.13 | &lt; .001***\nraq_15     |     raq_19 |  0.23 | [ 0.19,  0.27] |   11.99 | &lt; .001***\nraq_15     |     raq_20 |  0.16 | [ 0.12,  0.20] |    8.12 | &lt; .001***\nraq_15     |     raq_21 |  0.17 | [ 0.13,  0.20] |    8.50 | &lt; .001***\nraq_15     |     raq_22 |  0.24 | [ 0.20,  0.27] |   12.38 | &lt; .001***\nraq_15     |     raq_23 |  0.15 | [ 0.11,  0.19] |    7.71 | &lt; .001***\nraq_16     |     raq_17 |  0.18 | [ 0.14,  0.21] |    9.10 | &lt; .001***\nraq_16     |     raq_18 |  0.12 | [ 0.09,  0.16] |    6.36 | &lt; .001***\nraq_16     |     raq_19 |  0.14 | [ 0.10,  0.17] |    6.93 | &lt; .001***\nraq_16     |     raq_20 |  0.23 | [ 0.20,  0.27] |   12.11 | &lt; .001***\nraq_16     |     raq_21 |  0.26 | [ 0.23,  0.30] |   13.87 | &lt; .001***\nraq_16     |     raq_22 |  0.11 | [ 0.07,  0.15] |    5.58 | &lt; .001***\nraq_16     |     raq_23 |  0.10 | [ 0.06,  0.14] |    5.10 | &lt; .001***\nraq_17     |     raq_18 |  0.25 | [ 0.22,  0.29] |   13.31 | &lt; .001***\nraq_17     |     raq_19 |  0.20 | [ 0.16,  0.23] |   10.17 | &lt; .001***\nraq_17     |     raq_20 |  0.22 | [ 0.18,  0.26] |   11.38 | &lt; .001***\nraq_17     |     raq_21 |  0.26 | [ 0.22,  0.29] |   13.54 | &lt; .001***\nraq_17     |     raq_22 |  0.21 | [ 0.17,  0.24] |   10.71 | &lt; .001***\nraq_17     |     raq_23 |  0.14 | [ 0.10,  0.17] |    7.01 | &lt; .001***\nraq_18     |     raq_19 |  0.30 | [ 0.26,  0.33] |   15.91 | &lt; .001***\nraq_18     |     raq_20 |  0.18 | [ 0.14,  0.22] |    9.24 | &lt; .001***\nraq_18     |     raq_21 |  0.19 | [ 0.16,  0.23] |    9.97 | &lt; .001***\nraq_18     |     raq_22 |  0.27 | [ 0.23,  0.30] |   14.08 | &lt; .001***\nraq_18     |     raq_23 |  0.13 | [ 0.09,  0.17] |    6.65 | &lt; .001***\nraq_19     |     raq_20 |  0.16 | [ 0.13,  0.20] |    8.43 | &lt; .001***\nraq_19     |     raq_21 |  0.18 | [ 0.15,  0.22] |    9.47 | &lt; .001***\nraq_19     |     raq_22 |  0.42 | [ 0.39,  0.45] |   23.34 | &lt; .001***\nraq_19     |     raq_23 |  0.44 | [ 0.41,  0.47] |   24.85 | &lt; .001***\nraq_20     |     raq_21 |  0.35 | [ 0.31,  0.38] |   18.85 | &lt; .001***\nraq_20     |     raq_22 |  0.17 | [ 0.13,  0.21] |    8.65 | &lt; .001***\nraq_20     |     raq_23 |  0.11 | [ 0.08,  0.15] |    5.82 | &lt; .001***\nraq_21     |     raq_22 |  0.18 | [ 0.14,  0.21] |    9.16 | &lt; .001***\nraq_21     |     raq_23 |  0.14 | [ 0.10,  0.18] |    7.28 | &lt; .001***\nraq_22     |     raq_23 |  0.40 | [ 0.37,  0.43] |   22.17 | &lt; .001***\n\np-value adjustment method: Holm (1979)\nObservations: 2571\n\n# pipe into summary() to get a condensed table of correlations:\ncorrelation::correlation(raq_items_tib) |&gt;\n  summary() |&gt; \n  knitr::kable(digits = 2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nParameter\nraq_23\nraq_22\nraq_21\nraq_20\nraq_19\nraq_18\nraq_17\nraq_16\nraq_15\nraq_14\nraq_13\nraq_12\nraq_11\nraq_10\nraq_09\nraq_08\nraq_07\nraq_06\nraq_05\nraq_04\nraq_03\nraq_02\n\n\n\n\nraq_01\n0.08\n0.12\n0.23\n0.21\n0.11\n0.12\n0.17\n0.18\n0.11\n0.12\n0.11\n0.15\n0.19\n0.11\n0.08\n0.22\n0.11\n0.16\n0.22\n0.22\n-0.18\n0.11\n\n\nraq_02\n0.39\n0.34\n0.16\n0.13\n0.39\n0.26\n0.17\n0.10\n0.19\n0.20\n0.23\n0.09\n0.15\n0.18\n0.39\n0.18\n0.26\n0.34\n0.28\n0.12\n-0.12\nNA\n\n\nraq_03\n-0.05\n-0.12\n-0.26\n-0.23\n-0.12\n-0.15\n-0.18\n-0.20\n-0.15\n-0.09\n-0.13\n-0.15\n-0.19\n-0.10\n-0.08\n-0.22\n-0.13\n-0.17\n-0.25\n-0.19\nNA\nNA\n\n\nraq_04\n0.08\n0.16\n0.32\n0.28\n0.14\n0.17\n0.24\n0.27\n0.18\n0.15\n0.16\n0.20\n0.22\n0.18\n0.10\n0.25\n0.17\n0.23\n0.32\nNA\nNA\nNA\n\n\nraq_05\n0.17\n0.27\n0.37\n0.34\n0.29\n0.38\n0.30\n0.24\n0.33\n0.27\n0.35\n0.19\n0.31\n0.31\n0.16\n0.36\n0.36\n0.51\nNA\nNA\nNA\nNA\n\n\nraq_06\n0.17\n0.33\n0.26\n0.23\n0.38\n0.51\n0.28\n0.16\n0.40\n0.36\n0.46\n0.11\n0.29\n0.40\n0.20\n0.34\n0.45\nNA\nNA\nNA\nNA\nNA\n\n\nraq_07\n0.16\n0.22\n0.20\n0.14\n0.26\n0.35\n0.21\n0.12\n0.29\n0.25\n0.31\n0.10\n0.21\n0.28\n0.17\n0.25\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nraq_08\n0.14\n0.22\n0.30\n0.26\n0.21\n0.28\n0.55\n0.21\n0.23\n0.23\n0.20\n0.13\n0.58\n0.19\n0.16\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nraq_09\n0.55\n0.43\n0.17\n0.10\n0.46\n0.15\n0.14\n0.08\n0.15\n0.12\n0.15\n0.06\n0.17\n0.11\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nraq_10\n0.07\n0.16\n0.16\n0.18\n0.21\n0.29\n0.18\n0.13\n0.24\n0.22\n0.25\n0.08\n0.17\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nraq_11\n0.13\n0.20\n0.29\n0.27\n0.19\n0.24\n0.47\n0.17\n0.20\n0.19\n0.20\n0.10\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nraq_12\n0.05\n0.08\n0.16\n0.12\n0.07\n0.10\n0.07\n0.15\n0.07\n0.07\n0.07\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nraq_13\n0.14\n0.23\n0.17\n0.16\n0.26\n0.34\n0.18\n0.15\n0.29\n0.27\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nraq_14\n0.13\n0.22\n0.17\n0.15\n0.22\n0.26\n0.17\n0.08\n0.23\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nraq_15\n0.15\n0.24\n0.17\n0.16\n0.23\n0.32\n0.21\n0.11\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nraq_16\n0.10\n0.11\n0.26\n0.23\n0.14\n0.12\n0.18\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nraq_17\n0.14\n0.21\n0.26\n0.22\n0.20\n0.25\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nraq_18\n0.13\n0.27\n0.19\n0.18\n0.30\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nraq_19\n0.44\n0.42\n0.18\n0.16\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nraq_20\n0.11\n0.17\n0.35\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nraq_21\n0.14\n0.18\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nraq_22\n0.40\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n\n\n\n\nbecause the items use Likert scales, should use polychoric() function from the psych package\n\n\nraq_poly &lt;- psych::polychoric(raq_items_tib)\nraq_poly\n\nCall: psych::polychoric(x = raq_items_tib)\nPolychoric correlations \n       rq_01 rq_02 rq_03 rq_04 rq_05 rq_06 rq_07 rq_08 rq_09 rq_10 rq_11\nraq_01  1.00                                                            \nraq_02  0.12  1.00                                                      \nraq_03 -0.20 -0.13  1.00                                                \nraq_04  0.24  0.14 -0.22  1.00                                          \nraq_05  0.25  0.31 -0.28  0.35  1.00                                    \nraq_06  0.18  0.37 -0.18  0.26  0.57  1.00                              \nraq_07  0.12  0.28 -0.14  0.18  0.40  0.50  1.00                        \nraq_08  0.25  0.20 -0.24  0.28  0.40  0.38  0.28  1.00                  \nraq_09  0.09  0.43 -0.09  0.11  0.18  0.22  0.18  0.18  1.00            \nraq_10  0.12  0.20 -0.11  0.20  0.35  0.44  0.31  0.20  0.12  1.00      \nraq_11  0.21  0.17 -0.21  0.25  0.35  0.33  0.24  0.64  0.19  0.19  1.00\nraq_12  0.17  0.10 -0.16  0.21  0.21  0.13  0.10  0.14  0.07  0.08  0.11\nraq_13  0.12  0.25 -0.14  0.18  0.39  0.50  0.34  0.22  0.17  0.27  0.22\nraq_14  0.13  0.22 -0.10  0.16  0.30  0.39  0.28  0.25  0.14  0.24  0.20\nraq_15  0.12  0.21 -0.17  0.21  0.36  0.45  0.32  0.25  0.16  0.27  0.22\nraq_16  0.19  0.11 -0.22  0.30  0.27  0.18  0.13  0.23  0.09  0.14  0.19\nraq_17  0.18  0.19 -0.19  0.26  0.33  0.31  0.23  0.61  0.16  0.20  0.51\nraq_18  0.13  0.29 -0.17  0.19  0.42  0.56  0.39  0.31  0.16  0.32  0.27\nraq_19  0.12  0.43 -0.13  0.16  0.32  0.42  0.28  0.23  0.51  0.23  0.21\nraq_20  0.23  0.14 -0.26  0.32  0.38  0.25  0.16  0.29  0.11  0.20  0.30\nraq_21  0.25  0.18 -0.29  0.36  0.41  0.29  0.22  0.33  0.19  0.18  0.32\nraq_22  0.14  0.38 -0.13  0.17  0.30  0.36  0.25  0.25  0.48  0.17  0.22\nraq_23  0.09  0.43 -0.06  0.09  0.19  0.19  0.18  0.16  0.60  0.08  0.14\n       rq_12 rq_13 rq_14 rq_15 rq_16 rq_17 rq_18 rq_19 rq_20 rq_21 rq_22\nraq_12  1.00                                                            \nraq_13  0.08  1.00                                                      \nraq_14  0.08  0.30  1.00                                                \nraq_15  0.08  0.32  0.26  1.00                                          \nraq_16  0.17  0.17  0.09  0.12  1.00                                    \nraq_17  0.08  0.20  0.19  0.24  0.20  1.00                              \nraq_18  0.11  0.38  0.28  0.35  0.14  0.28  1.00                        \nraq_19  0.08  0.29  0.24  0.26  0.15  0.22  0.33  1.00                  \nraq_20  0.14  0.18  0.16  0.18  0.26  0.24  0.20  0.18  1.00            \nraq_21  0.18  0.19  0.18  0.18  0.29  0.29  0.21  0.20  0.39  1.00      \nraq_22  0.09  0.26  0.25  0.26  0.12  0.23  0.30  0.46  0.19  0.20  1.00\nraq_23  0.06  0.15  0.14  0.17  0.11  0.15  0.14  0.49  0.13  0.16  0.45\n[1]  1.00\n\n with tau of \n          1     2       3    4\nraq_01 -2.0 -0.99 -0.0229 1.01\nraq_02 -2.0 -1.03  0.0532 1.03\nraq_03 -2.0 -0.99 -0.0093 0.99\nraq_04 -2.0 -0.99  0.0044 1.00\nraq_05 -1.9 -1.00  0.0356 0.97\nraq_06 -2.0 -0.99  0.0132 0.97\nraq_07 -2.0 -1.01 -0.0278 0.95\nraq_08 -2.0 -1.00 -0.0239 1.01\nraq_09 -2.1 -1.02 -0.0288 0.97\nraq_10 -2.0 -0.98  0.0307 0.97\nraq_11 -2.0 -0.98 -0.0219 1.04\nraq_12 -2.0 -1.04 -0.0219 1.00\nraq_13 -2.0 -1.00 -0.0180 1.02\nraq_14 -2.0 -1.00 -0.0424 0.95\nraq_15 -2.0 -1.03 -0.0678 0.95\nraq_16 -2.0 -0.99  0.0063 1.01\nraq_17 -2.0 -1.02  0.0122 1.02\nraq_18 -2.0 -1.03  0.0015 1.02\nraq_19 -2.0 -0.98  0.0112 0.99\nraq_20 -2.0 -0.99 -0.0336 0.96\nraq_21 -2.1 -0.99 -0.0073 1.02\nraq_22 -2.1 -1.05  0.0210 1.03\nraq_23 -2.1 -1.01 -0.0015 1.01\n\n\n\nmatrix of correlations is stored in in a variable called rho, accessible with raq_poly$rho but we can store it in an object\n\n\nraq_cor &lt;- raq_poly$rho\n\n\npsych::cor.plot(raq_cor, upper = FALSE)\n\n\n\n\n\n\n\n\n\nnote items close to 0 - no correlation\nnote items between $+/-$0.3\nnote items greater than \\(+/-\\) 0.9 as those may be collinear or singular\nin this case, all questions correlate reasonably well and none are excessively large"
  },
  {
    "objectID": "posts/dsur_ch18_efa/index.html#bartletts-test-and-kmo-test",
    "href": "posts/dsur_ch18_efa/index.html#bartletts-test-and-kmo-test",
    "title": "dsus_18 - Exploratory Factor Analysis",
    "section": "Bartlett’s test and KMO test",
    "text": "Bartlett’s test and KMO test\n\nBartlett’s test of Sphericity\n\ntests whether the correlation matrix is significantly different from an identity matrix (whether the correlations are all 0\n\nin FA, sample sizes are large, so the test will almost always be significant, but if it is not, then there is a problem\n\n\n\npsych::cortest.bartlett(raq_cor, n = 2571)\n\n$chisq\n[1] 17387.52\n\n$p.value\n[1] 0\n\n$df\n[1] 253\n\n\n\ngiven large sample size, Bartlett’s test is highly significant, indicating there is not a significant problem\n\n\n\nKaiser-Meyer-Olkin (KMO)\n\ncheck for sampling adequacy\nKMO varies between 0 and 1 with 0 indicating FA is not appropriate\nvalues closer to 1 indicate compact patterns of correlations and FA should reveal distinct and reliable factors\n\nMarvellous: values in the 0.90s\nMeritorious: values in the 0.80s\nMiddling: values in the 0.70s\nMediocre: values in the 0.60s\nMiserable: values in the 0.50s\n\n\n\npsych::KMO(raq_cor)\n\nKaiser-Meyer-Olkin factor adequacy\nCall: psych::KMO(r = raq_cor)\nOverall MSA =  0.92\nMSA for each item = \nraq_01 raq_02 raq_03 raq_04 raq_05 raq_06 raq_07 raq_08 raq_09 raq_10 raq_11 \n  0.95   0.94   0.93   0.93   0.95   0.91   0.96   0.86   0.84   0.95   0.89 \nraq_12 raq_13 raq_14 raq_15 raq_16 raq_17 raq_18 raq_19 raq_20 raq_21 raq_22 \n  0.90   0.95   0.96   0.96   0.92   0.90   0.95   0.93   0.93   0.93   0.94 \nraq_23 \n  0.84 \n\n\n\nKMO statistic (Overall MSA) is 0.92 - well above the threshold of 0.5\nMSA for each individual item ranges from 0.84 - 0.96\n\n\n\n\n\n\n\nNote\n\n\n\nIf you find KMO values below 0.5, consider removing that variable, but be sure to run the KMO statistic again without the removed variable. Also run the analysis with and without the variable to compare"
  },
  {
    "objectID": "posts/dsur_ch18_efa/index.html#parallel-analysis",
    "href": "posts/dsur_ch18_efa/index.html#parallel-analysis",
    "title": "dsus_18 - Exploratory Factor Analysis",
    "section": "Parallel analysis",
    "text": "Parallel analysis\n\nto determine how many factors to extract, run psych::fa.parallel()\nmost likely arguments\n\nn.obs() - need to tell the function the sample size (`n.obs = 2571`)\nfm = “minres” - psych packages uses minimum residual (minres) by default\n\nother options include principal axes (pa), alpha factoring (alpha), weighted least squares wls, minimum rank (minrank), or maximum likelihood (ml). Match this option to the one you’re going to use in the main factor analysis\n\nfa = \"both\" - by default the function will tell the number of factors to extract, but also the number of components for PCA.\n\ncan change to fa = \"fa” to see only the number of factors to extract. It is useful to look at both methods\n\nuse = “pairwise” - by default, missing data are handled using all complete pairwise observations to calculate the correlation coefficients\ncor - default is the function assumes you are providing Pearson correlation coefficients, however, with ordinal variables (likert), use cor = \"poly\" (polychoric) and for binary, use tetrachoric cor = \"tet\"; with a mix of variable types, use cor =\"mixed\"\n\n\n\nCode example\n\npsych::fa.parallel(raq_items_tib, cor = \"poly\")\n\n\n\n\n\n\n\n\nParallel analysis suggests that the number of factors =  4  and the number of components =  4 \n\n\n\nor since we already stored polychoric correlations in raq_cor, we can just apply the function to that correlation matrix and specify the sample size\n\n\npsych::fa.parallel(raq_cor, n.obs = 2571, fa = \"fa\")\n\n\n\n\n\n\n\n\nParallel analysis suggests that the number of factors =  4  and the number of components =  NA \n\n\n\neigenvalues represent the size of the factor\nfactors are plotted on the x-axis with the eigenvalues on the y-axis\neach eigenvalue is compared to an eigenvalue from a simulated data set that has no underlying factors\n\nessentially, we are asking if the factors are bigger than imaginary factor\nfactors that are bigger than their imaginary counterparts are retained\n\neigenvalues for the observed factors are blue triangles connected by a blue line.\nthe red line shows corresponding simulated data.\nwe keep the number of factors that are above the red line, in this case, four\n\n\n\nQuiz\n\n\n\n\n\n\nBased on the parallel analysis that used principal components to compute the eiegenvalues, how many factors should be extracted?\n\n\n\n\n\n\n4\n\nYes, fortunately this analysis agrees with the parallel analysis based on eigenvalues from factor analysis."
  },
  {
    "objectID": "posts/dsur_ch18_efa/index.html#factor-analysis",
    "href": "posts/dsur_ch18_efa/index.html#factor-analysis",
    "title": "dsus_18 - Exploratory Factor Analysis",
    "section": "Factor Analysis",
    "text": "Factor Analysis\n\nwe are now ready to run the FA, extracting four factors, using the psych::fa() function\n\nmy_fa_object &lt;- psych::fa(r,    \n                          n.obs = 2571,  \n                          nfactors = 1,  \n                          fm = \"minres\", \n                          rotate = \"oblimin\", \n                          scores = \"regression\", \n                          max.iter = 50, \n                          use = \"pairwise\", \n                          cor = \"cor\" \n                          )\n\nr -&gt; the data being fed into the function (raq_items_tib or raq_cor)\nn.obs = 2571 as with parallel analysis, if we run the FA from the correlation matrix instead of the raw data, we must tell the function the sample size\nnfactors = 1 the number of factors to extract (default is 1)\nfm = \"minres\" method of factor analysis, typically leave the default\nrotate = \"oblimin\" method of factor rotation\nscores = \"regression\" method of computing factor scores. because we should use oblique rotation, change this argument to scores = \"tenBerge\"\nmax.iter = 50 number of iterations. If you get a n error message about convergence, increase this number\nuse = \"pairwise\" determines how missing values are treated, default is fine\ncor = \"cor\" same as defined for fa.parallel()\n\n\n\n\n\n\n\nFactor rotation\n\n\n\nfactor rotation requires GPArotation package loaded\n\n\n\nCode Example\n\nas with parallel analysis we can either feed the raw data into the function remembering to set cor = \"poly\" so the analysis is based on polychoric correlations\nor we can feed in the correlation matrix and sample size\n\n\nraq_fa &lt;- psych::fa(raq_items_tib, \n  nfactors = 4, \n  scores = \"tenBerge\", \n  cor = \"poly\"\n  )\n  raq_fa \n\nFactor Analysis using method =  minres\nCall: psych::fa(r = raq_items_tib, nfactors = 4, scores = \"tenBerge\", \n    cor = \"poly\")\nStandardized loadings (pattern matrix) based upon correlation matrix\n         MR1   MR2   MR4   MR3   h2   u2 com\nraq_01 -0.03  0.01  0.39  0.06 0.17 0.83 1.1\nraq_02  0.25  0.48  0.02 -0.03 0.38 0.62 1.5\nraq_03  0.00  0.01 -0.43 -0.03 0.20 0.80 1.0\nraq_04  0.03 -0.02  0.56  0.00 0.33 0.67 1.0\nraq_05  0.45 -0.01  0.39  0.02 0.54 0.46 2.0\nraq_06  0.84  0.00 -0.01  0.03 0.73 0.27 1.0\nraq_07  0.56  0.04  0.00  0.04 0.35 0.65 1.0\nraq_08  0.00 -0.01 -0.01  0.88 0.75 0.25 1.0\nraq_09 -0.07  0.81  0.00  0.03 0.62 0.38 1.0\nraq_10  0.49 -0.05  0.09 -0.02 0.26 0.74 1.1\nraq_11 -0.01  0.01  0.03  0.72 0.55 0.45 1.0\nraq_12 -0.01  0.01  0.37 -0.07 0.11 0.89 1.1\nraq_13  0.57  0.03  0.04 -0.03 0.34 0.66 1.0\nraq_14  0.42  0.04  0.01  0.06 0.22 0.78 1.1\nraq_15  0.48  0.03  0.03  0.05 0.29 0.71 1.0\nraq_16 -0.05  0.02  0.51 -0.01 0.23 0.77 1.0\nraq_17  0.03  0.02  0.00  0.68 0.49 0.51 1.0\nraq_18  0.63  0.01 -0.02  0.07 0.43 0.57 1.0\nraq_19  0.26  0.56  0.00 -0.01 0.50 0.50 1.4\nraq_20  0.00  0.01  0.54  0.05 0.32 0.68 1.0\nraq_21 -0.02  0.05  0.59  0.06 0.40 0.60 1.0\nraq_22  0.19  0.52  0.03  0.05 0.41 0.59 1.3\nraq_23 -0.08  0.79  0.02 -0.01 0.59 0.41 1.0\n\n                       MR1  MR2  MR4  MR3\nSS loadings           3.04 2.24 2.03 1.91\nProportion Var        0.13 0.10 0.09 0.08\nCumulative Var        0.13 0.23 0.32 0.40\nProportion Explained  0.33 0.24 0.22 0.21\nCumulative Proportion 0.33 0.57 0.79 1.00\n\n With factor correlations of \n     MR1  MR2  MR4  MR3\nMR1 1.00 0.38 0.50 0.48\nMR2 0.38 1.00 0.28 0.28\nMR4 0.50 0.28 1.00 0.57\nMR3 0.48 0.28 0.57 1.00\n\nMean item complexity =  1.1\nTest of the hypothesis that 4 factors are sufficient.\n\ndf null model =  253  with the objective function =  6.79 with Chi Square =  17387.52\ndf of  the model are 167  and the objective function was  0.1 \n\nThe root mean square of the residuals (RMSR) is  0.01 \nThe df corrected root mean square of the residuals is  0.02 \n\nThe harmonic n.obs is  2571 with the empirical chi square  205.03  with prob &lt;  0.024 \nThe total n.obs was  2571  with Likelihood Chi Square =  267.21  with prob &lt;  1.3e-06 \n\nTucker Lewis Index of factoring reliability =  0.991\nRMSEA index =  0.015  and the 90 % confidence intervals are  0.012 0.019\nBIC =  -1044.08\nFit based upon off diagonal values = 1\nMeasures of factor score adequacy             \n                                                   MR1  MR2  MR4  MR3\nCorrelation of (regression) scores with factors   0.93 0.91 0.88 0.92\nMultiple R square of scores with factors          0.87 0.83 0.77 0.85\nMinimum correlation of possible factor scores     0.73 0.66 0.54 0.70\n\n\n\nfactors are labeled MR1, MR2, MR3, and MR4\nbelow the pattern matrix is information about how much variance each factor accounts for\n\nProportion var - MR1 accounts for 0.13 of the overall variance (13%), etc\nCumulative var is the proportion of the variance explained cumulatively by the factors - MR1 accounts for 0.13 and MR1 + MR2 together account for 0.13 + 0.10 = 0.23 (23%)\n\nall four together account for 0.40 (40%)\n\nProportion explained is the proportion of the explained variance that is explained by a factor, so of the 40% of the variiance accounted for, 0.33 (33%) is attributable to MR1\n\nnext, correlations between factors are displayed\n\nall are non-zero, indicating the factors are correlated (and oblique rotation was appropriate)\nall factors are positively and fairly strongly correlated to teach other meaning the latent constructs represented by the factors are related.\n\nseveral fit indices that tell us how the model fits the data\n\nchi-square statistic for the model is given as the likelihood chi square, \\(X^2=267.21, p&lt;0.001\\)\n\nwe want this to be non-significant but ours is highly significant. Our sample size is 2571 so small deviations from a good fit will be significant. This highlights the limitation of using significance to indicate model fit.\nthe Tucker Lewis Index of factoring reliability (TLI) is given as 0.991\nRMSEA is 0.015 90% CI[0.012, 0.019]\nRMSR is 0.01\n\n\n\n\n\n\n\n\n\nFit Indices\n\n\n\nGood fit is (probably) indicated by\n\ncombination of TLI &gt; 0.96, and SRMR (RMSR in the output) &lt; 0.06\ncombination of RMSEA &lt; 0.05 and SRMR &lt; 0.09\n\nThe TLI is 0.99, which is greater than 0.96, and RMSR is 0.01 which is smaller than both 0.09 and 0.06. Furthermore, RMSEA is 0.015, which is less than 0.05. With the caveat that universal cut-offs need to be taken with a pinch of salt, it’s reasonable to conclude that the model has excellent fit.\n\n\n\n\nInterpreting FA\n\nlook at factor loadings (top of output in the pattern matrix) for each question on each factor to see which items load most heavily onto which factors\nthis is difficult to interpret in the raw form, so use parameters::model_parameters() to sort items by their factor loadings and suppress factor loading sbelow a certain value\n\nGeneral form\nparameters::model_parameters(my_fa_object, sort = TRUE, threshold = \"max\")\n\nmy_fa_object is the factor analysis object containing the factor loadings.\nsort = \"TRUE\" sorts items by their factor loadings\nset threshold to a value above which we will show values. Default is maximum loading. To see all factor loadings, set threshold = NULL\n\n\nparameters::model_parameters(raq_fa, sort = TRUE, threshold = \"0.2\") |&gt;\n  knitr::kable(digits = 2)\n\n\n\n\nVariable\nMR1\nMR2\nMR4\nMR3\nComplexity\nUniqueness\n\n\n\n\nraq_06\n0.84\nNA\nNA\nNA\n1.00\n0.27\n\n\nraq_18\n0.63\nNA\nNA\nNA\n1.03\n0.57\n\n\nraq_13\n0.57\nNA\nNA\nNA\n1.02\n0.66\n\n\nraq_07\n0.56\nNA\nNA\nNA\n1.02\n0.65\n\n\nraq_10\n0.49\nNA\nNA\nNA\n1.08\n0.74\n\n\nraq_15\n0.48\nNA\nNA\nNA\n1.04\n0.71\n\n\nraq_05\n0.45\nNA\n0.39\nNA\n1.97\n0.46\n\n\nraq_14\n0.42\nNA\nNA\nNA\n1.06\n0.78\n\n\nraq_09\nNA\n0.81\nNA\nNA\n1.02\n0.38\n\n\nraq_23\nNA\n0.79\nNA\nNA\n1.02\n0.41\n\n\nraq_19\n0.26\n0.56\nNA\nNA\n1.41\n0.50\n\n\nraq_22\nNA\n0.52\nNA\nNA\n1.29\n0.59\n\n\nraq_02\n0.25\n0.48\nNA\nNA\n1.54\n0.62\n\n\nraq_21\nNA\nNA\n0.59\nNA\n1.04\n0.60\n\n\nraq_04\nNA\nNA\n0.56\nNA\n1.01\n0.67\n\n\nraq_20\nNA\nNA\n0.54\nNA\n1.02\n0.68\n\n\nraq_16\nNA\nNA\n0.51\nNA\n1.02\n0.77\n\n\nraq_03\nNA\nNA\n-0.43\nNA\n1.01\n0.80\n\n\nraq_01\nNA\nNA\n0.39\nNA\n1.06\n0.83\n\n\nraq_12\nNA\nNA\n0.37\nNA\n1.07\n0.89\n\n\nraq_08\nNA\nNA\nNA\n0.88\n1.00\n0.25\n\n\nraq_11\nNA\nNA\nNA\n0.72\n1.00\n0.45\n\n\nraq_17\nNA\nNA\nNA\n0.68\n1.00\n0.51\n\n\n\n\n\n\nNow we can see patterns in the questions that load onto the same factors\nItems that load highly on MR1 seem to be items that relate to fear of computers\n\nraq_05: I don’t understand statistics (also loads highly onto MR4)\nraq_06: I have little experience of computers\nraq_07: All computers hate me\nraq_10: Computers are useful only for playing games\nraq_13: I worry that I will cause irreparable damage because of my incompetence with computers\nraq_14: Computers have minds of their own and deliberately go wrong whenever I use them\nraq_15: Computers are out to get me\nraq_18: R always crashes when I try to use it\n\nItems that load onto MR2 relate to fear of peer/social evaluation\n\nraq_02: My friends will think I’m stupid for not being able to cope with R\nraq_09: My friends are better at statistics than me\nraq_19: Everybody looks at me when I use R\nraq_22: My friends are better at R than I am\nraq_23: If I am good at statistics people will think I am a nerd\n\nQuestions that load onto MR4 relate to fear of statistics\n\nraq_01: Statistics make me cry\nraq_03: Standard deviations excite me\nraq_04: I dream that Pearson is attacking me with correlation coefficients\nraq_05: I don’t understand statistics\nraq_12: People try to tell you that R makes statistics easier to understand but it doesn’t\nraq_16: I weep openly at the mention of central tendency\nraq_20: I can’t sleep for thoughts of eigenvectors\nraq_21: I wake up under my duvet thinking that I am trapped under a normal distribution\n\nquestions that load onto MR3 relate to fear of math\n\nraq_08: I have never been good at mathematics\nraq_11: I did badly at mathematics at school\nraq_17: I slip into a coma whenever I see an equation\n\n\nAnalysis seems to reveal that the questionnaire is composed of four subscales: fear of statistics, fear of computers, fear of maths, fear of negative peer evaluation.\n\ntwo possibilities:\n\nthe RAQ failed to measure what it set out to measure -&gt; R anxiety but instead measures related constructs\nthese four constructs are subcomponents of R anxiety\n\nHowever, the factor analysis does not indicate which of these is true."
  },
  {
    "objectID": "posts/dsur_ch18_efa/index.html#reliability-analysis",
    "href": "posts/dsur_ch18_efa/index.html#reliability-analysis",
    "title": "dsus_18 - Exploratory Factor Analysis",
    "section": "Reliability Analysis",
    "text": "Reliability Analysis\n\nMcDonald’s \\(\\omega_t\\) and \\(\\omega_h\\)\n\nif items are all scored in the same direction, we can select the variables on a particular subscale and pipe them into omega() functions in the psych package\n\nmy_omg &lt;- psych::omega(\n    my_tibble\n    nfactors = 1,\n    fm = \"minres\",\n    key = c(1, 1, -1, 1, 1 … 1),\n    rotate = \"oblimin\",\n    poly = FALSE\n  )\n\nmy_omg\n\nname to give the object that stores the results\n\nmy_tibble\n\nname of the tibble containing your data\n\n\n\n\nCode Example\n\nneed to recreate the original FA\narguments will be the same as they were set for the fa\nthe key argument allows us to reverse item scoring on the fly\nsupply the key argument with a vector of 1s and -1s that is the same length as the number of variables being fed into the omega() function\nfor RAQ, we have 23 items with the third being reverse coded\nentering the items in order into omega() gives\n\nkey = c(1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)\nor\nkey = c(1, 1, -1, rep(1, 20))\n\nraq_omg &lt;- psych::omega(raq_items_tib,\n                        nfactors = 4,\n                        fm = \"minres\",\n                        key = c(1, 1, -1, rep(1, 20)),\n                        poly = TRUE\n                        )\n\n\n\n\n\n\n\nraq_omg\n\nOmega \nCall: omegah(m = m, nfactors = nfactors, fm = fm, key = key, flip = flip, \n    digits = digits, title = title, sl = sl, labels = labels, \n    plot = plot, n.obs = n.obs, rotate = rotate, Phi = Phi, option = option, \n    covar = covar)\nAlpha:                 0.88 \nG.6:                   0.89 \nOmega Hierarchical:    0.68 \nOmega H asymptotic:    0.75 \nOmega Total            0.9 \n\nSchmid Leiman Factor loadings greater than  0.2 \n           g   F1*   F2*   F3*   F4*   h2   u2   p2\nraq_01  0.32              0.26       0.17 0.83 0.57\nraq_02  0.37        0.43             0.38 0.62 0.37\nraq_03- 0.34              0.29       0.20 0.80 0.56\nraq_04  0.43              0.38       0.33 0.67 0.56\nraq_05  0.62  0.32        0.27       0.54 0.46 0.70\nraq_06  0.61  0.60                   0.73 0.27 0.51\nraq_07  0.44  0.39                   0.35 0.65 0.54\nraq_08  0.62                    0.61 0.75 0.25 0.51\nraq_09  0.32        0.73             0.62 0.38 0.17\nraq_10  0.37  0.35                   0.26 0.74 0.53\nraq_11  0.54                    0.50 0.55 0.45 0.54\nraq_12  0.22              0.25       0.11 0.89 0.44\nraq_13  0.42  0.40                   0.34 0.66 0.51\nraq_14  0.36  0.30                   0.22 0.78 0.59\nraq_15  0.41  0.34                   0.29 0.71 0.59\nraq_16  0.34              0.34       0.23 0.77 0.49\nraq_17  0.52                    0.47 0.49 0.51 0.55\nraq_18  0.48  0.44                   0.43 0.57 0.54\nraq_19  0.43        0.51             0.50 0.50 0.37\nraq_20  0.43              0.36       0.32 0.68 0.58\nraq_21  0.49              0.40       0.40 0.60 0.59\nraq_22  0.41        0.46             0.41 0.59 0.41\nraq_23  0.30        0.71             0.59 0.41 0.15\n\nWith Sums of squares  of:\n   g  F1*  F2*  F3*  F4* \n4.40 1.39 1.71 0.85 0.86 \n\ngeneral/max  2.58   max/min =   2.01\nmean percent general =  0.49    with sd =  0.13 and cv of  0.26 \nExplained Common Variance of the general factor =  0.48 \n\nThe degrees of freedom are 167  and the fit is  0.1 \nThe number of observations was  2571  with Chi Square =  267.21  with prob &lt;  1.3e-06\nThe root mean square of the residuals is  0.01 \nThe df corrected root mean square of the residuals is  0.02\nRMSEA index =  0.015  and the 10 % confidence intervals are  0.012 0.019\nBIC =  -1044.08\n\nCompare this with the adequacy of just a general factor and no group factors\nThe degrees of freedom for just the general factor are 230  and the fit is  2.39 \nThe number of observations was  2571  with Chi Square =  6122.42  with prob &lt;  0\nThe root mean square of the residuals is  0.1 \nThe df corrected root mean square of the residuals is  0.11 \n\nRMSEA index =  0.1  and the 10 % confidence intervals are  0.098 0.102\nBIC =  4316.45 \n\nMeasures of factor score adequacy             \n                                                 g  F1*  F2*   F3*  F4*\nCorrelation of scores with factors            0.84 0.76 0.87  0.67 0.74\nMultiple R square of scores with factors      0.71 0.57 0.75  0.44 0.55\nMinimum correlation of factor score estimates 0.42 0.14 0.51 -0.11 0.10\n\n Total, General and Subset omega for each subset\n                                                 g  F1*  F2*  F3*  F4*\nOmega total for total scores and subscales    0.90 0.83 0.80 0.69 0.81\nOmega general for total scores and subscales  0.68 0.48 0.23 0.38 0.43\nOmega group for total scores and subscales    0.18 0.35 0.56 0.31 0.38\n\n\n\nnote the table of factor loadings and that raq_03- is labeled with a minus to indicate it is reverse coded\ncolumn labeled gshows the loading of each item on the general factor\n\nif any items load poorly, then a common factor model isn’t appropriate\nall items here have a factor loading far enough from zero that a general factor model is appropriate\n\nColumns F1-F4 show the loadings of each item on the four factors we extracted\n\nthese values differ from the original factor analysis because this one includes a general factor\nthe patterns of item loadings to factors follow the same pattern as the original FA\n\nat the top of the text output are reliability estimates for the general factor, including\n\nChronbach’s \\(\\alpha=0.88\\)\n\\(\\omega_h=0.68\\)\n\\(\\omega_t=0.9\\)\n\n\\(\\omega_h\\) is a measure of how much the items reflect a single construct, and a value of 0.68 suggests they do but there is still a lot of unexplained variance in the general factor\nfurther down in the output, we see that only 48% of the variance in the common factor is explained by the items\n\\(\\omega_t\\) is the total reliability and 0.90 is high, suggesting scores are reliable\nnext are two sets of model fit stats\nthe first set are for the model that has four factors and repeat the nformation from the original EFA\n\nchi-square is significant, \\(X^2= 267.21, p&lt;0.001\\), which is a bad thing but unsurprising given the sample size\nRMSR = 0.01\nRMSEA = 0.02 90% CI[0.01-0.02]\n\nNext is the same information but for the model that contains only the general factor and not the four sub-factors\n\nthe fit gets worse as shown by\n\nlarger and more significant chi-square, $X^2=6122.42, p&lt;0.001$\nlarger RMSR=0.10\nlarger RMSEA=0.1090% CI[0.10, 0.10]\n\n\nall this tells us that the model that characterizes the RAQ in terms of four factors is a better fit of the data than a model that characterises it as a single factor\nfinally, under the Total, General, and Subset omega for each subset, we get the \\(\\omega_t\\) (labeled omega total) and \\(\\omega_h\\) (labeled omega general) for the general factor (column g) and for the subfactors.\nvalues for the general factor repeat the information at tht e start of the output\nthe values for the subfactors are particularly relvant for \\(\\omega_t\\) because it represents the total reliability of the scores, so these values tell us the total reliability of the scores from the underlying subscales\n\nfor anxiety related to computers (F1) we have \\(\\omega_t=0.83\\)\nanxiety around peer or social evaluation (F2) $_t=0.80\nanxiety around statistics (F3) \\(\\omega_t=0.69\\)\nanxiety around maths (F4) \\(\\omega_t=0.81\\)\n\nscores from ach subscale are reliable, although somewhat less so for anxiety around statistics"
  },
  {
    "objectID": "posts/dsur_ch18_efa/index.html#cronbachs-alpha",
    "href": "posts/dsur_ch18_efa/index.html#cronbachs-alpha",
    "title": "dsus_18 - Exploratory Factor Analysis",
    "section": "Cronbach’s \\(\\alpha\\)",
    "text": "Cronbach’s \\(\\alpha\\)\n\nLots of reasons not to use Cronbach’s \\(\\alpha\\) (see the book for details)\nif Cronbach’s \\(\\alpha\\) is needed, it must be computed on the individual subscales\n\nAnxiety related to computers: raq_06, raq_07, raq_10, raq_13, raq_14, raq_15 and raq_18\nAnxiety around peer or social evaluation: raq_02, raq_09, raq_19, raq_22, and raq_2\nAnxiety around statistics: raq_01, raq_03 (reverse scored), raq_04, raq_05, raq_12, raq_16, raq_20, and raq_21\nAnxiety around maths: raq_08, raq_11 and raq_17\n\npipe the variables for each subscale into psych::alpha() function\n\n\nCode example for fear of computers\n\nexample excludes raq_05, which makes more sense on the fear of statistics factor [See github issue]\n\n\nraq_tib |&gt; \n  dplyr::select(raq_06, raq_07, raq_10, raq_13, raq_14, raq_15, raq_18) |&gt; \n  psych::alpha()\n\n\nReliability analysis   \nCall: psych::alpha(x = dplyr::select(raq_tib, raq_06, raq_07, raq_10, \n    raq_13, raq_14, raq_15, raq_18))\n\n  raw_alpha std.alpha G6(smc) average_r S/N    ase mean   sd median_r\n      0.77      0.77    0.75      0.32 3.3 0.0069  3.5 0.64     0.29\n\n    95% confidence boundaries \n         lower alpha upper\nFeldt     0.75  0.77  0.78\nDuhachek  0.76  0.77  0.78\n\n Reliability if an item is dropped:\n       raw_alpha std.alpha G6(smc) average_r S/N alpha se  var.r med.r\nraq_06      0.70      0.70    0.66      0.28 2.3   0.0092 0.0015  0.28\nraq_07      0.74      0.74    0.71      0.32 2.9   0.0079 0.0075  0.29\nraq_10      0.75      0.75    0.73      0.34 3.1   0.0075 0.0068  0.32\nraq_13      0.74      0.74    0.71      0.32 2.9   0.0079 0.0074  0.29\nraq_14      0.76      0.76    0.73      0.35 3.2   0.0073 0.0065  0.32\nraq_15      0.75      0.75    0.72      0.33 3.0   0.0076 0.0076  0.31\nraq_18      0.73      0.73    0.70      0.31 2.7   0.0082 0.0063  0.29\n\n Item statistics \n          n raw.r std.r r.cor r.drop mean   sd\nraq_06 2571  0.79  0.79  0.77   0.68  3.5 0.99\nraq_07 2571  0.65  0.65  0.56   0.49  3.5 1.00\nraq_10 2571  0.59  0.59  0.48   0.42  3.5 1.00\nraq_13 2571  0.64  0.64  0.55   0.48  3.5 0.98\nraq_14 2571  0.57  0.57  0.45   0.39  3.5 1.00\nraq_15 2571  0.61  0.61  0.51   0.44  3.5 0.99\nraq_18 2571  0.67  0.68  0.60   0.53  3.5 0.97\n\nNon missing response frequency for each item\n          1    2    3    4    5 miss\nraq_06 0.02 0.14 0.35 0.33 0.17    0\nraq_07 0.02 0.13 0.33 0.34 0.17    0\nraq_10 0.02 0.14 0.35 0.32 0.17    0\nraq_13 0.02 0.14 0.33 0.35 0.15    0\nraq_14 0.02 0.13 0.33 0.35 0.17    0\nraq_15 0.02 0.13 0.32 0.35 0.17    0\nraq_18 0.02 0.13 0.35 0.35 0.15    0\n\n\n\nvalue at the top is Cronbach’s \\(\\alpha\\), with the 95% CI below.\nlooking for between 0.70 and 0.80, in this case Cronbach’s \\(\\alpha\\)=0.77 [0.75, 0.78] indicating good reliability\nnext is a table of statistics for the scale if we deleted each item in turn\nthe values in the column raw_alpha are the values of the overall \\(\\alpha\\)\nwe are looking for a change in Cronbach’s \\(\\alpha\\) (0.77)\n\nif values are greater than 0.77, then reliability would have improved if the item were removed - not the case here\n\nthe table labeled item statistics shows, in raw.r, the correlations between each item and the total score from the scale - item-total correlations\n\nthere is a problem with this statistic in that the item is included in the scale total, which inflates the overall correlation.\nwe want these correlations to be computed without the item in question, and these values are in r.drop\nin a reliable scale all items should correlate with the total, so we’re looking for items that don’t correlate with the overall score from the subscale. If any values of r.drop are elss than 0.3, we have problems becasuse that means an item does not correlate well with the subscale\n\n0.3 is ‘reasonable’ - use your judgement\n\n\nthe final table tells us what percentage of people gave each response to each of the items, which is useful to make sure everyone in the sample is not giving the same response.\n\nusually, if everyone gives the same response, the item will have poor reliability statistics\nfor this subscale, few people responded with a 1 on any of the items suggesting that no one is feeling the love for computers or that the items are doing a poor job of eliciting those extreme responses\n\n\n\n\nCode example for fear of peer/social evaluation\n\nraq_tib |&gt; \n  dplyr::select(raq_02, raq_09, raq_19, raq_22, raq_23)  |&gt; \n  psych::alpha()\n\n\nReliability analysis   \nCall: psych::alpha(x = dplyr::select(raq_tib, raq_02, raq_09, raq_19, \n    raq_22, raq_23))\n\n  raw_alpha std.alpha G6(smc) average_r S/N    ase mean   sd median_r\n      0.78      0.78    0.75      0.42 3.6 0.0067  3.5 0.72     0.41\n\n    95% confidence boundaries \n         lower alpha upper\nFeldt     0.77  0.78   0.8\nDuhachek  0.77  0.78   0.8\n\n Reliability if an item is dropped:\n       raw_alpha std.alpha G6(smc) average_r S/N alpha se  var.r med.r\nraq_02      0.77      0.77    0.71      0.45 3.3   0.0075 0.0027  0.43\nraq_09      0.72      0.72    0.67      0.40 2.6   0.0089 0.0011  0.40\nraq_19      0.74      0.74    0.69      0.42 2.8   0.0084 0.0049  0.40\nraq_22      0.76      0.76    0.71      0.44 3.1   0.0078 0.0038  0.42\nraq_23      0.73      0.73    0.67      0.41 2.7   0.0086 0.0018  0.40\n\n Item statistics \n          n raw.r std.r r.cor r.drop mean   sd\nraq_02 2571  0.68  0.69  0.55   0.49  3.5 0.97\nraq_09 2571  0.77  0.77  0.70   0.62  3.5 0.99\nraq_19 2571  0.74  0.74  0.64   0.57  3.5 1.00\nraq_22 2571  0.70  0.71  0.59   0.52  3.5 0.96\nraq_23 2571  0.76  0.76  0.68   0.60  3.5 0.98\n\nNon missing response frequency for each item\n          1    2    3    4    5 miss\nraq_02 0.02 0.13 0.37 0.33 0.15    0\nraq_09 0.02 0.13 0.33 0.34 0.17    0\nraq_19 0.02 0.14 0.34 0.34 0.16    0\nraq_22 0.02 0.13 0.36 0.34 0.15    0\nraq_23 0.02 0.14 0.34 0.34 0.16    0\n\n\n\ngood overall reliability (\\(\\alpha=0.78 [o.77, 0.80]\\))\nno items improve the value if they are dropped\nitem correlations with the total are all good\nagain we have issues with the items not eliciting extremely low responses\n\n\n\nCode example for fear of maths\n\nraq_tib |&gt; \n  dplyr::select(raq_08, raq_11, raq_17)  |&gt; \n  psych::alpha()\n\n\nReliability analysis   \nCall: psych::alpha(x = dplyr::select(raq_tib, raq_08, raq_11, raq_17))\n\n  raw_alpha std.alpha G6(smc) average_r S/N    ase mean   sd median_r\n      0.77      0.77     0.7      0.53 3.4 0.0078  3.5 0.81     0.55\n\n    95% confidence boundaries \n         lower alpha upper\nFeldt     0.76  0.77  0.79\nDuhachek  0.76  0.77  0.79\n\n Reliability if an item is dropped:\n       raw_alpha std.alpha G6(smc) average_r S/N alpha se var.r med.r\nraq_08      0.63      0.63    0.47      0.47 1.7    0.014    NA  0.47\nraq_11      0.71      0.71    0.55      0.55 2.4    0.012    NA  0.55\nraq_17      0.74      0.74    0.58      0.58 2.8    0.010    NA  0.58\n\n Item statistics \n          n raw.r std.r r.cor r.drop mean   sd\nraq_08 2571  0.86  0.86  0.75   0.66  3.5 0.98\nraq_11 2571  0.82  0.82  0.68   0.60  3.5 0.99\nraq_17 2571  0.81  0.81  0.65   0.57  3.5 0.97\n\nNon missing response frequency for each item\n          1    2    3    4    5 miss\nraq_08 0.02 0.14 0.33 0.35 0.16    0\nraq_11 0.02 0.14 0.33 0.36 0.15    0\nraq_17 0.02 0.13 0.35 0.34 0.15    0\n\n\n\nfairly high reliability (\\(\\alpha=0.77 [0.76, 0.79]\\))\nno items improve value if they are dropped\nitem correlations with the total subscale are all good\nstill issues with items not eliciting low responses\n\n\n\nCode example for fear of statistics\n\nthe fear of statistics subscale contains raq_3, which is reverse scored so we need to include the keys argument within the function\nnote that in the omega() function, the argument is key, but in the alpha() function, the argument is keys\n\n\n\nraq_tib |&gt; \n  dplyr::select(raq_01, raq_03, raq_04, raq_05, raq_12, raq_16, raq_20, raq_21)  |&gt; \n  psych::alpha(keys = c(1, -1, 1, 1, 1, 1, 1, 1))\n\n\nReliability analysis   \nCall: psych::alpha(x = dplyr::select(raq_tib, raq_01, raq_03, raq_04, \n    raq_05, raq_12, raq_16, raq_20, raq_21), keys = c(1, -1, \n    1, 1, 1, 1, 1, 1))\n\n  raw_alpha std.alpha G6(smc) average_r S/N    ase mean   sd median_r\n      0.71      0.54    0.56      0.13 1.2 0.0087  3.4 0.57      0.2\n\n    95% confidence boundaries \n         lower alpha upper\nFeldt     0.69  0.71  0.72\nDuhachek  0.69  0.71  0.72\n\n Reliability if an item is dropped:\n        raw_alpha std.alpha G6(smc) average_r  S/N alpha se  var.r med.r\nraq_01       0.69      0.49    0.52      0.12 0.95   0.0092 0.0520  0.20\nraq_03-      0.69      0.69    0.66      0.24 2.19   0.0094 0.0051  0.23\nraq_04       0.67      0.44    0.48      0.10 0.79   0.0100 0.0461  0.18\nraq_05       0.66      0.44    0.47      0.10 0.78   0.0102 0.0421  0.18\nraq_12       0.71      0.51    0.54      0.13 1.04   0.0088 0.0542  0.23\nraq_16       0.68      0.48    0.51      0.11 0.91   0.0095 0.0496  0.20\nraq_20       0.67      0.46    0.49      0.11 0.84   0.0099 0.0447  0.19\nraq_21       0.66      0.44    0.47      0.10 0.78   0.0103 0.0415  0.19\n\n Item statistics \n           n raw.r std.r r.cor r.drop mean   sd\nraq_01  2571  0.52  0.52  0.39   0.33  3.5 0.99\nraq_03- 2571  0.54 -0.12 -0.46   0.36  2.5 0.99\nraq_04  2571  0.61  0.62  0.56   0.45  3.5 0.99\nraq_05  2571  0.64  0.62  0.58   0.48  3.5 1.00\nraq_12  2571  0.46  0.47  0.31   0.27  3.5 0.98\nraq_16  2571  0.55  0.55  0.44   0.37  3.5 0.98\nraq_20  2571  0.61  0.59  0.52   0.44  3.5 1.00\nraq_21  2571  0.64  0.63  0.58   0.49  3.5 0.98\n\nNon missing response frequency for each item\n          1    2    3    4    5 miss\nraq_01 0.02 0.14 0.33 0.35 0.16    0\nraq_03 0.02 0.14 0.34 0.34 0.16    0\nraq_04 0.02 0.14 0.34 0.34 0.16    0\nraq_05 0.03 0.13 0.35 0.32 0.16    0\nraq_12 0.02 0.12 0.34 0.35 0.16    0\nraq_16 0.02 0.14 0.34 0.34 0.16    0\nraq_20 0.02 0.14 0.33 0.35 0.17    0\nraq_21 0.02 0.14 0.34 0.35 0.15    0\n\n\n\nnote that raq_03- has a minus to indicate reverse scoring\nacceptable overall reliability (\\(\\alpha=0.71 [0.69, 0.72]\\))\nno items improve the value if they are dropped\nitem correlations with the total subscale are ok (0.27-0.49)\nissue with items not eliciting extreme responses"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "Hey!\n\nI plan to use this space to record my learning and review of stats using R and this blog built with Quarto. The last time I was semi-serious about a blog was in Grav, and WordPress before that, so I’m curious to see how this works out.\nI’m using a few books in this journey, including\nDesjardins, C. D., & Bulut, O. (2018). Handbook of Educational Measurement and Psychometrics Using R (1st ed.). Chapman and Hall/CRC. https://doi.org/10.1201/b20498\nField, A. P. (2018). Discovering statistics using IBM SPSS statistics (5th edition, North American edition). Sage Publications Inc.\nNavarro, D. (n.d.). Learning statistics with R: A tutorial for psychology students and other beginners. (Version 0.6.1). Retrieved January 8, 2020, from https://learningstatisticswithr.com/book/"
  },
  {
    "objectID": "posts/variance-std-dev/index.html",
    "href": "posts/variance-std-dev/index.html",
    "title": "Variance and Standard Deviation",
    "section": "",
    "text": "require(UsingR)\n\nLoading required package: UsingR\n\n\nWarning in library(package, lib.loc = lib.loc, character.only = TRUE,\nlogical.return = TRUE, : there is no package called 'UsingR'\n\nrequire(HistData)\n\nLoading required package: HistData\n\n\nWarning in library(package, lib.loc = lib.loc, character.only = TRUE,\nlogical.return = TRUE, : there is no package called 'HistData'\n\n\n\\(sample variance=s^2=\\frac{1}{n-1}‎‎\\sum_i(x_i - \\bar{x})^2\\)\n\nwts &lt;- c(38, 43, 48, 61, 47, 24, 29, 48, 59, 24, 40, 27)\n\n\nsort(wts)\n\n [1] 24 24 27 29 38 40 43 47 48 48 59 61"
  },
  {
    "objectID": "posts/deck-otessa24/index.html#evolving-our-understanding-of-technology-integrated-assessment-a-review-of-the-literature-and-development-of-a-new-framework",
    "href": "posts/deck-otessa24/index.html#evolving-our-understanding-of-technology-integrated-assessment-a-review-of-the-literature-and-development-of-a-new-framework",
    "title": "OTESSA24",
    "section": "Evolving our understanding of technology-integrated assessment: A review of the literature and development of a new framework",
    "text": "Evolving our understanding of technology-integrated assessment: A review of the literature and development of a new framework"
  },
  {
    "objectID": "posts/deck-otessa24/index.html#acknowledgements",
    "href": "posts/deck-otessa24/index.html#acknowledgements",
    "title": "OTESSA24",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nOur team is spread across most of the country and we each live and work on land that has been cared for by Indigenous people for millenia. We each acknowledge that our abundant lives were made possible because of the displacement of the original stewards of the land."
  },
  {
    "objectID": "posts/deck-otessa24/index.html#fall-2019",
    "href": "posts/deck-otessa24/index.html#fall-2019",
    "title": "OTESSA24",
    "section": "Fall 2019",
    "text": "Fall 2019\n2 years into a different dissertation project"
  },
  {
    "objectID": "posts/deck-otessa24/index.html#spring-2020",
    "href": "posts/deck-otessa24/index.html#spring-2020",
    "title": "OTESSA24",
    "section": "Spring 2020",
    "text": "Spring 2020\n\nScreenshot of Ross yelling ‘Pivot’ while trying to move a couch around a tight corner and up some stairs."
  },
  {
    "objectID": "posts/deck-otessa24/index.html#summer-2021",
    "href": "posts/deck-otessa24/index.html#summer-2021",
    "title": "OTESSA24",
    "section": "Summer 2021",
    "text": "Summer 2021\nCandidacy"
  },
  {
    "objectID": "posts/deck-otessa24/index.html#what-are-your-most-important-concerns-related-to-technology-integrated-assessment",
    "href": "posts/deck-otessa24/index.html#what-are-your-most-important-concerns-related-to-technology-integrated-assessment",
    "title": "OTESSA24",
    "section": "What are your most important concerns related to technology-integrated assessment?",
    "text": "What are your most important concerns related to technology-integrated assessment?\n\n\n\n\nQR Code"
  },
  {
    "objectID": "posts/deck-otessa24/index.html#section",
    "href": "posts/deck-otessa24/index.html#section",
    "title": "OTESSA24",
    "section": "…",
    "text": "…"
  },
  {
    "objectID": "posts/deck-otessa24/index.html#section-3",
    "href": "posts/deck-otessa24/index.html#section-3",
    "title": "OTESSA24",
    "section": "",
    "text": "Acceptance"
  },
  {
    "objectID": "posts/simple-regression/index.html",
    "href": "posts/simple-regression/index.html",
    "title": "Simple Regression",
    "section": "",
    "text": "Linear regression expresses the relationship between two variables, \\(X_i\\) and \\(Y_i\\).\nThe \\(_i\\) refers to the cases in the dataset, so if there are 1000 cases, then \\(_i\\) is 1-1000. Each \\(_i\\) refers to a ‘student’.\n\\[Y_i=b_0+{b_1}{X_i}+\\epsilon_i\\]\nWhere:\n\n\\(b_1\\) is the slope or regression coefficient (how much \\(Y\\) will change when \\(X\\) increases by 1)\n\n\\(b_0\\) is the \\(Y\\)-intercept, where the regression line crosses the \\(Y\\) axis\n\\(\\epsilon_i\\) is the error term\n\nor\n\\[\\hat{Y}=b_0+{b_1}{X_i}\\]\nWhere:\n\n\\(\\hat{Y}\\) is the predicted value of \\(Y\\)\n\nNotes:\nLarger values of \\(b_1\\) indicate a steeper regression line\n\n\n\nexamples of slope and intercept\n\n\n\nleft image, red is positive slope and green is negative slope (negative correlation; when predictor increases, outcome decreases)\nright image, all slopes positive"
  },
  {
    "objectID": "posts/simple-regression/index.html#regression-equation",
    "href": "posts/simple-regression/index.html#regression-equation",
    "title": "Simple Regression",
    "section": "",
    "text": "Linear regression expresses the relationship between two variables, \\(X_i\\) and \\(Y_i\\).\nThe \\(_i\\) refers to the cases in the dataset, so if there are 1000 cases, then \\(_i\\) is 1-1000. Each \\(_i\\) refers to a ‘student’.\n\\[Y_i=b_0+{b_1}{X_i}+\\epsilon_i\\]\nWhere:\n\n\\(b_1\\) is the slope or regression coefficient (how much \\(Y\\) will change when \\(X\\) increases by 1)\n\n\\(b_0\\) is the \\(Y\\)-intercept, where the regression line crosses the \\(Y\\) axis\n\\(\\epsilon_i\\) is the error term\n\nor\n\\[\\hat{Y}=b_0+{b_1}{X_i}\\]\nWhere:\n\n\\(\\hat{Y}\\) is the predicted value of \\(Y\\)\n\nNotes:\nLarger values of \\(b_1\\) indicate a steeper regression line\n\n\n\nexamples of slope and intercept\n\n\n\nleft image, red is positive slope and green is negative slope (negative correlation; when predictor increases, outcome decreases)\nright image, all slopes positive"
  },
  {
    "objectID": "posts/simple-regression/index.html#calculating-regression",
    "href": "posts/simple-regression/index.html#calculating-regression",
    "title": "Simple Regression",
    "section": "Calculating Regression",
    "text": "Calculating Regression\n\nany dataset is made up of points, and regression finds the best fitting straight line for the dataset"
  },
  {
    "objectID": "posts/simple-regression/index.html#best-fit",
    "href": "posts/simple-regression/index.html#best-fit",
    "title": "Simple Regression",
    "section": "Best Fit",
    "text": "Best Fit\n\nneed to define mathematically the distance between each data point and the regression line\n\nfor every \\(X\\) value in the data, the linear equation can determine the \\(Y\\) value on the line - called predicted \\(Y\\), or \\(\\hat{Y}\\)\nthis distance measures the error between the line and the data\nif all data points were on the line, then the line ‘fits’ the data perfectly\n\\(\\hat{Y}\\) should be close to actual \\(Y\\)\n\n\n\n\ngraph showing the difference between predicted and actual values of \\(Y\\)\n\n\n\ngreen dots = \\(\\hat{Y}\\)\nblue dots = \\(Y\\)\ndistance between the green and blue dots is the prediction error - need to minimize\nsome are positive (above the line) and some are negative (below the line)\n\nsince some are positive and some are negative, if we want to add the error values together, we need to get rid of the signs by squaring the distance and adding the squared error\n\n\\[total \\space squared \\space error ={\\sum^n_{i=1}}({Y_i}-{\\hat{Y_i}})^2\\]\n\nfor each poissible line, we can calculate the total squared error, then choose the line with the lowest total squared error, which is the regression line\nthis minimizes the difference between the line and the observed data\nthis line is called the least squared error solution\nthis method is inefficient due to the infinite number of lines possible\ncalculus is a mathematical technique to find the maxima or minima of a mathematical expression.\nUsing this method, the regression coefficients (slope (\\(b_1\\)) and intercept (\\(b_0\\))) that produce the minimum errors can be calculated."
  },
  {
    "objectID": "posts/simple-regression/index.html#calculating-regression-coefficients",
    "href": "posts/simple-regression/index.html#calculating-regression-coefficients",
    "title": "Simple Regression",
    "section": "Calculating Regression Coefficients",
    "text": "Calculating Regression Coefficients\n\\[\\hat{Y}_i=b_0+b_1X_i\\]\n\\[\\hat{b_1}=r\\frac{S_Y}{S_X}\\]\n\\[\\hat{b_0}=M_Y-\\hat{b_1}M_x\\]\nWhere\n\\(r\\) is the correlation between \\(X\\) and \\(Y\\)\n\\(S_X\\) and \\(S_Y\\) are sample standard deviations for \\(X\\) and \\(Y\\)\n\\(M_X\\) and \\(M_Y\\) are sample means for \\(X\\) and \\(Y\\)\nThese are raw regression coefficients (\\(b_1\\)): the change in outcome is associated with a change in the predictor\n\nStandardized regression coefficients\n\ntell us the same thing, except expressed as standard deviations\nif both variables \\(X\\) and \\(Y\\) have been standardized by transforming tehm into Z-scores before calculating the regression coefficients, then the regression coefficients become *standardized coefficients\n\n\\[\\hat{Z_{Y_i}}=\\beta{Z_{X_i}}\\]\n\\[\\beta=r\\]\nWhere\n\\(\\hat{Z_{Y_i}}\\) is the standard deviation of \\(Y\\)\n\\(\\hat{Z_{X_i}}\\) is the styandard deviation of \\(X\\)\n\\(\\beta\\) is the correlation coefficient\nStandardizing \\(X\\) and \\(Y\\) puts the variables on the same scale - when \\(X\\) increases by 1SD, \\(Y\\) increases by 1SD"
  },
  {
    "objectID": "posts/simple-regression/index.html#analysis-of-regression",
    "href": "posts/simple-regression/index.html#analysis-of-regression",
    "title": "Simple Regression",
    "section": "Analysis of regression",
    "text": "Analysis of regression\n\nregression line is only a model based on the data\nprocess of testing the significance of a regression equation is called analysis of regression\nnull hypothesis in the analysis of regression states that the equation does not account for a significant proportion of the variance in \\(Y\\) scores\nanalysis of regression is similar to ANOVA\n\n\nSum Square Total (\\(SS_T\\) (left plot)) is \\(\\sum(Y-\\bar{Y})^2\\)\nSum Square Residual (\\(SS_R\\) (right plot)) is \\(\\sum(Y-\\hat{Y})^2\\)\n\nprediction error - squared distance between the data and the regression line\nneeds to be minimized\nthis is the variability that is not explained by the regression equation\n\n\n\n\nAlt text\n\n\nSum Square model (\\(SS_M\\)) is \\(\\sum(\\hat{Y}-\\bar{Y})^2\\)\n\nin this equation, \\(\\hat{Y}=b_0+b_1X\\)\n\\(SS_T\\): total variability is the variability between the scores and the mean\n\n\\(SS_R\\): sum squared residual is the residual or error variability between the regression model and the data\n\n\\(SS_M\\) or sum square regression: Model variability - difference in variability between the model and the mean.\\(Y\\) are determined by the value of \\(X\\)\n\n\\(SS_T\\) is the sum of \\(SS_R\\) and \\(SS_M\\)\n\npart of the variability is residual - we don’t know what is causing\n\npart of it is due to the model - the relationship between \\(Y\\) and \\(X\\)\n\n\nThe proportion of variability in \\(Y\\) that is predicted by its relationship with \\(X\\):\n\\[\\frac{SS_{regression}}{SS_T}=r^2\\]\n\\(r^2\\) is an effect size that tells us the proportion of variability that is accounted for by the model\nThe proportion of the variability that is not accounted for is\n\\[\\frac{SS_{residual}}{SS_T}=1-r^2\\]\n\\(r^2\\) is very mieaningful\nif \\(R^2\\) is 20%, then the amount of the variability in \\(Y\\) that is accounted for by the model is 20%, which is quite large.\nTherefore\n\\[SS_{regression}=r^2SS_T\\]\n\\[SS_{residual}=(1-r^2)SS_Y\\]\n\\(SS_Y\\) is the same thing as \\(SS_T\\)\n\\(r^2\\) is the Pearson Correlation Coefficient squared\n\n\n\nAlt text\n\n\nTotal variability is partitioned into two - sum squared regression (explained by the model) and sum squared residual (not explained by the model).\nSame for degrees of freedom\nSums of squares are total values and can be expressed as averages called Mean Squares, or \\(MS\\) AKA variance\n\\[MS_{regression}=\\frac{SS_{regression}}{df_{regression}}\\]\n\nmean square variance is the variance accounted for in the model\n\n\\[MS_{residual}=\\frac{SS_{residual}}{df_{residual}}\\]\n\nmean square residual is the variance not accounted for in the model\n\nAn \\(F\\)-ratio can be used to compare these two mean squares\n\\[F=\\frac{MS_{regression}}{MS_{residual}} \\text{with}\\: df=1,n-2\\]\n\nif the mean square regression is small compared to the mean square residual, then the model does not account for much variance\n\nif the mean square regression is larger than the mean square residual, then the model accounts for more variance"
  },
  {
    "objectID": "posts/simple-regression/index.html#an-example-fields",
    "href": "posts/simple-regression/index.html#an-example-fields",
    "title": "Simple Regression",
    "section": "An Example (Fields)",
    "text": "An Example (Fields)\n\nrecord company boss interested in predicting album sales from advertising bsed on 200 different album releases\n\noutcome variable: sales of CDs and downloads in the week after release\n\npredictor variable: amount in GBP spent promoting the album before release\n\n\n\n\nscatterplot of album sales by advertising budget\n\n\n\\(r^2\\) = 0.335, so 33.5% of the variability is shared by both variables (cutoffs, 9% - small, 25% - large effect size)\n\n\n\nAlt text\n\n\n\n\n\nAlt text\n\n\n\n\n\nAlt text"
  },
  {
    "objectID": "posts/simple-regression/index.html#activities",
    "href": "posts/simple-regression/index.html#activities",
    "title": "Simple Regression",
    "section": "Activities",
    "text": "Activities\nIn a survey that included assessment of husband and wife heights, Hodges, Krech and Crutchfield (1975) reported the following results. Let’s treat wife height as the predictor (X) variable and husband as the outcome (Y) variable:\n\\(r_XY\\) = .32, \\(N\\) = 1, 296\nWife height: \\(M_X\\) = 64.42, \\(S_X\\) = 2.56\nHusband height: \\(M_Y\\) = 70.46, \\(S_Y\\) = 2.87\na. Calculates the values of \\(b_0\\) and \\(b_1\\) to predict husband height in inches (Y) from wife height in inches (X), and write out this raw score predictive equation\n\\[\\hat{b_1}=r\\frac{S_Y}{S_X}\\]\n\\[\\hat{b_1}=.32\\frac{2.87}{2.56}\\]\n\\[\\hat{b_1}=0.35\\]\n\\[b_0=M_Y-\\hat{b_1}M_X\\]\n\\[b_0=70.46-(0.35*64.42)\\]\n\\[b_0=70.46-22.54\\]\n\\[b_0=47.91\\]\nPredictive Equation \\[\\hat{Y}=b_0+{b_1}{X_i}\\] \\[\\hat{Y}=47.91+(0.35*{X_i})\\]\n\nFor women: what is your own height? Substitute your own height into the equation from step a, and calculate the predicted height of your present or future spouse.\n\n\\[\\hat{Y}=47.91+(0.35*67)\\]\n\\[\\hat{Y}=71.36\\]\n\nNow reverse the roles of the variables (i.e., use husband height as the predictor and wife height as the outcome variable). Calculates the values of bb0 and bb1.\n\n\\[\\hat{b_1}=r\\frac{S_Y}{S_X}\\]\n\\[\\hat{b_1}=.32\\frac{2.56}{2.87}\\]\n\\[\\hat{b_1}=.32*.89\\]\n\\[\\hat{b_1}=.28\\]\n\\[b_0=M_Y-\\hat{b_1}M_X\\]\n\\[b_0=64.42-(0.28*70.46)\\]\n\\[b_0=64.42-19.72\\]\n\\[b_0=44.7\\]\n\\[\\hat{Y}=44.7+(0.28*{X_i})\\]\n\nFor men: what is your height in inches? Substitute your own height into the equation from step c, and calculate the predicted height of your present or future spouse.\n\n\\[\\hat{Y}=44.7+(0.28*70)\\]\n\\[\\hat{Y}=64.3\\]\n\nWhat proportion of variance in husband height is predictable from wife height? Test the significance of the regression equation.\n\n\\[r^2=.10\\]\n\nIf both X and Y have been standardized by transforming into Z scores before calculating the regression coefficients, what would be the values of bb0 and bb1 to predict husband height in inches (Y) from wife height in inches (X)? How does this standardized version of the prediction equation tell us about “regression toward the mean” for predictions?\n\n\\[\\hat{Z_{Y_i}}=\\beta{Z_{X_i}}\\]\n\\[\\hat{2.87}=0.32{2.56}\\]"
  },
  {
    "objectID": "posts/discovr_02_summarizing_data/index.html",
    "href": "posts/discovr_02_summarizing_data/index.html",
    "title": "discovr_02 - Summarizing Data",
    "section": "",
    "text": "library(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nice_tib &lt;- here::here(\"data/ice_bucket.csv\") |&gt; readr::read_csv()\n\nRows: 2323000 Columns: 1\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (1): upload_day\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message."
  },
  {
    "objectID": "posts/discovr_02_summarizing_data/index.html#frequency-tables",
    "href": "posts/discovr_02_summarizing_data/index.html#frequency-tables",
    "title": "discovr_02 - Summarizing Data",
    "section": "Frequency tables",
    "text": "Frequency tables\n\nuse group_by() and summarise() and n() functions from dplyr\n\n\ngroup_by()\n\ngroups data by whatever variable(s) you name within the function\n\nsummarise()\n\ncreates summary table based on the variables in the function\n\nn()\n\ncounts the number of scores\n\n\nTo count frequencies:\n\ntell R to treat values that are the same, as being in the same category\n\ngroup_by(upload_day) tells R that scores that are the same within upload_day are in the same group\nsubsequent operations are conducted on the groups\n\ncount how many scores fall into each category\n\nsummarize() creates a variable called frequency that counts how many items are in each group created by group_by()\n\n\n\nfreq_tbl &lt;- ice_tib |&gt;\n  dplyr::group_by(upload_day) |&gt; \n  dplyr::summarise(\n    frequency = n()\n  )\nfreq_tbl\n\n# A tibble: 56 × 2\n   upload_day frequency\n        &lt;dbl&gt;     &lt;int&gt;\n 1         21      2000\n 2         22      2000\n 3         23      4000\n 4         24      4000\n 5         25      8000\n 6         26      8000\n 7         27     10000\n 8         28     16000\n 9         29     20000\n10         30     29000\n# ℹ 46 more rows\n\n\n\nthis is a large table and a bit unwieldy\nuse a grouped frequency distribution\n\nplace values of upload_days into bins\n\nif we want to split the variable upload_day into bins of 4 days…\n\nggplot2::cut_width(upload_day, 4)\n\ncombine this with dplyr::mutate() to create a new variable called days_group\n\n\ngp_freq_dist &lt;- ice_tib |&gt; \n  dplyr::mutate(\n    days_group = ggplot2::cut_width(upload_day, 4)\n    )\ngp_freq_dist\n\n# A tibble: 2,323,000 × 2\n   upload_day days_group\n        &lt;dbl&gt; &lt;fct&gt;     \n 1         34 (30,34]   \n 2         36 (34,38]   \n 3         31 (30,34]   \n 4         30 (26,30]   \n 5         33 (30,34]   \n 6         38 (34,38]   \n 7         36 (34,38]   \n 8         46 (42,46]   \n 9         45 (42,46]   \n10         31 (30,34]   \n# ℹ 2,322,990 more rows\n\n\n\nthis creates a new object called gp_freq_dist that contains each value within ice_tib but with an extra column/variable called days_group that indicates the bin the value of upload_day is in\n\n\n\n\n\n\n\nSet notation\n\n\n\n\nthe value of upload_day now has a corresponding value of days_group containing the bin\nthe first score of 34 has been assigned to the bin labelled `(30, 34] which is the bin containing any score above 30, up to and including 34\nthe label uses standard mathematical notation for sets where ( or ) means ‘not including’ and [ or ] means ‘including’\n\n\n\n\nnow we can use summarize() and n() to count scores like before, except to use days_group instead of upload_day\n\n\nCoding challenge\nCreate a grouped frequency table called gp_freq_dist by starting with the code in the code example and then using the code we used to create freq_tbl to create a pipe that summarizes the grouped scores.\n\ngp_freq_dist &lt;- ice_tib |&gt; \n  dplyr::mutate(\n    days_group = ggplot2::cut_width(upload_day, 4)\n    ) |&gt;\n  dplyr::group_by(days_group) |&gt; \n  dplyr::summarise(\n    frequency = n()\n  )\ngp_freq_dist\n\n# A tibble: 15 × 2\n   days_group frequency\n   &lt;fct&gt;          &lt;int&gt;\n 1 [18,22]         4000\n 2 (22,26]        24000\n 3 (26,30]        75000\n 4 (30,34]       367000\n 5 (34,38]       770000\n 6 (38,42]       534000\n 7 (42,46]       255000\n 8 (46,50]       102000\n 9 (50,54]        70000\n10 (54,58]        38000\n11 (58,62]        26000\n12 (62,66]        18000\n13 (66,70]        18000\n14 (70,74]        16000\n15 (74,78]         6000"
  },
  {
    "objectID": "posts/discovr_02_summarizing_data/index.html#relative-frequencies",
    "href": "posts/discovr_02_summarizing_data/index.html#relative-frequencies",
    "title": "discovr_02 - Summarizing Data",
    "section": "Relative Frequencies",
    "text": "Relative Frequencies\n\nwe have an object gp_freq_dist that contains the number of days grouped into bins of 4 days and the number of videos uploaded during each of the time periods represented by those bins\nto calculate the relative frequency we can use dplyr::mutate() to add a variable that divides the frequency by the total number of videos using sum()\n\n... |&gt;\n    dplyr::mutate(\n        relative_freq = frequency/sum(frequency) # creates a new column\n    )"
  },
  {
    "objectID": "posts/discovr_02_summarizing_data/index.html#efficient-code",
    "href": "posts/discovr_02_summarizing_data/index.html#efficient-code",
    "title": "discovr_02 - Summarizing Data",
    "section": "Efficient Code",
    "text": "Efficient Code\n\nrather than creating the table of relative frequencies step-by-step, it is usually more efficient to carry out the steps in one piece of code\n\n\n\ngp_freq_dist &lt;- ice_tib |&gt; \n  dplyr::mutate(\n    days_group = ggplot2::cut_width(upload_day, 4)\n    ) |&gt; \n  dplyr::group_by(days_group) |&gt; \n  dplyr::summarise(\n    frequency = n()\n  ) |&gt; \n  dplyr::mutate(\n    relative_freq = frequency/sum(frequency),\n    percent = relative_freq*100\n  )\n  \ngp_freq_dist\n\n# A tibble: 15 × 4\n   days_group frequency relative_freq percent\n   &lt;fct&gt;          &lt;int&gt;         &lt;dbl&gt;   &lt;dbl&gt;\n 1 [18,22]         4000       0.00172   0.172\n 2 (22,26]        24000       0.0103    1.03 \n 3 (26,30]        75000       0.0323    3.23 \n 4 (30,34]       367000       0.158    15.8  \n 5 (34,38]       770000       0.331    33.1  \n 6 (38,42]       534000       0.230    23.0  \n 7 (42,46]       255000       0.110    11.0  \n 8 (46,50]       102000       0.0439    4.39 \n 9 (50,54]        70000       0.0301    3.01 \n10 (54,58]        38000       0.0164    1.64 \n11 (58,62]        26000       0.0112    1.12 \n12 (62,66]        18000       0.00775   0.775\n13 (66,70]        18000       0.00775   0.775\n14 (70,74]        16000       0.00689   0.689\n15 (74,78]         6000       0.00258   0.258"
  },
  {
    "objectID": "posts/discovr_02_summarizing_data/index.html#histograms",
    "href": "posts/discovr_02_summarizing_data/index.html#histograms",
    "title": "discovr_02 - Summarizing Data",
    "section": "Histograms",
    "text": "Histograms\n\nggplot2 can produce data visualizations\n\n\n\n\n\n\n\nTip: Always load ggplot2!\n\n\n\nWe’ve discussed elsewhere that if you include packages when you use functions (e.g., dplyr::mutate()) you don’t need to explicitly load the package (in this case dplyr). However, to create plots with ggplot2 you build them up layer by layer, which means you use a lot of ggplot2 functions. For this reason, I advise loading it at the start of your Quarto document and not worrying too much about including package references when you use functions. You can load it either with library(ggplot2) or by loading the entire tidyverse using library(tidyverse).\n\n\n\ngeneral form of ggplot2\n\n`ggplot2::ggplot(my_tib, aes(variable_for_x_axis, variable_for_y_axis))`\n\nggplot2::ggplot(ice_tib, aes(upload_day))\n\n\n\n\n\n\n\n\n\nsomething is missing b/c we only told ggplot2 ‘what’ to plot, not ‘how’ to plot it.\nneed to add a geom with geom_histogram()\n\n\nggplot2::ggplot(ice_tib, aes(upload_day)) +\ngeom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\n\nChanging bin widths\n\nggplot2::ggplot(ice_tib, aes(upload_day)) +\ngeom_histogram(binwidth=1)\n\n\n\n\n\n\n\n\n\n\nChanging colours\n\ninclude fill = within the geom_histogram() function\n\n\nggplot2::ggplot(ice_tib, aes(upload_day)) +\ngeom_histogram(binwidth = 1, fill = \"#440154\")\n\n\n\n\n\n\n\n\n\n\nTransparency and axis labels\n\nggplot2::ggplot(ice_tib, aes(upload_day)) +\ngeom_histogram(binwidth = 1, fill = \"#440154\", alpha = 0.25) +\nlabs(y = \"Frequency\", x = \"Days since first ice bucket challenge video\")\n\n\n\n\n\n\n\n\n\n\nThemes\n\nggplot2::ggplot(ice_tib, aes(upload_day)) +\ngeom_histogram(binwidth = 1, fill = \"#440154\", alpha = 0.9) +\nlabs(y = \"Frequency\", x = \"Days since first ice bucket challenge video\") +\ntheme_minimal()"
  },
  {
    "objectID": "posts/discovr_02_summarizing_data/index.html#summarizing-data",
    "href": "posts/discovr_02_summarizing_data/index.html#summarizing-data",
    "title": "discovr_02 - Summarizing Data",
    "section": "Summarizing data",
    "text": "Summarizing data\n\nMean and median\nmean(variable, trim = 0, na.rm = FALSE)\n\ntrim\n\nallows you to trim scores before calculating the mean by specifying a value between 0 and 0.5. default is 0 (no trim). to trim 10% of scores fom each end of the distribution you could set trim = 0.1\n\nna.rm\n\nstands for NA remove. Missing values are denoted NA, for ‘not available’. by setting na.rm = TRUE (or na.rm = T), R will remove missing values before computing the mean\n\n\n\n\n\n\n\n\nMissing Values\n\n\n\nThe default in many functions is not to remove missing values (e.g. na.rm = FALSE). If you have missing values in your data and don’t change this default behaviour R will throw an error. Therefore, if you get an error from a function like mean(), check whether you have missing values and whether you have forgotten to set na.rm = TRUE.\n\n\nThe function for median is similar, except no trim (median is effectively the mean with 50% trim)\nmedian(variable, na.rm = FALSE)\n\n\nCode example\n\nif the defaults are ok, there is no need to set those arguments\n\n\nmean(ice_tib$upload_day)\n\n[1] 39.678\n\n\nTo remove missing values:\n\nmean(ice_tib$upload_day , na.rm = TRUE)\n\n[1] 39.678\n\n\n\n?mean\n\n\n\ncoding challenge\nFind the median number of days after the original ice bucket video that other videos were uploaded.\n\nmedian(ice_tib$upload_day)\n\n[1] 38"
  },
  {
    "objectID": "posts/discovr_02_summarizing_data/index.html#quantifying-the-fit-of-the-mean",
    "href": "posts/discovr_02_summarizing_data/index.html#quantifying-the-fit-of-the-mean",
    "title": "discovr_02 - Summarizing Data",
    "section": "Quantifying the ‘fit’ of the mean",
    "text": "Quantifying the ‘fit’ of the mean\n\nvar()\n\nvariance\n\n\nvar(variable_name, na.rm = FALSE)\n\nsd()\n\nstandard deviation\n\n\nsd(variable_name, na.rm = FALSE)\n\n\nvar() and sd() take the same syntax as mean()\n\ncoding challenge\nUse what you learned in the previous section and the code example above to get the variance and standard deviation of the days since the original ice bucket video that other videos were uploaded.\n\nvar(ice_tib$upload_day)\n\n[1] 59.94197\n\nsd(ice_tib$upload_day)\n\n[1] 7.74222"
  },
  {
    "objectID": "posts/discovr_02_summarizing_data/index.html#inter-quartile-range",
    "href": "posts/discovr_02_summarizing_data/index.html#inter-quartile-range",
    "title": "discovr_02 - Summarizing Data",
    "section": "Inter-quartile range",
    "text": "Inter-quartile range\n\nIQR()\n\nIQR(variable_name, na.rm = FALSE, type = 7)\n\n\n\nIQR(ice_tib$upload_day, type = 7)\n\n[1] 7\n\nIQR(ice_tib$upload_day, type = 8)\n\n[1] 7"
  },
  {
    "objectID": "posts/discovr_02_summarizing_data/index.html#creating-a-summary-table",
    "href": "posts/discovr_02_summarizing_data/index.html#creating-a-summary-table",
    "title": "discovr_02 - Summarizing Data",
    "section": "Creating a summary table",
    "text": "Creating a summary table\n\nused to comine all the above into one table\n\nsummarise()\nice_tib |&gt;\n  dplyr::summarise(\n    median =  median(upload_day),  # creates new variable called `median` from the output of `median(upload_day)`\n    mean =  mean(upload_day), # creates new variable called `mean` from the output of `mean(upload_day)`\n    ...\n    )\n\ncoding challenge\nCreate a summary table containing the mean, median, IQR, variance and SD of the number of days since the original ice bucket video.\n\nice_tib |&gt;\n  dplyr::summarise(\n    median =  median(upload_day),  # creates new variable called `median` from the output of `median(upload_day)`\n    mean =  mean(upload_day), # creates new variable called `mean` from the output of `mean(upload_day)`\n    IQR = IQR(upload_day),\n    var = var(upload_day),\n    sd = sd(upload_day)\n    )\n\n# A tibble: 1 × 5\n  median  mean   IQR   var    sd\n   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     38  39.7     7  59.9  7.74\n\n\n\n\nCode example\nTo store the summary of stats, we assign it to a new object:\n\nupload_summary &lt;- ice_tib |&gt;\n  dplyr::summarise(\n    median =  median(upload_day),\n    mean =  mean(upload_day),\n    IQR = IQR(upload_day),\n    variance = var(upload_day),\n    std_dev = sd(upload_day)\n    ) \n    upload_summary\n\n# A tibble: 1 × 5\n  median  mean   IQR variance std_dev\n   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;\n1     38  39.7     7     59.9    7.74"
  },
  {
    "objectID": "posts/discovr_02_summarizing_data/index.html#rounding-values",
    "href": "posts/discovr_02_summarizing_data/index.html#rounding-values",
    "title": "discovr_02 - Summarizing Data",
    "section": "Rounding Values",
    "text": "Rounding Values\nuse round() function to round values and use kable() from knitr to round an entire table of values\n\nround()\n\nround(x, digits = 0)\n\n\ndefault is 0\n\n\n\nround(3.211420)\n\n[1] 3\n\n\nalso use a pipe to feed a mean, median, or variance into the round function\n\nvar(ice_tib$upload_day) |&gt;\n  round(3)\n\n[1] 59.942\n\nmean(ice_tib$upload_day) |&gt;\n  round(3)\n\n[1] 39.678\n\nsd(ice_tib$upload_day) |&gt;\n  round(3)\n\n[1] 7.742\n\n\n\nupload_summary &lt;- ice_tib |&gt;\n  dplyr::summarise(\n    median =  median(upload_day),\n    mean =  mean(upload_day),\n    IQR = IQR(upload_day),\n    variance = var(upload_day),\n    std_dev = sd(upload_day)\n    ) \n    upload_summary |&gt;\n    round(2)\n\n# A tibble: 1 × 5\n  median  mean   IQR variance std_dev\n   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;\n1     38  39.7     7     59.9    7.74"
  },
  {
    "objectID": "posts/discovr_02_summarizing_data/index.html#datawizard",
    "href": "posts/discovr_02_summarizing_data/index.html#datawizard",
    "title": "discovr_02 - Summarizing Data",
    "section": "Datawizard",
    "text": "Datawizard\n\ndatawizard::describe_distribution(x = ice_tib,\n  select = NULL,\n  exclude = NULL,\n  centrality = \"mean\",\n  dispersion = TRUE,\n  iqr = TRUE,\n  range = TRUE,\n  quartiles = FALSE,\n  include_factors = FALSE,\n  ci = NULL)\n\nVariable   |  Mean |   SD | IQR |          Range | Skewness | Kurtosis |       n | n_Missing\n--------------------------------------------------------------------------------------------\nupload_day | 39.68 | 7.74 |   7 | [21.00, 76.00] |     1.72 |     4.43 | 2323000 |         0"
  },
  {
    "objectID": "posts/discovr_05_visualizing_data/index.html",
    "href": "posts/discovr_05_visualizing_data/index.html",
    "title": "discovr_05 - Visualizing Data",
    "section": "",
    "text": "library(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nwish_tib &lt;- here::here(\"data/jiminy_cricket.csv\") |&gt; readr::read_csv()\n\nRows: 500 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): strategy, time\ndbl (2): id, success\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nnotebook_tib &lt;- here::here(\"data/notebook.csv\") |&gt; readr::read_csv()\n\nRows: 40 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): sex, film\ndbl (1): arousal\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nexam_tib &lt;- here::here(\"data/exam_anxiety.csv\") |&gt; readr::read_csv()\n\nRows: 103 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): sex\ndbl (4): id, revise, exam_grade, anxiety\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nwish_tib &lt;- wish_tib |&gt;\n  dplyr::mutate(\n    strategy = forcats::as_factor(strategy),\n    time = forcats::as_factor(time) |&gt; forcats::fct_relevel(\"Baseline\")\n  )\nnotebook_tib &lt;- notebook_tib |&gt;\n  dplyr::mutate(\n    sex = forcats::as_factor(sex),\n    film = forcats::as_factor(film)\n  )\nexam_tib &lt;- exam_tib |&gt;\n  dplyr::mutate(\n    id = forcats::as_factor(id),\n    sex = forcats::as_factor(sex)\n  )"
  },
  {
    "objectID": "posts/discovr_05_visualizing_data/index.html#ggplot2",
    "href": "posts/discovr_05_visualizing_data/index.html#ggplot2",
    "title": "discovr_05 - Visualizing Data",
    "section": "ggplot2",
    "text": "ggplot2\n\npart of the tidyverse\n\n\naes()\n\ncontrols aesthetics of the plot\n\n\n\nGeometric objects\n\nobjects that represent data\n\n\ngeom_point()\n\nplots data by points/dots\n\ngeom_boxplot()\n\nplots boxplots\n\ngeom_histogram()\n\nplots histograms\n\ngeom_errorbar()\n\nplots error bars\n\ngeom_smooth()\n\nplots summary lines\n\n\n\n\nObjects or ‘stats’\n\nsome situations where it is easier to display a summary of the data directly to the plot (usually stat_summary())\n\n\n\nScales\n\ncontrol details of how data are mapped to their visual objects to control what appears on x and y axes using scale_x_continuous() and scale_y_continuous(), axis labels are controlled with labs()\n\n\n\nCoordinate system\n\nggplot2 uses a Cartesian system.\ncoord_cartesian() sets limits on x and y axes\n\n\n\nPosition adjustments\n\nposition_dodge()forces objects to not overlap side by side\nposition_jitter() adds small random adjustments to data points\n\n\n\nFacets\n\nused to plot different parts of the data in different panels\n\n\n\nThemes\n\nvarious themes to style the output\ncan be overridden with theme() function\n\nEach of the above are layers that can be added to a plot, as below\n\n\n\nExplanation of the layered approach to ggplot2"
  },
  {
    "objectID": "posts/discovr_05_visualizing_data/index.html#boxplots-box-whisker-plots",
    "href": "posts/discovr_05_visualizing_data/index.html#boxplots-box-whisker-plots",
    "title": "discovr_05 - Visualizing Data",
    "section": "Boxplots (box-whisker plots)",
    "text": "Boxplots (box-whisker plots)\n\nimaginary data based on peoples’ level of success (0-100)\none group told to wish for good success, other group told to work hard for success\nmeasured success again 5 years later\nThe data are in wish_tib. The variables are id (the person’s id), strategy (hard work or wishing upon a star), time (baseline or 5 years), and success (the rating on my dodgy scale).\n\nCreating a boxplot…\n\ngeom_boxplot()\n\nggplot2::ggplot(my_tib, aes(variable_for_x_axis, variable_for_y_axis))\n\n\n\nwish_plot &lt;- ggplot2::ggplot(wish_tib, aes(time, success))  # creates an object called `wish_plot` that contains the boxplot\n# ggplot() function specifies the plot will use `wish_tib` and plots time on *x* and success on *y*\nwish_plot +\n  geom_boxplot() # adds boxplot geom to wish_plot\n\n\n\n\n\n\n\n\n\nwish_plot &lt;- ggplot2::ggplot(wish_tib, aes(time, success))\nwish_plot +\n  geom_boxplot() +\n  labs(x = \"Time\", y = \"Success (%)\") + # add labels to axes\n  theme_minimal() # add minimal theme layer\n\n\n\n\n\n\n\n\n\nplot shows slight increase of success, but doesn’t show the effect of hard work"
  },
  {
    "objectID": "posts/discovr_18_efa/index.html",
    "href": "posts/discovr_18_efa/index.html",
    "title": "discovr_18 - Exploratory Factor Analysis",
    "section": "",
    "text": "library(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nraq_tib &lt;- here::here(\"data/raq.csv\") |&gt;\n  readr::read_csv()\n\nRows: 2571 Columns: 24\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (1): id\ndbl (23): raq_01, raq_02, raq_03, raq_04, raq_05, raq_06, raq_07, raq_08, ra...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message."
  },
  {
    "objectID": "posts/discovr_07_associations/index.html",
    "href": "posts/discovr_07_associations/index.html",
    "title": "discovr_07 - Associations",
    "section": "",
    "text": "library(tidyverse, ggplot2)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nliar_tib &lt;- here::here(\"data/biggest_liar.csv\") |&gt; readr::read_csv()\n\nRows: 68 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): id, novice\ndbl (2): creativity, position\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nexam_tib &lt;- here::here(\"data/exam_anxiety.csv\") |&gt; readr::read_csv()\n\nRows: 103 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): sex\ndbl (4): id, revise, exam_grade, anxiety\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nliar_tib &lt;- liar_tib |&gt; \n  dplyr::mutate(\n    novice = forcats::as_factor(novice)\n  )\nexam_tib &lt;- exam_tib |&gt;\n  dplyr::mutate(\n    id = forcats::as_factor(id),\n    sex = forcats::as_factor(sex)\n  )\nexam_tib\n\n# A tibble: 103 × 5\n   id    revise exam_grade anxiety sex   \n   &lt;fct&gt;  &lt;dbl&gt;      &lt;dbl&gt;   &lt;dbl&gt; &lt;fct&gt; \n 1 1          4         40    86.3 Male  \n 2 2         11         65    88.7 Female\n 3 3         27         80    70.2 Male  \n 4 4         53         80    61.3 Male  \n 5 5          4         40    89.5 Male  \n 6 6         22         70    60.5 Female\n 7 7         16         20    81.5 Female\n 8 8         21         55    75.8 Female\n 9 9         25         50    69.4 Female\n10 10        18         40    82.3 Female\n# ℹ 93 more rows\nGGally::ggscatmat(exam_tib, columns = c(\"exam_grade\", \"revise\", \"anxiety\")) +\ntheme_minimal()\n\nRegistered S3 method overwritten by 'GGally':\n  method from   \n  +.gg   ggplot2"
  },
  {
    "objectID": "posts/discovr_07_associations/index.html#pearsons-correlation-using-r",
    "href": "posts/discovr_07_associations/index.html#pearsons-correlation-using-r",
    "title": "discovr_07 - Associations",
    "section": "Pearson’s correlation using R",
    "text": "Pearson’s correlation using R\n\ncorrelation package -&gt; workhorse function called correlation()\n\ncorrelation::correlation(tibble,\n                         method = \"pearson\",\n                         p_adjust = \"holm\",\n                         ci = 0.95\n                         )\n\ntibble\n\nshould be replaced with the name of tibble containing any variables to correlate\n\nmethod\n\nmethod of correlation coefficient, default is pearson, but can also accept spearman, kendall, biserial, polychoric, tetrachoric, and percentage\n\np_adjust\n\ncorrects the \\(p\\)-value for the number of tests you have performed using the Holm-Bonferroni method\n\n\napplies the Bonferroni criterion in a slightly less strict way that controls the type I error, but with less risk of a type II error\n\n\ncan change to none (bad idea), bonferroni (to apply the standard Bonferroni method) or several other methods.\n\nci\n\nset the confidence interval width; default is 0.95 for general use\n\n\nTo use the function, - pipe tibble into the select() function from dplyr to select variables to correlate, then pipe that into the correlation function - use the same syntax whether you want to correlate two variables or produce all correlations between pairs of multiple variables]\nTo calculate Pearson correlation btwn variables exam_grade and revise in exam_tib…\n\nexam_tib |&gt; \n  dplyr::select(exam_grade, revise) |&gt; \n  correlation::correlation() |&gt; \n  knitr::kable(digits = 3)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nParameter1\nParameter2\nr\nCI\nCI_low\nCI_high\nt\ndf_error\np\nMethod\nn_Obs\n\n\n\n\nexam_grade\nrevise\n0.397\n0.95\n0.22\n0.548\n4.343\n101\n0\nPearson correlation\n103\n\n\n\n\n\n\nexam_tib |&gt; \n  dplyr::select(exam_grade, anxiety) |&gt; \n  correlation::correlation() |&gt; \n  knitr::kable(digits = 3)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nParameter1\nParameter2\nr\nCI\nCI_low\nCI_high\nt\ndf_error\np\nMethod\nn_Obs\n\n\n\n\nexam_grade\nanxiety\n-0.441\n0.95\n-0.585\n-0.271\n-4.938\n101\n0\nPearson correlation\n103\n\n\n\n\n\n\nexam_tib |&gt; \n  dplyr::select(exam_grade, revise, anxiety) |&gt; \n  correlation::correlation() |&gt; \n  knitr::kable(digits = 3)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nParameter1\nParameter2\nr\nCI\nCI_low\nCI_high\nt\ndf_error\np\nMethod\nn_Obs\n\n\n\n\nexam_grade\nrevise\n0.397\n0.95\n0.220\n0.548\n4.343\n101\n0\nPearson correlation\n103\n\n\nexam_grade\nanxiety\n-0.441\n0.95\n-0.585\n-0.271\n-4.938\n101\n0\nPearson correlation\n103\n\n\nrevise\nanxiety\n-0.709\n0.95\n-0.794\n-0.598\n-10.111\n101\n0\nPearson correlation\n103\n\n\n\n\n\n\n\n\n\n\n\nThe confidence interval for the association between exam grade and revision is 0.22 to 0.55. What does this tell us?\n\n\n\n\n\nIf this confidence interval is one of the 95% that contains the population value then the population value of r lies between 0.22 and 0.55.\n\n\n\n\n\n\n\n\n\nThe p-value for the association between exam grade and revision is &lt; 0.001, what does this value mean?\n\n\n\n\n\nThe probability of getting a value of t at least as big as the value we have observed, if the value of r were, in fact, zero is less than 0.001. I’m going to assume, therefore, that the association between exam grade and revision is not zero.\n\n\n\n\nexam grade correlates with revision - \\(r\\)=0.4\nexam grade had a similar strength relationship with exam anxiety \\(r\\)=-0.44 but in the opposite direction\nrevision had a strong negative relationship with anxiety - \\(r\\)=-0.709\nthe more you revise, the better your performance\nthe more anxiety you have, the worse your performance\nthe mopre you revise, the less anxiety you have\nall \\(p\\)-values are less than 0.001 and would be interpreted as the correlation coefficients being significantly different from zero\nsignificance values tell us that the probability of getting correlation coefficients at least as big as this in a sample of 103 people if the null were true (that there was no relationship between the variables) is very low\nif we assume the sample is one of the 95% of samples that will produce a confidence interval containing the population value, then the confidence intervals tell us about the uncertainty around \\(r\\).\n\n\n\n\n\n\n\nRounding\n\n\n\nWe can control the number of decimal places using knitr::kable(digits = 3)\nWe can also specify different columns to contain different rounding using knitr::kable(digits = c(2, 2, 2, 2, 2, 2, 2, 2, 8)) (column 9 to 8 decimal places) or knitr::kable(digits = c(rep(2, 8), 8))"
  },
  {
    "objectID": "posts/discovr_07_associations/index.html#robust-correlation-coefficients",
    "href": "posts/discovr_07_associations/index.html#robust-correlation-coefficients",
    "title": "discovr_07 - Associations",
    "section": "Robust correlation coefficients",
    "text": "Robust correlation coefficients\nGiven the skew in the variables, we should use a robust correlation coefficient, like the percentage bend correlation coefficient by setting method = \"percentage\" within correlation()\n\nexam_tib |&gt; \n  dplyr::select(exam_grade, revise, anxiety) |&gt; \n  correlation::correlation(\n   method = \"percentage\"\n   ) |&gt; \n  knitr::kable(digits = c(rep(2, 8), 8))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nParameter1\nParameter2\nr\nCI\nCI_low\nCI_high\nt\ndf_error\np\nMethod\nn_Obs\n\n\n\n\nexam_grade\nrevise\n0.34\n0.95\n0.15\n0.50\n3.60\n101\n0.00049502\nPercentage Bend correlation\n103\n\n\nexam_grade\nanxiety\n-0.40\n0.95\n-0.55\n-0.23\n-4.41\n101\n0.00005235\nPercentage Bend correlation\n103\n\n\nrevise\nanxiety\n-0.61\n0.95\n-0.72\n-0.47\n-7.66\n101\n0.00000000\nPercentage Bend correlation\n103\n\n\n\n\n\n\n\n\nPar1\nPar2\nPercentage Bend \\(r\\)\nraw Pearson \\(r\\)\n\n\n\n\nexam_grade\nrevise\n0.34\n0.397\n\n\nexam_grade\nanxiety\n-0.40\n-0.441\n\n\nrevise\nanxiety\n-0.61\n-0.709\n\n\n\nAll robust correlations (percentage bend) are less than raw, though all are significant at \\(p&lt;0.001\\)"
  },
  {
    "objectID": "posts/discovr_07_associations/index.html#spearmans-correlation-coefficient",
    "href": "posts/discovr_07_associations/index.html#spearmans-correlation-coefficient",
    "title": "discovr_07 - Associations",
    "section": "Spearman’s correlation coefficient",
    "text": "Spearman’s correlation coefficient\n\ndata from World’s Best Liar competition\nwant to know if creativity impacts lying ability\nposition data (1st, 2nd, etc) is ordinal, so Spearman’s correlation coefficient should be used\nData are in\n\n\nliar_tib\n\n# A tibble: 68 × 4\n   id    creativity position novice          \n   &lt;chr&gt;      &lt;dbl&gt;    &lt;dbl&gt; &lt;fct&gt;           \n 1 lnwe          53        1 First time      \n 2 vxob          36        3 Previous entrant\n 3 qpli          31        4 First time      \n 4 pwsq          43        2 First time      \n 5 xafq          30        4 Previous entrant\n 6 njra          41        1 First time      \n 7 lxty          32        4 First time      \n 8 dxcw          54        1 Previous entrant\n 9 uxgp          47        2 Previous entrant\n10 dvew          50        2 First time      \n# ℹ 58 more rows\n\nGGally::ggscatmat(liar_tib, columns = c(\"creativity\", \"position\")) +\ntheme_minimal()\n\n\n\n\n\n\n\n\nTo get Spearman correlation, we use correlation() the same way as we did with Pearson, except we add method = \"spearman\" to the function\n\nliar_tib |&gt;\n  dplyr::select(creativity, position) |&gt; \n  correlation::correlation(method = \"spearman\") |&gt; \n    knitr::kable(digits = c(rep(2, 7), 8))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nParameter1\nParameter2\nrho\nCI\nCI_low\nCI_high\nS\np\nMethod\nn_Obs\n\n\n\n\ncreativity\nposition\n-0.37\n0.95\n-0.57\n-0.14\n71948.4\n0.00172042\nSpearman correlation\n68\n\n\n\n\n\n\nSpearman correlation between the two is \\(r_s=-0.37\\) with an associated \\(p\\)-value of 0.002 and a sample size of 68\nas creativity increased, position decreased\n\nthis might seem contrary to the hypothesis, but a position of 4 is a lower position than 1"
  },
  {
    "objectID": "posts/discovr_07_associations/index.html#kendalls-tau-tau",
    "href": "posts/discovr_07_associations/index.html#kendalls-tau-tau",
    "title": "discovr_07 - Associations",
    "section": "Kendall’s tau (\\(\\tau\\))",
    "text": "Kendall’s tau (\\(\\tau\\))\n\nanother non-parametric correlation\nused instead of Spearman’s correlation when data set is small with a large number of tied ranks\ncorrelation() function will calculate Kendall’s \\(\\tau\\) by including method = \"kendall\"\n\n\nliar_tib |&gt;\n  dplyr::select(creativity, position) |&gt; \n  correlation::correlation(method = \"kendall\") |&gt; \n    knitr::kable(digits = c(rep(4, 7), 8))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nParameter1\nParameter2\ntau\nCI\nCI_low\nCI_high\nz\np\nMethod\nn_Obs\n\n\n\n\ncreativity\nposition\n-0.3002\n0.95\n-0.4396\n-0.1468\n-3.2252\n0.0012588\nKendall correlation\n68\n\n\n\n\n\n\noutput shows \\(\\tau=-0.3\\) -&gt; closer to 0 than Spearman (-.38) therefore Kendall’s value is likely a more accurate guage of what the correlation in the population would be\n\n\n\n\n\n\n\nCover Photo by Zhuojun Yu on Unsplash"
  },
  {
    "objectID": "posts/discovr_08_the_glm/index.html",
    "href": "posts/discovr_08_the_glm/index.html",
    "title": "discovr_08 - General Linear Model",
    "section": "",
    "text": "library(tidyverse, ggplot2)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(ggfortify)\nlibrary(robust)\n\nLoading required package: fit.models\n\nalbum_tib &lt;- here::here(\"data/album_sales.csv\") |&gt; readr::read_csv()\n\nRows: 200 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): album_id\ndbl (4): adverts, sales, airplay, image\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nsoc_anx_tib &lt;- here::here(\"data/social_anxiety.csv\") |&gt; readr::read_csv()\n\nRows: 134 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (4): spai, iii, obq, tosca\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nmetal_tib &lt;- here::here(\"data/metal_health.csv\")  |&gt; readr::read_csv()\n\nRows: 2506 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (2): hm, suicide\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message."
  },
  {
    "objectID": "posts/discovr_08_the_glm/index.html#interpretation",
    "href": "posts/discovr_08_the_glm/index.html#interpretation",
    "title": "discovr_08 - General Linear Model",
    "section": "Interpretation",
    "text": "Interpretation\n\n3 predictors have reasonably linear relationships with album sales and no obvious outliers (except bottom left of ‘band image’ scatterplots)\nacross the diagonal (distributions)\n\nadvertising is very skewed\nairplay and sales look heavy-tailed\n\ncorrelations in the plot give us an idea of the relationships between predictors and outcome\nif we ignore album sales, the highest correlation is between image ratings and amount of airplay, which is significant and the 0.01 level (\\(r=0.18\\))\nfocussing on the outcome variable, adverts and airplay correlate best with the outcome (\\(r=0.58\\) and \\(r=0.6\\))"
  },
  {
    "objectID": "posts/discovr_08_the_glm/index.html#one-predictor",
    "href": "posts/discovr_08_the_glm/index.html#one-predictor",
    "title": "discovr_08 - General Linear Model",
    "section": "One predictor",
    "text": "One predictor\n\nFitting the model\n\npredicting sales from advertising alone\n\n\\[Y_i=b_0+{b_1}{X_i}+\\epsilon_i\\]\n\\[\\text{Sales}_i=b_0+{b_1}{\\text{Advertising}_i}+\\epsilon_i\\]\n\nit is clear from the bottom left scatterplot and the correlation (\\(r=0.58\\)) that a positive relation exists. More advertising money spent leads to greater album sales.\nsome albums sell well regardless of advertising (top-left of scatterplot)\nno albums sell badly when adverts are high (bottom-right of scatterplot)\nto fit a linear model, we use lm() function my_model &lt;- lm(outcome ~ predictor(s), data = tibble, na.action = an action)\n\nmy_model is the name of the model\noutcome is the name of the outcome variable (sales)\npredictor is the name of the predictor variable (adverts) or, a list of variables separated by + symbols\ntibble is the name of the tibble containing the data (album_tib)\n\nthis function maps directly to the equation for the model\n\nadverts ~ sales maps to \\(\\text{Sales}_i=b_0+{b_1}{\\text{Advertising}_i}+\\epsilon_i\\) except we ignore the error term and parameter estimates and we replace the = with ~ (which means ‘predicted from’)\n\n\n\nalbum_lm &lt;- lm(sales ~ adverts, data = album_tib, na.action = na.exclude)\nsummary(album_lm)\n\n\nCall:\nlm(formula = sales ~ adverts, data = album_tib, na.action = na.exclude)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-152.949  -43.796   -0.393   37.040  211.866 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 1.341e+02  7.537e+00  17.799   &lt;2e-16 ***\nadverts     9.612e-02  9.632e-03   9.979   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 65.99 on 198 degrees of freedom\nMultiple R-squared:  0.3346,    Adjusted R-squared:  0.3313 \nF-statistic: 99.59 on 1 and 198 DF,  p-value: &lt; 2.2e-16\n\nbroom::glance(album_lm)  |&gt; \n  knitr::kable(digits = 3)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nr.squared\nadj.r.squared\nsigma\nstatistic\np.value\ndf\nlogLik\nAIC\nBIC\ndeviance\ndf.residual\nnobs\n\n\n\n\n0.335\n0.331\n65.991\n99.587\n0\n1\n-1120.688\n2247.375\n2257.27\n862264.2\n198\n200\n\n\n\n\n\n\nnote \\(df=1\\) and \\(df.residual=198\\) therefore we can say that adding the predictor of advertising significantly improved the fit of the model to the data compared to having no predictors in the model\n\\(F(1,198)=99.59, p&lt;.001\\)\n\n\n\nModel Parameters\nTo see model parameters, use broom::tidy()\nbroom::tidy(model_name, conf.int = FALSE, conf.level = 0.95)\n\nput the model name into the function, then two optional arguments\n\nconfidence intervals conf.int=TRUE (confidence intervals are not included by default)\ndefault is 95%, but you can change it with conf.level=.99 for 99% confidence interval\n\n\n\nbroom::tidy(album_lm, conf.int = TRUE)\n\n# A tibble: 2 × 7\n  term        estimate std.error statistic  p.value conf.low conf.high\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept) 134.       7.54        17.8  5.97e-43 119.       149.   \n2 adverts       0.0961   0.00963      9.98 2.94e-19   0.0771     0.115\n\n\n\noutput provides estimates of the model parameters (\\(\\hat{b}\\)-values)\n\\(Y\\) intercept (\\(b_0\\)) is 134.14 (when \\(X\\) is 0, sales will be 134140)\n\\(X\\) (\\(b_1\\)) is 0.096.\n\nrepresents the change in outcome associated with a unit change in predictor.\nwhen the predictor increases by 1, the outcome increases by .096, but since the units of measurement was thousands of pounds and thousands of sales, an increase in $1000 will lead to 96 more albums sold\nnot very good return\nBUT, we know that advertising only accounts for 1/3 of the variance\n\nIf a predictor is having a significant impact on our ability to predict the outcome, then \\(\\hat{b}\\) should be different from 0 and large relative to its standard error\nthe \\(t\\)-test (labelled statistic) and the associated \\(p\\)-value tell us whether the \\(\\hat{b}\\) is significantly different from 0\nthe column p.value contains the exact probability that a value of \\(t\\) at least as big as the one in the table would occur if the value of \\(b\\) in the population were 0\nif this propbability is less than 0.05, then we interpret that as the predictor being a significant predictor of hte outcome.\nfor both \\(t\\)s, the probabilities are in scientific notation\n\n2.91e-19 means \\(2.91*10^{-19}\\), or move the decimal 19 places to the left or 0.000000000000000000291\n2.91e+19 means \\(2.91*10^{19}\\), or move the decimal 19 places to the right or 29100000000000000000\n\n\nboth values are 0 at 3 decimal places\n\n\n\nExploring the standard error of \\(\\hat{b}\\)\nhttps://www.youtube-nocookie.com/embed/3L9ZMdzJyyI?si=ET90VDYq3RVKnKDq\n\n\nConfidence intervals for \\(\\hat{b}\\)\nImagine we collect 100 samples of data measuring the same variables as the current model, then estimate the same model, including confidence intervals for unstandardized values.The boundaries are constructed such that 95% of our 100 samples contain the population value of \\(b\\). 95 of 100 sample will yield confidence intervals for \\(b\\) that contain the population value, but we don’t know if our sample is one of the 95.\nWe might just assume that it does, but if the confidence interval contains 0, then there is a possibility that there is no relationship, or the relationship might be negative. The trouble is that we would be wrong 5% of the time.\nIf the interval does not contain 0, we might conclude there is a genuine positive relationship.\n\n\nUsing the model\n\\[\\text{Sales}_i=\\hat{b_0}+\\hat{b_1}{\\text{Advertising}_i}\\]\n\\[\\text{Sales}_i=134.14+.096*{Advertising}_i\\]\nNow we can make a prediction by entering a value for the advertising budget, say 100 (equal to 100,000 gbp)\n\\[\\text{Sales}_i=134.14+.096*{100}_i\\]\n\\[\\text{Sales}_i=143.74\\]\nor 143,740 sales"
  },
  {
    "objectID": "posts/discovr_08_the_glm/index.html#several-predictors",
    "href": "posts/discovr_08_the_glm/index.html#several-predictors",
    "title": "discovr_08 - General Linear Model",
    "section": "Several Predictors",
    "text": "Several Predictors\nadd multiple predictors hierarchically, after advertising is shown to be significant\n\\[Y_i=b_0+{b_1}{X_1i}+{b_2}{X_2i}+{b_3}{X_3i}+ ... +{b_n}{X_ni}\\epsilon_i\\]\n\\[Y_i=b_0+{b_1}{advertising_i}+{b_2}{airplay_i}+{b_3}{image_i}+\\epsilon_i\\]\n\nBuilding the model\n\nadd predictors to the R code the same way as we do in the equation, by adding +\n\n\nalbum_full_lm &lt;- lm(sales ~ adverts + airplay + image, data = album_tib, na.action = na.exclude)\nsummary(album_full_lm)\n\n\nCall:\nlm(formula = sales ~ adverts + airplay + image, data = album_tib, \n    na.action = na.exclude)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-121.324  -28.336   -0.451   28.967  144.132 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -26.612958  17.350001  -1.534    0.127    \nadverts       0.084885   0.006923  12.261  &lt; 2e-16 ***\nairplay       3.367425   0.277771  12.123  &lt; 2e-16 ***\nimage        11.086335   2.437849   4.548 9.49e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 47.09 on 196 degrees of freedom\nMultiple R-squared:  0.6647,    Adjusted R-squared:  0.6595 \nF-statistic: 129.5 on 3 and 196 DF,  p-value: &lt; 2.2e-16\n\nbroom::glance(album_full_lm)  |&gt; \n  knitr::kable(digits = 3)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nr.squared\nadj.r.squared\nsigma\nstatistic\np.value\ndf\nlogLik\nAIC\nBIC\ndeviance\ndf.residual\nnobs\n\n\n\n\n0.665\n0.66\n47.087\n129.498\n0\n3\n-1052.168\n2114.337\n2130.828\n434574.6\n196\n200\n\n\n\n\n\n\nr.squared \\(R^2\\) tells us the variance in album sales -&gt; .665, or 66.5%.\n\\(R^2\\) for adverts was 0.335, so the difference -&gt; .665-.335=.33 means that airplay and image account for a further 33% of the variance\nadjusted \\(R^2\\) adj.r.squared tells us how well the model generalizes, and the value should be close to r.squared.\n\nin this case it is .66, or only .005 away\n\n\\(F\\)-statistic is the ratio of the improvementin prediction that results from fitting the model, relative to the inaccuracy that sitll exists in the model\nthe variable p.value contains the p-value associated with \\(F\\) -&gt; in this case is \\(2.88×10^{−46}\\) -&gt; much smaller than .001\ndegrees of freedom are df and df.residual\nwe can interpret the result as meaning that the model significantly improves our ability to predict the outcome variable ocmpared to not fitting the model.\nreported as \\(F(3, 196)=129.5, p=&lt;0.001\\)"
  },
  {
    "objectID": "posts/discovr_08_the_glm/index.html#comparing-models",
    "href": "posts/discovr_08_the_glm/index.html#comparing-models",
    "title": "discovr_08 - General Linear Model",
    "section": "Comparing Models",
    "text": "Comparing Models\n\nwe can compare hierarchical models using an \\(F\\)-statisticusing the anova() function -&gt; anova(model_1, model_2, … , model_n)\nlist the models in order that we want to compare\n\n\nanova(album_lm, album_full_lm) |&gt;\nbroom::tidy()\n\n# A tibble: 2 × 7\n  term                      df.residual    rss    df   sumsq statistic   p.value\n  &lt;chr&gt;                           &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 sales ~ adverts                   198 8.62e5    NA     NA       NA   NA       \n2 sales ~ adverts + airpla…         196 4.35e5     2 427690.      96.4  6.88e-30\n\n\n\n\\(F\\) (statistic) is 96.5\n\\(p\\)-value (p.value) is \\(6.8793949^{-30}\\)\ndf is 2 (difference between df in 2 models)\ndf.residual is 196\nConclusion -&gt; adding airplay and image to the model significantly improved model fit\n\n\\(F(2, 196) = 96.45, p &lt; 0.001\\)\n\n\n\n\n\n\n\n\nTip\n\n\n\nWe can only compare hierarchical models; that is to say that the second model must contain everything that was in the first model plus something new, and the third model must contain everything in the second model plus something new, and so on.\n\n\n\nModel parameter estimates (\\(\\hat{b}\\))\n\nbroom::tidy(album_full_lm, conf.int = TRUE)  |&gt; \n  knitr::kable(digits = 3)\n\n\n\n\n\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n\n(Intercept)\n-26.613\n17.350\n-1.534\n0.127\n-60.830\n7.604\n\n\nadverts\n0.085\n0.007\n12.261\n0.000\n0.071\n0.099\n\n\nairplay\n3.367\n0.278\n12.123\n0.000\n2.820\n3.915\n\n\nimage\n11.086\n2.438\n4.548\n0.000\n6.279\n15.894\n\n\n\n\n\n\noutput gives estimates of the \\(b\\)-balues (column labelled estimate) and statistics that indicate the individual contribution of each predictor in the model.\nall three predictors have positive \\(\\hat{b}\\)values, indicating positive relationships\n\nas all three predictors increase, so do album sales\n\n\\(\\hat{b}\\) vlaues also tell us to what degree each predictor affects the outcome if the effects of all other predictors are held constant\n\nadvertising budget -&gt; \\(\\hat{b}=0.085\\) -&gt; as advertising increases by 1 unit (1000gbp), sales increases by 0.085 units (85 albums), but this is only true if the other two predictors are held constant\nairplay \\(\\hat{b}=3.367\\) -&gt; as airplay prior to release increases by one unit (1 play), album sales increase by 3.367 units (3367 album sales, but only if the other two predictors are held constant)\n\n\n\n\n\n\n\n\nHow would we interpret the 𝑏̂ (11.086) for band image?\n\n\n\n\n\nIf a band can increase their image rating by 1 unit they can expect additional album sales of 11,086 units\n\n\n\n\n\nStandardized \\(\\hat{b}\\)s\nThe lm() function does not provide standardized betas, so use model_parameters()\nparameters::model_parameters(my_model, standardize = refit)\n\nparameters::model_parameters(album_full_lm, standardize = \"refit\") |&gt; \n  knitr::kable(digits = 3)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nParameter\nCoefficient\nSE\nCI\nCI_low\nCI_high\nt\ndf_error\np\n\n\n\n\n(Intercept)\n0.000\n0.041\n0.95\n-0.081\n0.081\n0.000\n196\n1\n\n\nadverts\n0.511\n0.042\n0.95\n0.429\n0.593\n12.261\n196\n0\n\n\nairplay\n0.512\n0.042\n0.95\n0.429\n0.595\n12.123\n196\n0\n\n\nimage\n0.192\n0.042\n0.95\n0.109\n0.275\n4.548\n196\n0\n\n\n\n\n\n\nadvertising budget -&gt; Standardized \\(\\hat{\\beta}=0.511\\)\n\nas the advertising budgetr increases by one standard deviation (485,655gbp), album sales increase by .511 standard deviations.\nthe standard deviation for album sales is 80,699, so .511 SD is 41,240 sales\nfor every increase in advert budget of 485655gbp, album sales will increase by 41,240 IF the other predictors are held constant\n\nImage -&gt; Standardized \\(\\hat{\\beta}=0.192\\)\n\na band rated 1 SD on the image scale (1.4 units) will sell .192 SD more units.\nthis is a change of 15,490 sales IF the other predictors are held constant\n\n\n\n\n\n\n\n\nHow would we interpret the Standardized B (0.512) for airplay?\n\n\n\n\n\nAs the number of plays on radio in the week before release increases by 1 standard deviation, album sales increase by 0.512 standard deviations\n\n\n\n\n\nConfidence Intervals\n\n\n\n\n\n\nThe confidence interval for airplay ranges from 2.82 to 3.92. What does this tell us?\n\n\n\n\n\n\nThe probability of this confidence interval containing the population value is 0.95.\nIf this confidence interval is one of the 95% that contains the population value then the population value of b lies between 2.82 and 3.92.\nI can be 95% confident that the population value of b lies between 2.82 and 3.92\nThere is a 95% chance that the population value of b lies between 2.82 and 3.92\n\n\n\n\nAssuming that each confidence interval is one of the 95% that contains the population parameter: - the true size of the relationship between advertising budget and album sales lies somewhere between 0.071 and 0.099 - the true size of the relationship between band image and album sales lies somewhere between 6.279 and 15.894\n\nthe two best predictors have tight confidence intervals (airplay and adverts) indicating that the estimates are likely representative of the true population values\nthe interval for band image is much wider, but does not cross zero indicating this parameter is less representative of the population, but is still significant.\n\n\n\nSignificance tests\nThe values in statistic are the values of \\(t\\) associated with each \\(\\hat{b}\\) and p.value is the associated significance of the \\(t\\)-statistic. For every predictor, the \\(\\hat{b}\\) is significantly different from 0 (\\(p&lt;.001\\)) meaning that all predictors significantly predict album sales.\n\n\n\n\n\n\nHow might we interpret the statistic and p.value for the three predictors?\n\n\n\n\n\n\nThe probability of the null hypothesis is less than 0.001 in all cases\nThe probability that each b is a chance result is less than 0.001\nThey tell us that the probability of getting a value of t at least as big as these values if the value of b were, in fact, zero is smaller than 0.001 for all predictors.\n\n\n\n\n\n\n\n\n\n\n\\(p\\)-values\n\n\n\nMany students and researchers think of p-values in terms of the ‘probability of a chance result’ or ‘the probability of a hypothesis being true’ but they are neither of these things. They are the long-run probability that you would get a test-statistic (in this case t) at least as large as the one you have if the null hypothesis were true. In other words, if there really were no relationship between advertising budget and album sales (the null hypothesis) then the population value of b would be zero.\nImagine we sampled from this null population and computed t, and then repeated this process 1000 times. We’d have 1000 values of t from a population in which there was no effect. We could plot these values as a histogram. This would tell us how often certain values of t occur. From it we could work out the probability of getting a particular value of t. If we then took another sample, and computed t (because we’re kind of obsessed with this sort of thing) we would be able to compare this value of t to the distribution of all the previous 1000 samples. Is the t in our current sample large of small compared to the others? Let’s say it was larger than 999 of the previous values. That would be quite an unlikely value of t whereas if it was larger than 500 of them this would not surprise us. This is what a p-value is: it is the long run probability of getting test statistic at least as large as the one you have if the null hypothesis were true. If the value is less than 0.05, people typically take this as supporting the idea that the null hypothesis isn’t true.\n\n\n\n\n\n\n\n\nReport\n\n\n\n\nthe model that included the band’s image and airplay was significantly better fit than the model that included advertising budget alone \\(f(2, 196)=96.45, p&lt;0.001\\)\nthe final model explained 66.5% of the variance in album sales\nadvertising budget significantly predicted album sales \\(\\hat{b}=0.08[0.07, 0.10], t(196)=12.26, p&gt;.001\\)\nairplay significantly predicted album sales \\(\\hat{b}=3.37[2.82, 3.92], t(196)=12.12, p&lt;.001\\)\nband image significantly predicted album sales -&gt; \\(\\hat{b}=11.09[6.28, 15.89], t=4.55, p&lt;.001\\)"
  },
  {
    "objectID": "posts/discovr_08_the_glm/index.html#unguided-example",
    "href": "posts/discovr_08_the_glm/index.html#unguided-example",
    "title": "discovr_08 - General Linear Model",
    "section": "Unguided Example",
    "text": "Unguided Example\n\nMetal and mental health\n\nmetal_tib\n\n# A tibble: 2,506 × 2\n      hm suicide\n   &lt;dbl&gt;   &lt;dbl&gt;\n 1    10       7\n 2    10      11\n 3    10       8\n 4     4      14\n 5    10      13\n 6     8      12\n 7     8       7\n 8     5      15\n 9     8       7\n10     9      10\n# ℹ 2,496 more rows\n\n\n\nGGally::ggscatmat(metal_tib, columns=c(\"hm\", \"suicide\")) + theme_minimal()\n\nWarning: Removed 200 rows containing non-finite outside the scale range\n(`stat_density()`).\n\n\nWarning: Removed 284 rows containing non-finite outside the scale range\n(`stat_density()`).\n\n\n\n\n\n\n\n\n\n\nmetal_lm &lt;- lm(suicide ~ hm, data = metal_tib, na.action = na.exclude)\nbroom::glance(metal_lm) |&gt;\nknitr::kable(digits=3)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nr.squared\nadj.r.squared\nsigma\nstatistic\np.value\ndf\nlogLik\nAIC\nBIC\ndeviance\ndf.residual\nnobs\n\n\n\n\n0.125\n0.125\n4.842\n304.784\n0\n1\n-6401.851\n12809.7\n12826.7\n50047.05\n2135\n2137\n\n\n\n\n\n\nbroom::tidy(metal_lm, conf.int = TRUE, conf.level=0.95)\n\n# A tibble: 2 × 7\n  term        estimate std.error statistic  p.value conf.low conf.high\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)   16.0      0.261       61.4 0          15.5      16.5  \n2 hm            -0.612    0.0350     -17.5 6.64e-64   -0.680    -0.543\n\n\n\n\n\n\n\n\nHow much variance does the final model explain?\n\n\n\n\n\n12.5%\n\n\n\n\n\n\n\n\n\nWhat is the nature of the relationship between listening to heavy metal and suicide risk?\n\n\n\n\n\n\nAs love of heavy metal increases, suicide risk decreases\nbecause the \\(\\hat{b}\\) value is negative (-.0612)\n\n\n\n\n\n\n\n\n\n\nAs listening to heavy metal increases by 1 unit, by how much does suicide risk change?\n\n\n\n\n\n\n-0.612 units\n\n\n\n\n\n\nPredicting Social Anxiety\n\nsoc_anx_tib\n\n# A tibble: 134 × 4\n    spai   iii   obq tosca\n   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1    26 56.5   4.43  4.18\n 2    51 16.1   2.03  4.20\n 3    33 37.4   2.59  3.25\n 4   106 16.7   4.84  4.07\n 5    25  5.16  1.76  3.80\n 6   109 36.0   2.13  4.25\n 7    39 34.5   2.01  4.95\n 8   134 18.0   1.76  4.52\n 9    43 12.9   2.29  3.59\n10    57  7.10  3.20  3.64\n# ℹ 124 more rows\n\n\n\nGGally::ggscatmat(soc_anx_tib, columns = c(\"spai\", \"tosca\"))  +\ntheme_minimal()\n\n\n\n\n\n\n\n\n\nsoc_anx_lm &lt;- lm(spai ~ tosca, data = soc_anx_tib, na.action = na.exclude)\nbroom::glance(soc_anx_lm)  |&gt; \n  knitr::kable(digits = 3)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nr.squared\nadj.r.squared\nsigma\nstatistic\np.value\ndf\nlogLik\nAIC\nBIC\ndeviance\ndf.residual\nnobs\n\n\n\n\n0.102\n0.095\n29.918\n14.989\n0\n1\n-644.525\n1295.05\n1303.743\n118153.3\n132\n134\n\n\n\n\nbroom::tidy(soc_anx_lm, conf.int = TRUE, conf.level = 0.95)\n\n# A tibble: 2 × 7\n  term        estimate std.error statistic  p.value conf.low conf.high\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)    -54.8     30.0      -1.82 0.0703     -114.       4.60\n2 tosca           27.4      7.08      3.87 0.000169     13.4     41.4 \n\n\n\nsoc_anx_obq_lm &lt;- lm(spai ~ tosca + obq, data = soc_anx_tib, na.action = na.exclude)\nbroom::glance(soc_anx_obq_lm)  |&gt;\n  knitr::kable(digits = 3)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nr.squared\nadj.r.squared\nsigma\nstatistic\np.value\ndf\nlogLik\nAIC\nBIC\ndeviance\ndf.residual\nnobs\n\n\n\n\n0.148\n0.135\n29.015\n11.218\n0\n2\n-630.333\n1268.666\n1280.197\n108599.6\n129\n132\n\n\n\n\nbroom::tidy(soc_anx_obq_lm, conf.int = TRUE, conf.level = 0.95)\n\n# A tibble: 3 × 7\n  term        estimate std.error statistic p.value conf.low conf.high\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)   -53.3      29.2      -1.82 0.0704   -111.        4.50\n2 tosca          22.1       7.24      3.05 0.00276     7.77     36.4 \n3 obq             7.25      2.91      2.49 0.0141      1.49     13.0 \n\n\n\nparameters::model_parameters(soc_anx_obq_lm, standardize = \"refit\") |&gt; \n  knitr::kable(digits = 3)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nParameter\nCoefficient\nSE\nCI\nCI_low\nCI_high\nt\ndf_error\np\n\n\n\n\n(Intercept)\n0.000\n0.081\n0.95\n-0.160\n0.160\n0.000\n129\n1.000\n\n\ntosca\n0.261\n0.086\n0.95\n0.092\n0.430\n3.052\n129\n0.003\n\n\nobq\n0.213\n0.086\n0.95\n0.044\n0.382\n2.489\n129\n0.014\n\n\n\n\n\nanova(soc_anx_lm, soc_anx_obq_lm) |&gt;\nbroom::tidy()\n\n\n\n\n\n\nError message\n\n\n\nError in anova.lmlist(object, …) : models were not all fitted to the same size of dataset\n\nas of this writing (Mar-04-24), there is a solution listed in the discovr_08 tutorial to start by finding the error with soc_anx_tib |&gt; dplyr::summarise( across(.fns = list(valid = ~sum(!is.na(.x)), missing = ~sum(is.na(.x))), .names = \"{.col}_{.fn}\") )\n\nThe output from running this function in the tutorial is a table showing that there are missing values in obq. this means that spai and tosca have 134 values and obq has 132 values, so they are not the same size.\n\nHowever, the output above in my own .qmd file throws a deprecated error in dplyr 1.1.4 that the use of across without .cols is deprecated since dplyr 1.1.1\nthis means that I can’t currently generate the table showing the sizes of the datasets with the code provided\nthe workaround to the problem is normally to use multiple imputation to estimate the missing values, which is beyond the scope of this tutorial, so Field recommends a very bad practice, which is to omit the missing values.\nI’ve filed an issue in the discover.rocks repo\nFix’t\n\n\n\nHere is the fixed code, adding .cols = everything(), and the intended output\n\nsoc_anx_tib |&gt;\n  dplyr::summarise(\n    across(.cols = everything(), .fns = list(valid = ~sum(!is.na(.x)), missing = ~sum(is.na(.x))), .names = \"{.col}_{.fn}\")\n    )\n\n# A tibble: 1 × 8\n  spai_valid spai_missing iii_valid iii_missing obq_valid obq_missing\n       &lt;int&gt;        &lt;int&gt;     &lt;int&gt;       &lt;int&gt;     &lt;int&gt;       &lt;int&gt;\n1        134            0       129           5       132           2\n# ℹ 2 more variables: tosca_valid &lt;int&gt;, tosca_missing &lt;int&gt;\n\n\n\nsoc_anx_lm &lt;- soc_anx_tib |&gt;\n  dplyr::select(-iii) |&gt; \n  na.omit() |&gt; \n  lm(spai ~ tosca, data = _)\nanova(soc_anx_lm, soc_anx_obq_lm) |&gt; \n  broom::tidy() |&gt; \n  knitr::kable(digits = 3)\n\n\n\n\n\n\n\n\n\n\n\n\n\nterm\ndf.residual\nrss\ndf\nsumsq\nstatistic\np.value\n\n\n\n\nspai ~ tosca\n130\n113813.1\nNA\nNA\nNA\nNA\n\n\nspai ~ tosca + obq\n129\n108599.6\n1\n5213.423\n6.193\n0.014\n\n\n\n\n\n\n\n\n\n\n\nHow much variance in social anxiety do OCD and shame account for?\n\n\n\n\n\n14.8%\n\n\n\n\n\n\n\n\n\nThe confidence interval for shame ranges from 7.77 to 36.42. What does this tell us?\n\n\n\n\n\nIf this confidence interval is one of the 95% that contains the population value then the population value of b lies between 7.77 and 36.42.\n\n\n\n\n\n\n\n\n\nAs shame increases by 1 unit, by how much does social anxiety change?\n\n\n\n\n\n22.10 units\n\n\n\n\n\n\n\n\n\nAs OCD increases by 1 standard deviation, by how many standard deviations does social anxiety change?\n\n\n\n\n\n0.213\n\n\n\n\n\n\n\n\n\nThe p-value for OCD is 0.014, what does this mean?\n\n\n\n\n\nThe probability of getting a value of t at least as big as 2.49 if the value of b were, in fact, zero is 0.014. I’m going to assume, therefore, that b isn’t zero (i.e. OCD significantly predicts social anxiety.)"
  },
  {
    "objectID": "posts/discovr_08_the_glm/index.html#the-beast-of-bias",
    "href": "posts/discovr_08_the_glm/index.html#the-beast-of-bias",
    "title": "discovr_08 - General Linear Model",
    "section": "The Beast of Bias",
    "text": "The Beast of Bias\n\n\n\n\n\n\nWhich of these assumptions of the linear model is the most important\n\n\n\n\n\nLinearity and additivity\nThis assumption is the most important because if it is not met then the phenomenon you’re trying to model is not well represented by the model you are trying to fit\n\n\n\n\n\n\n\n\n\nWhich of these assumptions of the linear model is the least important\n\n\n\n\n\nNormality of errors\nThis assumption is the least important because even with non-normal errors the parameter estimates (using ordinary least squares methods) of the model will be unbiased (they match the expected population value) and optimal (they minimize the squared error).\n\n\n\n\n\n\n\n\n\nWhat does homoscedasticity mean?\n\n\n\n\n\nThe variance in errors from the (population) model is constant at all levels of the predictor variable(s)\n\n\n\n\nDiagnostic Plots\nUse plot() function\nplot(my_model, which = numbers_of_the_plots_you_want)\n\nplot(album_full_lm)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nplot() function can produce 6 plots (below) - default is to produce 1,2,3,5.\n\npredicted values from the model (x-axis) against the residuals (y-axis). Use to look for linearity and homoscedasticity.\nA Q-Q plot of standardised residuals to look for normality of residuals\npredicted values from the model(x-axis) against the square root of the standardized residuals (y-axis). This is a variant of plot 1 and is used to look for linearity and homoscedasticity.\nCase number (x-axis) against the Cooks distance (y-axis). This plot can help identify influential cases (large values of Cooks distance).\nThe leverage value for each case (x-axis) against the standardised residual (y-axis). Used to identify influential cases and outliers. Leverage values indicate the influence of an individual case on the model and are related to the Cooks distance.\nThe leverage value for each case (x-axis) against the corresponing Cooks distance (y-axis). Used to identify influential cases and outliers.\n\nTo get plot #1\n\nplot(album_full_lm, which = 1)\n\n\n\n\n\n\n\n\nTo get plot #1 and #2\n\nplot(album_full_lm, which = c(1, 2))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nTo get all six, use which = 1:6\n\n\nPlots below show the patterns of dots we want (random = assumptions met)\n\ncurvature indicates lack of linearity\nfunnel shape indicates heteroscedasticity\ncurvature and funnel shape indicate non-linearity and heteroscedasticity\n\n\nFigure 2 \n\nplot(album_full_lm, which = c(1,3))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIf both assumptions of linearity and homoscedasticity are met, then these plots should look like a random array of dots and the red trend line should be flat.\n\n\n\n\n\n\nComparing the plot to those in Figure 2, how would you interpret it?\n\n\n\n\n\nNo problems. random array of dots, no funnels, no bananas.\n\n\n\n\nplot(album_full_lm, which = 2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBased on the Q-Q plot, can we assume normality of the residuals?\n\n\n\n\n\nYes, the dots line up along the diagonal, indicating a normal distribution.\n\n\n\n\n\nPretty residual plots\nwith ggfortify, we can use ggplot2::autoplot() to produce nicely formatted plots that are ggplot2 objects, including themes\n\n\n\n\n\n\nUsing GGfortify\n\n\n\nfor autoplot() function to work, the ggfortify package must be loaded. We can’t use verbose code because ggfortify adds functionality to ggplot2 rather than to itself. Need to include library(ggfortify) at the beginning of the document.\n\n\n\nggplot2::autoplot(album_full_lm,\n                  which = c(1, 2, 3),\n                  colour = \"#440154\",\n                  smooth.colour = \"#5ec962\",\n                  alpha = 0.5,\n                  size = 1) + \n  theme_minimal()"
  },
  {
    "objectID": "posts/discovr_08_the_glm/index.html#influential-cases-and-outliers-plots",
    "href": "posts/discovr_08_the_glm/index.html#influential-cases-and-outliers-plots",
    "title": "discovr_08 - General Linear Model",
    "section": "Influential cases and outliers: plots",
    "text": "Influential cases and outliers: plots\n\nuse Cook’s distance to identify influential cases and\nuse standardized residuals to check for outliers\n\n\n\n\n\n\n\nImportant\n\n\n\n\nin an average sample,\n\n95% of standardized residuals should lie between \\(\\pm1.96\\) and\n99% should lie between \\(\\pm2.58\\), and\nany case for which absolute value of the standardized residual is 3 or more, is likely to be an outlier\n\nCook’s distance measure the influence of a single case on model as a whole.\n\nabsolute values greater than 1 may be cause for concern\n\n\n\n\n\nggplot2::autoplot(album_full_lm,\n                  which = c(4:6),\n                  colour = \"#440154\",\n                  smooth.colour = \"#5ec962\",\n                  alpha = 0.5,\n                  size = 1) + \n  theme_minimal()\n\n\n\n\n\n\n\n\n\nFirst plot show Cook’s distance and labels cases with largest values (1, 164, 169), but largest values are in the region of 0.06, well below the threshold of 1.\nsecond shows leverage values plotted against standardized residuals\n\nwe want the green trend line to be flat and lie along zero, which is the case here\nif the trend line deviates substantially from horizontal, then it indicates one of the assumptions of the model has been violated\nthis plot usually also has red dashed lines indicating values of Cook’s distance of 0.5 and 1. this plot has none, indicating all values of Cook’s distance are well below thresholds.\n\nFinal plot shows leverage, Cook’s distance, and the standardized residual on the same plot.\n\ncan be used to identify cases that have high leverage, high Cook’s distance, large residual, or some combination of all three.\ne.g. across the plots, case 164 has a standardized residual between -2.5 and -3 and the largest Cook’s distance (although still only around 0.7)"
  },
  {
    "objectID": "posts/discovr_08_the_glm/index.html#influential-cases-and-outliers-numbers",
    "href": "posts/discovr_08_the_glm/index.html#influential-cases-and-outliers-numbers",
    "title": "discovr_08 - General Linear Model",
    "section": "Influential cases and outliers: numbers",
    "text": "Influential cases and outliers: numbers\nFor a more precise look, we can see values for Cook’s distance and standardized residuals using broom::augment(). All we need to do is pass the lm object into the function and save the results as a new tibble. also useful to save the case number as a variable so that you can identify cases should you need to.\n\ncreate a new tibble called album_full_rsd (rsd for residuals) by\n\npiping model into broom::augment() to get residuals, then\ninto tibble::row_id_to_column() to create a variable that contains the row number\n\nthe var = \"case_no\" tells the function to name the variable containing the numbers case_no\nthe result is a tibble called album_full_rsd that contains the case number, the original data to which the model was fitted, and various diagnostic statistics.\n\n\n\n\n\n\n\nDe-bug: residuals when you have missing values\n\n\n\nIf you have missing values in the data and used na.action = na.exclude when fitting the model, you must also tell augment() where to find the original data so it can map the residuals to the original cases.\nIn this case: album_full_rsd &lt;- album_full_lm |&gt;    broom::augment(data = album_tib)\n\n\n\nalbum_full_rsd &lt;- album_full_lm |&gt; \n  broom::augment() |&gt; \n  tibble::rowid_to_column(var = \"case_no\") \nhead(album_full_rsd) |&gt;\n  tibble::glimpse() |&gt;\n    knitr::kable(digits = 2)\n\nRows: 6\nColumns: 11\n$ case_no    &lt;int&gt; 1, 2, 3, 4, 5, 6\n$ sales      &lt;dbl&gt; 330, 120, 360, 270, 220, 170\n$ adverts    &lt;dbl&gt; 10.256, 985.685, 1445.563, 1188.193, 574.513, 568.954\n$ airplay    &lt;dbl&gt; 43, 28, 35, 33, 44, 19\n$ image      &lt;dbl&gt; 10, 7, 7, 7, 5, 5\n$ .fitted    &lt;dbl&gt; 229.9203, 228.9490, 291.5576, 262.9760, 225.7529, 141.0954\n$ .resid     &lt;dbl&gt; 100.079745, -108.948992, 68.442368, 7.024026, -5.752861, 28…\n$ .hat       &lt;dbl&gt; 0.047190526, 0.008006536, 0.020700427, 0.012560946, 0.02606…\n$ .sigma     &lt;dbl&gt; 46.63346, 46.55347, 46.94739, 47.20520, 47.20607, 47.16186\n$ .cooksd    &lt;dbl&gt; 5.870388e-02, 1.088943e-02, 1.140066e-02, 7.166478e-05, 1.0…\n$ .std.resid &lt;dbl&gt; 2.1774041, -2.3230828, 1.4688016, 0.1501160, -0.1237983, 0.…\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncase_no\nsales\nadverts\nairplay\nimage\n.fitted\n.resid\n.hat\n.sigma\n.cooksd\n.std.resid\n\n\n\n\n1\n330\n10.26\n43\n10\n229.92\n100.08\n0.05\n46.63\n0.06\n2.18\n\n\n2\n120\n985.68\n28\n7\n228.95\n-108.95\n0.01\n46.55\n0.01\n-2.32\n\n\n3\n360\n1445.56\n35\n7\n291.56\n68.44\n0.02\n46.95\n0.01\n1.47\n\n\n4\n270\n1188.19\n33\n7\n262.98\n7.02\n0.01\n47.21\n0.00\n0.15\n\n\n5\n220\n574.51\n44\n5\n225.75\n-5.75\n0.03\n47.21\n0.00\n-0.12\n\n\n6\n170\n568.95\n19\n5\n141.10\n28.90\n0.01\n47.16\n0.00\n0.62\n\n\n\n\n\nStandardized residuals are in a variable called .std.resid and the Cook’s distances in .cooksd\nto see what percentage of standardized residuals fall outside the limits of 1.96 (95%), 2-2.5 (99%) or above 3, we can filter the tibble using the abs() function to return the absolute value (ignoring the plus/minus signs) of the residual\n\nalbum_full_rsd |&gt; \n  dplyr::filter(abs(.std.resid) &gt;= 1.96) |&gt;\n  dplyr::select(case_no, .std.resid, .resid) |&gt; \n  dplyr::arrange(.std.resid)\n\n# A tibble: 13 × 3\n   case_no .std.resid .resid\n     &lt;int&gt;      &lt;dbl&gt;  &lt;dbl&gt;\n 1     164      -2.63 -121. \n 2      47      -2.46 -115. \n 3      55      -2.46 -114. \n 4      68      -2.36 -110. \n 5       2      -2.32 -109. \n 6     200      -2.09  -97.2\n 7     167      -2.00  -93.6\n 8     100       2.10   97.3\n 9      52       2.10   97.4\n10      61       2.10   98.8\n11      10       2.13   99.5\n12       1       2.18  100. \n13     169       3.09  144. \n\n\n\nalbum_full_rsd |&gt; \n  dplyr::filter(abs(.std.resid) &gt;= 2.5) |&gt;\n  dplyr::select(case_no, .std.resid, .resid) |&gt; \n  dplyr::arrange(.std.resid)\n\n# A tibble: 2 × 3\n  case_no .std.resid .resid\n    &lt;int&gt;      &lt;dbl&gt;  &lt;dbl&gt;\n1     164      -2.63  -121.\n2     169       3.09   144.\n\n\n\nalbum_full_rsd |&gt; \n  dplyr::filter(abs(.std.resid) &gt;= 3.0) |&gt;\n  dplyr::select(case_no, .std.resid, .resid) |&gt; \n  dplyr::arrange(.std.resid)\n\n# A tibble: 1 × 3\n  case_no .std.resid .resid\n    &lt;int&gt;      &lt;dbl&gt;  &lt;dbl&gt;\n1     169       3.09   144.\n\n\n\n\n\n\n\n\nWhat percentage of cases have standardized residuals with absolute values greater than 1.96?\n\n\n\n\n\n13/200 = 6.5%\n\n\n\n\n\n\n\n\n\nWhat percentage of cases have standardized residuals with absolute values greater than 2.5?\n\n\n\n\n\n2/200 = 1%\n\n\n\n\n\n\n\n\n\nAll things considered do you think there are outliers?\n\n\n\n\n\nNo, the appropriate proportion of cases have standardized residuals in the expected range and no case has a value grossly exceeding 3.\n\n\n\n\nSomething similar with Cook’s distance…we can filter the tibble to look at cases with cooksd&gt;1, or sort the tibble in descending order using arrange()\nalso use select() so we only see Cook’s value and case numbers\n\n\nalbum_full_rsd |&gt; \n  dplyr::arrange(desc(.cooksd)) |&gt;\n  dplyr::select(case_no, .cooksd)\n\n# A tibble: 200 × 2\n   case_no .cooksd\n     &lt;int&gt;   &lt;dbl&gt;\n 1     164  0.0708\n 2       1  0.0587\n 3     169  0.0509\n 4      55  0.0404\n 5      52  0.0332\n 6     100  0.0314\n 7     119  0.0308\n 8      99  0.0281\n 9     200  0.0251\n10      47  0.0241\n# ℹ 190 more rows\n\n\n\nno cooksd&gt;1 so no cases having undue influence on the model as a whole"
  },
  {
    "objectID": "posts/discovr_08_the_glm/index.html#robust-linear-models",
    "href": "posts/discovr_08_the_glm/index.html#robust-linear-models",
    "title": "discovr_08 - General Linear Model",
    "section": "Robust linear models",
    "text": "Robust linear models\n\nmodel appears to be accurate for the sample and generalizable to the population\nnot always the case though\n2 things to do\n\ntest whether the parameter estimates have been biased\ncheck whether confidence intervals and significance tests have been biased\n\n\n\nRobust parameter estimates\n\nuse robust::lmRob()\nused the same way as lm()\nreplace lm() with lmRob()\ncan’t use any broom functions with lmRob(), so text output using summary()\n\n\nalbum_full_rob &lt;- lmRob(sales ~ adverts + airplay + image, data = album_tib, na.action = na.exclude)\nsummary(album_full_rob)\n\n\nCall:\nlmRob(formula = sales ~ adverts + airplay + image, data = album_tib, \n    na.action = na.exclude)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-124.62  -30.47   -1.76   28.89  141.35 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -31.523201  21.098348  -1.494 0.136756    \nadverts       0.085295   0.008501  10.033  &lt; 2e-16 ***\nairplay       3.418749   0.339017  10.084  &lt; 2e-16 ***\nimage        11.771441   2.973559   3.959 0.000105 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 44.61 on 196 degrees of freedom\nMultiple R-Squared: 0.5785 \n\nTest for Bias:\n            statistic p-value\nM-estimate      2.299  0.6809\nLS-estimate     7.659  0.1049\n\n\n\nbottom of output givessignificance tests of bias\nsignificance tests need to be interpreted within the context of sample size, but these tests suggest that bias in the original model is not problematic\n\n\\(p\\)-value is not significant\n\n\n\n\n\nVariable\nOriginal \\(\\hat{b}\\)\nRobust \\(\\hat{b}\\)\n\n\n\n\nAdverts\n0.085\n0.085\n\n\nAirplay\n3.37\n3.42\n\n\nImage\n11.09\n11.77\n\n\n\n\nestimates are virtually identical to the originals, suggesting the original model is unbiased"
  },
  {
    "objectID": "posts/discovr_08_the_glm/index.html#robust-confidence-intervals-and-significance-tests",
    "href": "posts/discovr_08_the_glm/index.html#robust-confidence-intervals-and-significance-tests",
    "title": "discovr_08 - General Linear Model",
    "section": "Robust Confidence Intervals and significance tests",
    "text": "Robust Confidence Intervals and significance tests\n\nto test whether confidence intervals and significance tests are biased, we can estimate the model with standard errors designed for heteroscedastic residuals\nif the sample is small, use a bootstrap\nboth can be done by placing the model in the parameters::model_parameters() function\nsame function to standardize parameter estimates, but three new arguments\nwe can obtain models based on robust standard errors by setting vcov = \"method\" and replacing method with the name of the method we want to use to compute robust standard errors.\nvcov = \"HC3\" willuse HC3 method, though some think HC4 is better\n\n\nparameters::model_parameters(album_full_lm, vcov = \"HC4\") |&gt; \n  knitr::kable(digits = 3)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nParameter\nCoefficient\nSE\nCI\nCI_low\nCI_high\nt\ndf_error\np\n\n\n\n\n(Intercept)\n-26.613\n16.242\n0.95\n-58.645\n5.419\n-1.638\n196\n0.103\n\n\nadverts\n0.085\n0.007\n0.95\n0.071\n0.098\n12.306\n196\n0.000\n\n\nairplay\n3.367\n0.314\n0.95\n2.749\n3.986\n10.738\n196\n0.000\n\n\nimage\n11.086\n2.260\n0.95\n6.630\n15.543\n4.906\n196\n0.000\n\n\n\n\n\n\nparameters::model_parameters(album_full_lm, vcov = \"HC3\") |&gt; \n  knitr::kable(digits = 3)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nParameter\nCoefficient\nSE\nCI\nCI_low\nCI_high\nt\ndf_error\np\n\n\n\n\n(Intercept)\n-26.613\n16.170\n0.95\n-58.503\n5.277\n-1.646\n196\n0.101\n\n\nadverts\n0.085\n0.007\n0.95\n0.071\n0.099\n12.246\n196\n0.000\n\n\nairplay\n3.367\n0.315\n0.95\n2.746\n3.989\n10.687\n196\n0.000\n\n\nimage\n11.086\n2.247\n0.95\n6.654\n15.519\n4.933\n196\n0.000\n\n\n\n\n\n\nValues are not much different from the non-robust versions, mainly becasue the original model didn’t violate its assumptions\nif a robust model yields the same-ish values as a non-robust model, we know the non-robust model has not been unduly biased.\nif the robust estimates are hugely different, then use and report the robust versions\nfitting a robust model is a win-win\n\n\nSmall samples\n\nwe might prefer to bootstrap confidence intervals by setting the bootstrap argument in model_parameters() to TRUE\nby default, 1000 bootstrap samples are used\n\n\nparameters::model_parameters(album_full_lm, bootstrap = TRUE) |&gt; \n  knitr::kable(digits = 3)\n\n\n\n\nParameter\nCoefficient\nCI\nCI_low\nCI_high\np\n\n\n\n\n(Intercept)\n-26.241\n0.95\n-54.736\n5.396\n0.108\n\n\nadverts\n0.085\n0.95\n0.071\n0.099\n0.000\n\n\nairplay\n3.351\n0.95\n2.807\n3.943\n0.000\n\n\nimage\n11.120\n0.95\n6.632\n15.272\n0.000\n\n\n\n\n\n\nthese bootstrapped confidence intervals don’t rely on assumptions of normality or homoscedasticity, so they give an accurate estimate of the population value of \\(b\\) for each predictor (assuming our sample is one of the 95% with confidence intervals that contain the population value)\n\n\n\n\n\n\n\nBootstrapping\n\n\n\nBecause bootstrapping relies on random sampling from the data, you will get a slightly different estimate each time you bootstrap a model. This is normal.\n\n\n\n\n\n\n\n\nBootstrapping is a technique from which the sampling distribution of a statistic is estimated by …\n\n\n\n\n\nTaking repeated samples (with replacement) from the data set.\n\n\n\n\n\n\n\n\n\nThe bootstrap confidence interval for image ranges from 6.63 to 15.45 (the values might not exactly match these). What does this tell us?\n\n\n\n\n\nIf this confidence interval is one of the 95% that contains the population value then the population value of b lies between 6.63 and 15.45."
  },
  {
    "objectID": "posts/discovr_08_the_glm/index.html#unguided-example-1",
    "href": "posts/discovr_08_the_glm/index.html#unguided-example-1",
    "title": "discovr_08 - General Linear Model",
    "section": "Unguided example",
    "text": "Unguided example\n\nplot(soc_anx_obq_lm, which = c(1, 3))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nplot(soc_anx_obq_lm, which = c(4:6))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsoc_anx_obq_rsd &lt;- soc_anx_obq_lm |&gt; \n  broom::augment(data = soc_anx_tib) |&gt;\n    tibble::rowid_to_column(var = \"case_no\") \n\n\nhead(soc_anx_obq_rsd) |&gt;\n  tibble::glimpse() |&gt;\n    knitr::kable(digits = 2)\n\nRows: 6\nColumns: 11\n$ case_no    &lt;int&gt; 1, 2, 3, 4, 5, 6\n$ spai       &lt;dbl&gt; 26, 51, 33, 106, 25, 109\n$ iii        &lt;dbl&gt; 56.45161, 16.12903, 37.41935, 16.67742, 5.16129, 36.03226\n$ obq        &lt;dbl&gt; 4.425287, 2.034483, 2.586207, 4.839080, 1.758621, 2.126437\n$ tosca      &lt;dbl&gt; 4.181818, 4.204545, 3.250000, 4.068182, 3.795455, 4.250000\n$ .fitted    &lt;dbl&gt; 71.21490, 54.38582, 37.29426, 71.70370, 43.34703, 56.05675\n$ .resid     &lt;dbl&gt; -45.214901, -3.385823, -4.294257, 34.296304, -18.347028, 52…\n$ .hat       &lt;dbl&gt; 0.03290194, 0.01449059, 0.06325813, 0.05273344, 0.02419968,…\n$ .sigma     &lt;dbl&gt; 28.84302, 29.12634, 29.12526, 28.96090, 29.08160, 28.74430\n$ .cooksd    &lt;dbl&gt; 2.847631e-02, 6.772251e-05, 5.263717e-04, 2.737010e-02, 3.3…\n$ .std.resid &lt;dbl&gt; -1.5846267, -0.1175478, -0.1529180, 1.2144842, -0.6401266, …\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncase_no\nspai\niii\nobq\ntosca\n.fitted\n.resid\n.hat\n.sigma\n.cooksd\n.std.resid\n\n\n\n\n1\n26\n56.45\n4.43\n4.18\n71.21\n-45.21\n0.03\n28.84\n0.03\n-1.58\n\n\n2\n51\n16.13\n2.03\n4.20\n54.39\n-3.39\n0.01\n29.13\n0.00\n-0.12\n\n\n3\n33\n37.42\n2.59\n3.25\n37.29\n-4.29\n0.06\n29.13\n0.00\n-0.15\n\n\n4\n106\n16.68\n4.84\n4.07\n71.70\n34.30\n0.05\n28.96\n0.03\n1.21\n\n\n5\n25\n5.16\n1.76\n3.80\n43.35\n-18.35\n0.02\n29.08\n0.00\n-0.64\n\n\n6\n109\n36.03\n2.13\n4.25\n56.06\n52.94\n0.01\n28.74\n0.02\n1.84\n\n\n\n\n\n\nsoc_anx_obq_rsd |&gt; \n  dplyr::filter(abs(.std.resid) &gt;= 1.96) |&gt;\n  dplyr::select(case_no, .std.resid, .resid) |&gt; \n  dplyr::arrange(.std.resid)\n\n# A tibble: 6 × 3\n  case_no .std.resid .resid\n    &lt;int&gt;      &lt;dbl&gt;  &lt;dbl&gt;\n1      49      -2.26  -65.1\n2      16      -2.13  -61.1\n3     127      -1.98  -56.7\n4     120       2.18   62.9\n5       8       2.61   74.6\n6      45       2.64   75.7\n\n\n\nsoc_anx_obq_rsd |&gt; \n  dplyr::filter(abs(.std.resid) &gt;= 2.5) |&gt;\n  dplyr::select(case_no, .std.resid, .resid) |&gt; \n  dplyr::arrange(.std.resid)\n\n# A tibble: 2 × 3\n  case_no .std.resid .resid\n    &lt;int&gt;      &lt;dbl&gt;  &lt;dbl&gt;\n1       8       2.61   74.6\n2      45       2.64   75.7\n\n\n\nsoc_anx_obq_rsd |&gt; \n  dplyr::filter(abs(.std.resid) &gt;= 3.0) |&gt;\n  dplyr::select(case_no, .std.resid, .resid) |&gt; \n  dplyr::arrange(.std.resid)\n\n# A tibble: 0 × 3\n# ℹ 3 variables: case_no &lt;int&gt;, .std.resid &lt;dbl&gt;, .resid &lt;dbl&gt;\n\n\n\nsoc_anx_obq_rsd |&gt; \n  dplyr::arrange(desc(.cooksd)) |&gt;\n  dplyr::select(case_no, .cooksd)\n\n# A tibble: 134 × 2\n   case_no .cooksd\n     &lt;int&gt;   &lt;dbl&gt;\n 1       8  0.0724\n 2      45  0.0497\n 3      14  0.0397\n 4      40  0.0373\n 5      16  0.0330\n 6      87  0.0304\n 7     127  0.0289\n 8       1  0.0285\n 9       4  0.0274\n10      12  0.0268\n# ℹ 124 more rows\n\n\n\n\n\n\n\n\nHow would you interpret the plot of ZPRED vs ZRESID?\n\n\n\n\n\nI can’t see any problems\nThe dots look like a random, evenly-dispersed pattern. No funnel shapes, no banana shapes, so all is fine.\n\n\n\n\n\n\n\n\n\nCan we assume normality of the residuals?\n\n\n\n\n\nYes The distribution is fairly normal: the dots in the P-P plot lie close to the diagonal.\n\n\n\n\n\n\n\n\n\nWhat percentage of cases have standardized residuals with absolute values greater than 1.96?\n\n\n\n\n\n4.55% 6 cases from 132 have standardized residuals with absolute values greater than 1.96, and \\(\\frac{6}{132}×100=4.55\\)\n\n\n\n\n\n\n\n\n\nWhat percentage of cases have standardized residuals with absolute values greater than 2.5?\n\n\n\n\n\n1.51% 2 cases from 132 have standardized residuals with absolute values greater than 2.5, and \\(\\frac{2}{132}×100=1.51\\)\n\n\n\n\n\n\n\n\n\nAll things considered do you think there are outliers?\n\n\n\n\n\nNo The appropriate proportion of cases have standardized residuals in the expected range and no case has a value exceeding 3.\n\n\n\n\n\n\n\n\n\nAre there any Cook’s distances greater than 1?\n\n\n\n\n\nNo The fact there are no Cook’s distances greater than 1 suggests that no cases are having undue influence on the model as a whole."
  },
  {
    "objectID": "posts/discovr_08_the_glm/index.html#bayesian-approaches",
    "href": "posts/discovr_08_the_glm/index.html#bayesian-approaches",
    "title": "discovr_08 - General Linear Model",
    "section": "Bayesian Approaches",
    "text": "Bayesian Approaches\n\n2 possible things to do\n\ncompare models using Bayes factors\nestimate model parameters using Bayesian methods\n\n\n\nBayes factors\n\ncompare models using a Bayes factor using regressionBF() funtion in the BayesFactor package\ncompare models against each other hierarchically to see which model has the largest Bayes factor, and to evaluate the strength of evidence that this Bayes factor suggests that a particular model predicts the outcome better than the intercept alone (i.e. a model with no predictors).\n\nmodel_bf &lt;- BayesFactor::regressionBF(formula, rscaleCont = \"medium\", data = tibble)\n\ncreates an object called model_bf based onthe same type of formula we put in the lm() function to specify the model\nthe argument rscaleCont sets the scale of the prior distribution for the distribution for the standardized \\(\\hat{b}s\\) in the model.\narguemnt can be set as a numeric value or one of three pre-defined values\ndefault value is medium, corresponding to a value of about \\(\\frac{\\sqrt{2}}{4}\\), or about 0.354\nexample uses the default to illustrate, but consider what value is appropriate for a given model\n\nFitting the model with Bayes factors\n\nalbum_bf &lt;- BayesFactor::regressionBF(sales ~ adverts + airplay + image, rscaleCont = \"medium\", data = album_tib)\n\nWarning: data coerced from tibble to data frame\n\nalbum_bf\n\nBayes factor analysis\n--------------\n[1] adverts                   : 1.320123e+16 ±0%\n[2] airplay                   : 4.723817e+17 ±0.01%\n[3] image                     : 6039.289     ±0%\n[4] adverts + airplay         : 5.65038e+39  ±0%\n[5] adverts + image           : 2.65494e+20  ±0%\n[6] airplay + image           : 1.034464e+20 ±0%\n[7] adverts + airplay + image : 7.746101e+42 ±0%\n\nAgainst denominator:\n  Intercept only \n---\nBayes factor type: BFlinearModel, JZS\n\n\n\nOutput shows Bayes factors for all potential models we can get from our predictor variables (7 in total)\neach model is compared to a model that contains only the intercept\nall models have huge Bayes factors suggesting they all provide strong evidence for the hypothesis that the model predicts the outcome better than the intercept alone\nbest model is the one with the largest Bayes factor, which is model 7, which includes all three predictors - it has a Bayes factor of \\(7.75×10^{42}\\)"
  },
  {
    "objectID": "posts/discovr_08_the_glm/index.html#bayesian-parameter-estimates",
    "href": "posts/discovr_08_the_glm/index.html#bayesian-parameter-estimates",
    "title": "discovr_08 - General Linear Model",
    "section": "Bayesian parameter estimates",
    "text": "Bayesian parameter estimates\n\nknowing model is best ( we know this from the nonBayesian model) we can estimate the parameters using the lmBF() function\nsame format as regressionBF()\nlmBF() fits only the model we specify\n\n\nalbum_full_bf &lt;- BayesFactor::lmBF(sales ~ adverts + airplay + image, rscaleCont = \"medium\", data = album_tib)\n\nWarning: data coerced from tibble to data frame\n\nalbum_full_bf\n\nBayes factor analysis\n--------------\n[1] adverts + airplay + image : 7.746101e+42 ±0%\n\nAgainst denominator:\n  Intercept only \n---\nBayes factor type: BFlinearModel, JZS\n\n\n\nsame result in the previous output\nbut we can extract \\(\\hat{b}\\)-values derived from Bayesian estimaten and their credible intervals using the posterior() function\n\nenter the name of the model we just created album_full_bf into ’posterior()` function in which we also set the number of iterations to 10000\nsamples taken from the posterios distribution of the album_full_bf model and stored in an object called album_full_post\nplace posterios samples into summary()\n\n\n\nalbum_full_post &lt;- BayesFactor::posterior(album_full_bf, iterations = 10000)\nsummary(album_full_post) \n\n\nIterations = 1:10000\nThinning interval = 1 \nNumber of chains = 1 \nSample size per chain = 10000 \n\n1. Empirical mean and standard deviation for each variable,\n   plus standard error of the mean:\n\n            Mean        SD  Naive SE Time-series SE\nmu       193.209 3.370e+00 3.370e-02      3.370e-02\nadverts    0.084 6.829e-03 6.829e-05      6.829e-05\nairplay    3.335 2.780e-01 2.780e-03      2.690e-03\nimage     10.945 2.465e+00 2.465e-02      2.465e-02\nsig2    2250.115 2.281e+02 2.281e+00      2.343e+00\ng          1.005 1.762e+00 1.762e-02      1.762e-02\n\n2. Quantiles for each variable:\n\n             2.5%       25%       50%       75%     97.5%\nmu      1.866e+02 1.910e+02  193.1880 1.955e+02 1.998e+02\nadverts 7.068e-02 7.939e-02    0.0839 8.859e-02 9.743e-02\nairplay 2.797e+00 3.150e+00    3.3363 3.522e+00 3.881e+00\nimage   6.225e+00 9.278e+00   10.9286 1.259e+01 1.581e+01\nsig2    1.845e+03 2.090e+03 2236.6918 2.396e+03 2.734e+03\ng       1.722e-01 3.759e-01    0.6027 1.049e+00 4.297e+00\n\n\n\nBayesian estimate of \\(\\hat{b}\\) can be found in the column labeled Mean\n\n0.084 for adverts\n3.34 for airplay\n10.96 for image\n\n\n\n\n\n\n\n\n\n\nPredictor\nBayesian estimate of \\(\\hat{b}\\)\nnon-Bayesian estimate of \\(\\hat{b}\\)\n\n\n\n\nadverts\n0.084\n0.085\n\n\nairplay\n3.34\n3.37\n\n\nimage\n10.96\n11.09\n\n\n\n\nmost useful are the credible intervals for these parameters\n\nif we want the 95% credible interval, then we read the values from columns labelled 2.5% and 97.5% in the table of quantiles\n\n\n\n\n\n\n\n\nImportant\n\n\n\nUnlike confidence intervals, credible intervals contain the population value with a probability of 0.95 (95%)\n\n\n\nthere is a 95% probability that the population value of \\(\\hat{b}\\) lies between | Predictor | 2.5% | 97.5% | | — | — | — | | adverts | 0.07 | 0.097 | | airplay | 2.78 | 3.87 | | image |6.26 | 15.86 |\nthese intervals are constructed assuming that an effect exists, so you cannot use them to test the hypothesis that the null is exactly 0, only to establish plausible population values of the \\(b\\)s in the model."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Stats Blog",
    "section": "",
    "text": "OTESSA24\n\n\n\n\n\n\n\n\n\n\n\nColin Madland1, Valerie Irvine1, Christopher DeLuca2, and Okan Bulut3\n\n\n\n\n\n\n\n\n\n\n\n\nTechnology-Integrated Assessment in BC Higher Education\n\n\n\n\n\n\nBCcampus\n\n\nfellowship\n\n\n\n\n\n\n\n\n\nMay 2, 2024\n\n\nColin Madland\n\n\n\n\n\n\n\n\n\n\n\n\ndsus_18 - Exploratory Factor Analysis\n\n\n\n\n\n\nEFA\n\n\ndsur\n\n\ndsus\n\n\nR\n\n\n\n\n\n\n\n\n\nApr 26, 2024\n\n\nColin Madland\n\n\n\n\n\n\n\n\n\n\n\n\ndiscovr_05 - Visualizing Data\n\n\n\n\n\n\nggplot2\n\n\nvisualizing-data\n\n\nR\n\n\ndiscovr\n\n\n\n\n\n\n\n\n\nApr 23, 2024\n\n\nColin Madland\n\n\n\n\n\n\n\n\n\n\n\n\ndiscovr_02 - Summarizing Data\n\n\n\n\n\n\nfrequency\n\n\nhistograms\n\n\nvariance\n\n\nstandard deviation\n\n\nIQR\n\n\nR\n\n\ndiscovr\n\n\n\n\n\n\n\n\n\nApr 18, 2024\n\n\nColin Madland\n\n\n\n\n\n\n\n\n\n\n\n\ndiscovr_06 - The Beast of Bias\n\n\n\n\n\n\nbias\n\n\nR\n\n\ndiscovr\n\n\n\n\n\n\n\n\n\nApr 16, 2024\n\n\nColin Madland\n\n\n\n\n\n\n\n\n\n\n\n\ndiscovr_07 - Associations\n\n\n\n\n\n\ncorrelation\n\n\nR\n\n\ndiscovr\n\n\n\n\n\n\n\n\n\nMar 10, 2024\n\n\nColin Madland\n\n\n\n\n\n\n\n\n\n\n\n\ndiscovr_18 - Exploratory Factor Analysis\n\n\n\n\n\n\nefa\n\n\nR\n\n\ndiscovr\n\n\n\n\n\n\n\n\n\nFeb 27, 2024\n\n\nColin Madland\n\n\n\n\n\n\n\n\n\n\n\n\ndiscovr_08 - General Linear Model\n\n\n\n\n\n\nregression\n\n\nR\n\n\ndiscovr\n\n\n\n\n\n\n\n\n\nFeb 26, 2024\n\n\nColin Madland\n\n\n\n\n\n\n\n\n\n\n\n\nSimple Regression\n\n\n\n\n\n\n505\n\n\n\n\n\n\n\n\n\nFeb 18, 2024\n\n\nColin Madland\n\n\n\n\n\n\n\n\n\n\n\n\nVariance and Standard Deviation\n\n\n\n\n\n\nintro-stats\n\n\nvariance\n\n\nstandard-deviation\n\n\n\n\n\n\n\n\n\nFeb 18, 2024\n\n\nColin Madland\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nFeb 14, 2024\n\n\nColin Madland\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/deck-otessa24/index.html#literature-review",
    "href": "posts/deck-otessa24/index.html#literature-review",
    "title": "OTESSA24",
    "section": "Literature Review",
    "text": "Literature Review\n\nDesigning assessment in a digital world from Bearman et al. (2022)\nDigital Tools\nDigital Literacies\nHuman Capabilities"
  },
  {
    "objectID": "posts/deck-otessa24/index.html#search",
    "href": "posts/deck-otessa24/index.html#search",
    "title": "OTESSA24",
    "section": "Search",
    "text": "Search\n\nPRISMA Diagram of Search Results"
  },
  {
    "objectID": "posts/deck-otessa24/index.html#references",
    "href": "posts/deck-otessa24/index.html#references",
    "title": "OTESSA24",
    "section": "References",
    "text": "References\n\n\nBearman, M., Nieminen, J., & Ajjawi, R. (2022). Designing assessment in a digital world: An organising framework. Assessment & Evaluation in Higher Education, 48(3). https://doi.org/10.1080/02602938.2022.2069674"
  },
  {
    "objectID": "posts/deck-otessa24/index.html#global-distribution",
    "href": "posts/deck-otessa24/index.html#global-distribution",
    "title": "OTESSA24",
    "section": "Global Distribution",
    "text": "Global Distribution\n\nMap of most common countries of publication."
  },
  {
    "objectID": "posts/deck-otessa24/index.html#predict-the-ranking-of-these-themes",
    "href": "posts/deck-otessa24/index.html#predict-the-ranking-of-these-themes",
    "title": "OTESSA24",
    "section": "Predict the Ranking of these Themes",
    "text": "Predict the Ranking of these Themes\n\nQR Code"
  },
  {
    "objectID": "posts/deck-otessa24/index.html#actual-ranking",
    "href": "posts/deck-otessa24/index.html#actual-ranking",
    "title": "OTESSA24",
    "section": "Actual Ranking",
    "text": "Actual Ranking\n\nNumber of Citations by Theme in the Literature."
  },
  {
    "objectID": "posts/deck-otessa24/index.html#comparing-themes-to-bearman2022",
    "href": "posts/deck-otessa24/index.html#comparing-themes-to-bearman2022",
    "title": "OTESSA24",
    "section": "Comparing Themes to Bearman et al. (2022)",
    "text": "Comparing Themes to Bearman et al. (2022)\n\nColours show different levels of alignment between Bearman et al. (2022) and the themes in the literature."
  },
  {
    "objectID": "posts/deck-otessa24/index.html#section-1",
    "href": "posts/deck-otessa24/index.html#section-1",
    "title": "OTESSA24",
    "section": "…",
    "text": "…"
  },
  {
    "objectID": "posts/deck-otessa24/index.html#section-4",
    "href": "posts/deck-otessa24/index.html#section-4",
    "title": "OTESSA24",
    "section": "",
    "text": "Full Model"
  },
  {
    "objectID": "posts/deck-otessa24/index.html#what-do-you-notice-what-do-you-wonder",
    "href": "posts/deck-otessa24/index.html#what-do-you-notice-what-do-you-wonder",
    "title": "OTESSA24",
    "section": "What do you notice? What do you wonder?",
    "text": "What do you notice? What do you wonder?\n\nQR Code"
  },
  {
    "objectID": "posts/deck-otessa24/index.html#section-2",
    "href": "posts/deck-otessa24/index.html#section-2",
    "title": "OTESSA24",
    "section": "…",
    "text": "…"
  },
  {
    "objectID": "posts/deck-otessa24/index.html#consolidating",
    "href": "posts/deck-otessa24/index.html#consolidating",
    "title": "OTESSA24",
    "section": "Consolidating",
    "text": "Consolidating\n\n\nWorkload\n\n\nEfficiency"
  },
  {
    "objectID": "posts/deck-otessa24/index.html#section-6",
    "href": "posts/deck-otessa24/index.html#section-6",
    "title": "OTESSA24",
    "section": "",
    "text": "Full Model"
  },
  {
    "objectID": "posts/deck-otessa24/index.html#section-5",
    "href": "posts/deck-otessa24/index.html#section-5",
    "title": "OTESSA24",
    "section": "",
    "text": "Full Model"
  }
]