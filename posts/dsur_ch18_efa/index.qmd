---
title: "dsus_18 - Exploratory Factor Analysis"
author: "Colin Madland"
date: "2024-04-26"
categories: [EFA, dsur, dsus, R]
editor: visual
image: "ellie-snow.jpeg"
bibliography: references.bib
---

```{r}
library(tidyverse, knittr)
library(ggplot2, ggfortify, robust)
library(viridis)
raq_tib <- here::here("data/raq.csv") |>
  readr::read_csv()
```

Notes here are from

Field, A. P. (2018). *Discovering statistics using IBM SPSS statistics* (5th edition, North American edition). Sage Publications Inc. -\> `DSuS`

Code examples will be from:

Field, A. (2023). discovr: Interactive tutorials and data for “Discovering Statistics Using R and RStudio” \[Manual\]. -\> `discovr`

R Core Team. (2023). R: A language and environment for statistical computing \[Manual\]. R Foundation for Statistical Computing. https://www.R-project.org/

## When to use factor analysis (p. 571)

-   when attempting to measure latent variables
-   factor analysis and principal component analysis are used to identify clusters of variables
-   3 main uses
    -   understand the structure of a set of variables
    -   to construct a questionnaire to measure an underlying variable
    -   reduce the size of a data set while retaining as much of the original information as possible
        -   (FA can be used to solve the problem of multicollinearity by combining factors that are collinear)

::: callout

`multicollinearity`

:   exists when there is a strong correlation between two factors

:   multicollinearity makes it difficult or impossible to determine the amount of variance accounted for by one of two correlated factors

:   e.g., if a factor predicts the outcome variable with $R=0.80$ and a second variable accounts for *the same variance* (i.e., it is highly correlated to the first variable) then it is only contributing to a very small amount of the unique variance in outcome and we might see $R=0.82$. If the two predictor variables are uncorrelated, then the second variable is contributing more unique variance and we might see $R=0.95$

:   multicollinearity leads to interchangable predictors
:::

### Examples of factor analysis

-   extroversion, introversion, neuroticism traits
-   personality questionnaires
-   in economics to see whether productivity, profits, and workforce contribute to the underlying dimension of company growth

### EFA and PCA

-   they are not the same thing
-   but they both are used to reduce a set of variables into a smaller set of dimensions (factors in EFA, and components in PCA)

## Factors and Components

-   measuring several variables with several questions gives us data that can be arranged in a correlation matrix (R matrix) as below. ![Example of a correlation matrix](cor-matrix.png)

    *Note:* Created with the `ggplot2` R package [@wickham2016] with data from the `discovr` package [@field2023.]

-   factor analysis tries to explain the maximum amount of *common variance* in the matrix using the least number of explanatory constructs (latent variables), which represent clusters of variables that correlate highly with each other.

-   PCA differs in that it tries to explain the maximum amount of *total variance* in a correlation matrix by transforming the original variables into linear components

## Example - Induced Anxiety (`discovr_18` @field2023)

-   questionnaire developed to measure anxiety related to using R
-   questions developed from interviews with anxious and non-anxious students
-   23 questions; 5-point likert (strongly disagree -\> strongly agree)
    -   raq_01: Statistics make me cry
    -   raq_02: My friends will think I’m stupid for not being able to cope with R
    -   raq_03: Standard deviations excite me
    -   raq_04: I dream that Pearson is attacking me with correlation coefficients
    -   raq_05: I don’t understand statistics
    -   raq_06: I have little experience of computers
    -   raq_07: All computers hate me
    -   raq_08: I have never been good at mathematics
    -   raq_09: My friends are better at statistics than me
    -   raq_10: Computers are useful only for playing games
    -   raq_11: I did badly at mathematics at school
    -   raq_12: People try to tell you that R makes statistics easier to understand but it doesn’t
    -   raq_13: I worry that I will cause irreparable damage because of my incompetence with computers
    -   raq_14: Computers have minds of their own and deliberately go wrong whenever I use them
    -   raq_15: Computers are out to get me
    -   raq_16: I weep openly at the mention of central tendency
    -   raq_17: I slip into a coma whenever I see an equation
    -   raq_18: R always crashes when I try to use it
    -   raq_19: Everybody looks at me when I use R
    -   raq_20: I can’t sleep for thoughts of eigenvectors
    -   raq_21: I wake up under my duvet thinking that I am trapped under a normal distribution
    -   raq_22: My friends are better at R than I am
    -   raq_23: If I am good at statistics people will think I am a nerd

```{r}
raq_tib
```

-   24 variables including the ID
-   we don't need the `id` in analyses so create a new tib without it

```{r}
raq_items_tib <- raq_tib |> 
  dplyr::select(-id)
raq_items_tib
```

### Correlation matrix

```{r}
# feed the tibble with only the RAQ items into the correlation() function
correlation::correlation(raq_items_tib)
# pipe into summary() to get a condensed table of correlations:
correlation::correlation(raq_items_tib) |>
  summary() |> 
  knitr::kable(digits = 2)
```

-   because the items use Likert scales, should use `polychoric()` function from the `psych` package

```{r}
raq_poly <- psych::polychoric(raq_items_tib)
raq_poly
```

-   matrix of correlations is stored in in a variable called `rho`, accessible with `raq_poly$rho` but we can store it in an object

```{r}
raq_cor <- raq_poly$rho

```

```{r}
psych::cor.plot(raq_cor, upper = FALSE)

```

-   note items close to 0 - no correlation

-   note items between \$+/-\$0.3

-   note items greater than $+/-$ 0.9 as those may be collinear or singular

-   in this case, all questions correlate reasonably well and none are excessively large