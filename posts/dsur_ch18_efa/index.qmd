---
title: "dsus_18 - Exploratory Factor Analysis"
author: "Colin Madland"
date: "2024-04-26"
categories: [EFA, dsur, dsus, R]
image: "ellie-snow.jpeg"
bibliography: references.bib
---

```{r}
library(tidyverse, knitr)
library(ggplot2, ggfortify, robust)
library(viridis)
library(GPArotation)
```

```{r}
raq_tib <- here::here("data/raq.csv") |>
  readr::read_csv()
```

Notes here are from

Field, A. P. (2018). *Discovering statistics using IBM SPSS statistics* (5th edition, North American edition). Sage Publications Inc. -\> `DSuS`

Code examples will be from:

Field, A. (2023). discovr: Interactive tutorials and data for “Discovering Statistics Using R and RStudio” \[Manual\]. -\> `discovr`

R Core Team. (2023). R: A language and environment for statistical computing \[Manual\]. R Foundation for Statistical Computing. https://www.R-project.org/

## When to use factor analysis (p. 571)

-   when attempting to measure latent variables
-   factor analysis and principal component analysis are used to identify clusters of variables
-   3 main uses
    -   understand the structure of a set of variables
    -   to construct a questionnaire to measure an underlying variable
    -   reduce the size of a data set while retaining as much of the original information as possible
        -   (FA can be used to solve the problem of multicollinearity by combining factors that are collinear)

::: callout

`multicollinearity`

:   exists when there is a strong correlation between two factors

:   multicollinearity makes it difficult or impossible to determine the amount of variance accounted for by one of two correlated factors

:   e.g., if a factor predicts the outcome variable with $R=0.80$ and a second variable accounts for *the same variance* (i.e., it is highly correlated to the first variable) then it is only contributing to a very small amount of the unique variance in outcome and we might see $R=0.82$. If the two predictor variables are uncorrelated, then the second variable is contributing more unique variance and we might see $R=0.95$

:   multicollinearity leads to interchangable predictors
:::

### Examples of factor analysis

-   extroversion, introversion, neuroticism traits
-   personality questionnaires
-   in economics to see whether productivity, profits, and workforce contribute to the underlying dimension of company growth

### EFA and PCA

-   they are not the same thing
-   but they both are used to reduce a set of variables into a smaller set of dimensions (factors in EFA, and components in PCA)

## Factors and Components

-   measuring several variables with several questions gives us data that can be arranged in a correlation matrix (R matrix) as below. ![Example of a correlation matrix](cor-matrix.png)

    *Note:* Created with the `ggplot2` R package [@wickham2016] with data from the `discovr` package [@field2023.]

-   factor analysis tries to explain the maximum amount of *common variance* in the matrix using the least number of explanatory constructs (latent variables), which represent clusters of variables that correlate highly with each other.

-   PCA differs in that it tries to explain the maximum amount of *total variance* in a correlation matrix by transforming the original variables into linear components

## Example - Induced Anxiety (`discovr_18` @field2023)

-   questionnaire developed to measure anxiety related to using R
-   questions developed from interviews with anxious and non-anxious students
-   23 questions; 5-point likert (strongly disagree -\> strongly agree)
    -   raq_01: Statistics make me cry
    -   raq_02: My friends will think I’m stupid for not being able to cope with R
    -   raq_03: Standard deviations excite me
    -   raq_04: I dream that Pearson is attacking me with correlation coefficients
    -   raq_05: I don’t understand statistics
    -   raq_06: I have little experience of computers
    -   raq_07: All computers hate me
    -   raq_08: I have never been good at mathematics
    -   raq_09: My friends are better at statistics than me
    -   raq_10: Computers are useful only for playing games
    -   raq_11: I did badly at mathematics at school
    -   raq_12: People try to tell you that R makes statistics easier to understand but it doesn’t
    -   raq_13: I worry that I will cause irreparable damage because of my incompetence with computers
    -   raq_14: Computers have minds of their own and deliberately go wrong whenever I use them
    -   raq_15: Computers are out to get me
    -   raq_16: I weep openly at the mention of central tendency
    -   raq_17: I slip into a coma whenever I see an equation
    -   raq_18: R always crashes when I try to use it
    -   raq_19: Everybody looks at me when I use R
    -   raq_20: I can’t sleep for thoughts of eigenvectors
    -   raq_21: I wake up under my duvet thinking that I am trapped under a normal distribution
    -   raq_22: My friends are better at R than I am
    -   raq_23: If I am good at statistics people will think I am a nerd

```{r}
raq_tib
```

-   24 variables including the ID
-   we don't need the `id` in analyses so create a new tib without it

```{r}
raq_items_tib <- raq_tib |> 
  dplyr::select(-id)
raq_items_tib
```

### Correlation matrix

```{r}
# feed the tibble with only the RAQ items into the correlation() function
correlation::correlation(raq_items_tib)
# pipe into summary() to get a condensed table of correlations:
correlation::correlation(raq_items_tib) |>
  summary() |> 
  knitr::kable(digits = 2)
```

-   because the items use Likert scales, should use `polychoric()` function from the `psych` package

```{r}
raq_poly <- psych::polychoric(raq_items_tib)
raq_poly
```

-   matrix of correlations is stored in in a variable called `rho`, accessible with `raq_poly$rho` but we can store it in an object

```{r}
raq_cor <- raq_poly$rho

```

```{r}
psych::cor.plot(raq_cor, upper = FALSE)

```

-   note items close to 0 - no correlation

-   note items between \$+/-\$0.3

-   note items greater than $+/-$ 0.9 as those may be collinear or singular

-   in this case, all questions correlate reasonably well and none are excessively large

## Bartlett's test and KMO test

### Bartlett's test of Sphericity

-   tests whether the correlation matrix is significantly different from an identity matrix (whether the correlations are all 0

    -   in FA, sample sizes are large, so the test will almost always be significant, but if it is not, then there is a problem

```{r}

psych::cortest.bartlett(raq_cor, n = 2571)
```

-   given large sample size, Bartlett's test is highly significant, indicating there is not a significant problem

### Kaiser-Meyer-Olkin (KMO)

-   check for sampling adequacy

-   KMO varies between 0 and 1 with 0 indicating FA is not appropriate

-   values closer to 1 indicate compact patterns of correlations and FA should reveal distinct and reliable factors

    -   Marvellous: values in the 0.90s

    -   Meritorious: values in the 0.80s

    -   Middling: values in the 0.70s

    -   Mediocre: values in the 0.60s

    -   Miserable: values in the 0.50s

```{r}
psych::KMO(raq_cor)

```

-   KMO statistic (Overall MSA) is 0.92 - well above the threshold of 0.5

-   MSA for each individual item ranges from 0.84 - 0.96

::: callout-note
If you find KMO values below 0.5, consider removing that variable, but be sure to run the KMO statistic again without the removed variable. Also run the analysis with and without the variable to compare
:::

## Parallel analysis

-   to determine how many factors to extract, run `psych::fa.parallel()`

-   most likely arguments

    -   `` n.obs() - need to tell the function the sample size (`n.obs = 2571`) ``

    -   `fm = “minres”` - `psych` packages uses minimum residual (minres) by default

        -   other options include principal axes (`pa`), alpha factoring (`alpha`), weighted least squares `wls`, minimum rank (`minrank`), or maximum likelihood (`ml`). Match this option to the one you’re going to use in the main factor analysis

    -   `fa = "both"` - by default the function will tell the number of factors to extract, but also the number of components for PCA.

        -   can change to `fa = "fa`" to see only the number of factors to extract. It is useful to look at both methods

    -   `use = “pairwise”` - by default, missing data are handled using all complete pairwise observations to calculate the correlation coefficients

    -   `cor` - default is the function assumes you are providing Pearson correlation coefficients, however, with ordinal variables (likert), use `cor = "poly"` (polychoric) and for binary, use tetrachoric `cor = "tet"`; with a mix of variable types, use `cor ="mixed"`

### Code example

```{r}
psych::fa.parallel(raq_items_tib, cor = "poly")

```

-   or since we already stored polychoric correlations in `raq_cor`, we can just apply the function to that correlation matrix and specify the sample size

```{r}
psych::fa.parallel(raq_cor, n.obs = 2571, fa = "fa")

```

-   eigenvalues represent the size of the factor

-   factors are plotted on the x-axis with the eigenvalues on the y-axis

-   each eigenvalue is compared to an eigenvalue from a simulated data set that has no underlying factors

    -   essentially, we are asking if the factors are bigger than imaginary factor

    -   factors that are bigger than their imaginary counterparts are retained

-   eigenvalues for the observed factors are blue triangles connected by a blue line.

-   the red line shows corresponding simulated data.

-   we keep the number of factors that are above the red line, in this case, four

### Quiz

::: {.callout-important collapse="TRUE"}
## **Based on the parallel analysis that used principal components to compute the eiegenvalues, how many factors should be extracted?**

-   4

Yes, fortunately this analysis agrees with the parallel analysis based on eigenvalues from factor analysis.
:::

## Factor Analysis

-   we are now ready to run the FA, extracting four factors, using the `psych::fa()` function

```         
my_fa_object <- psych::fa(r,    
                          n.obs = 2571,  
                          nfactors = 1,  
                          fm = "minres", 
                          rotate = "oblimin", 
                          scores = "regression", 
                          max.iter = 50, 
                          use = "pairwise", 
                          cor = "cor" 
                          )
```

-   `r` -\> the data being fed into the function (`raq_items_tib` or `raq_cor`)
-   `n.obs = 2571` as with parallel analysis, if we run the FA from the correlation matrix instead of the raw data, we must tell the function the sample size
-   `nfactors = 1` the number of factors to extract (default is 1)
-   `fm = "minres"` method of factor analysis, typically leave the default
-   `rotate = "oblimin"` method of factor rotation
-   `scores = "regression"` method of computing factor scores. because we should use oblique rotation, change this argument to `scores = "tenBerge"`
-   `max.iter = 50` number of iterations. If you get a n error message about convergence, increase this number
-   `use = "pairwise"` determines how missing values are treated, default is fine
-   `cor = "cor"` same as defined for `fa.parallel()`

::: callout-tip
## Factor rotation

factor rotation requires `GPArotation` package loaded
:::

### Code Example

-   as with parallel analysis we can either feed the raw data into the function remembering to set `cor = "poly"` so the analysis is based on polychoric correlations
-   or we can feed in the correlation matrix and sample size

```{r}
raq_fa <- psych::fa(raq_items_tib, 
  nfactors = 4, 
  scores = "tenBerge", 
  cor = "poly"
  )
  raq_fa 
```

-   factors are labeled `MR1`, `MR2`, `MR3`, and `MR4`
-   below the pattern matrix is information about how much variance each factor accounts for
    -   `Proportion var` - `MR1` accounts for 0.13 of the overall variance (13%), etc
    -   `Cumulative var` is the proportion of the variance explained cumulatively by the factors - `MR1` accounts for 0.13 and `MR1` + `MR2` together account for 0.13 + 0.10 = 0.23 (23%)
        -   all four together account for 0.40 (40%)
    -   `Proportion explained` is the proportion of the explained variance that is explained by a factor, so of the 40% of the variiance accounted for, 0.33 (33%) is attributable to `MR1`
-   next, correlations between factors are displayed
    -   all are non-zero, indicating the factors are correlated (and oblique rotation was appropriate)
    -   all factors are positively and fairly strongly correlated to teach other meaning the latent constructs represented by the factors are related.
-   several fit indices that tell us how the model fits the data
    -   chi-square statistic for the model is given as the `likelihood chi square`, $X^2=267.21, p<0.001$
        -   we want this to be non-significant but ours is highly significant. Our sample size is 2571 so small deviations from a good fit will be significant. This highlights the limitation of using significance to indicate model fit.
        -   the Tucker Lewis Index of factoring reliability (TLI) is given as 0.991
        -   RMSEA is 0.015 90% CI\[0.012, 0.019\]
        -   RMSR is 0.01

::: callout-note
## Fit Indices

Good fit is (probably) indicated by

-   combination of TLI \> 0.96, and SRMR (RMSR in the output) \< 0.06
-   combination of RMSEA \< 0.05 and SRMR \< 0.09

The TLI is 0.99, which is greater than 0.96, and RMSR is 0.01 which is smaller than both 0.09 and 0.06. Furthermore, RMSEA is 0.015, which is less than 0.05. With the caveat that universal cut-offs need to be taken with a pinch of salt, it’s reasonable to conclude that the model has excellent fit.
:::

### Interpreting FA

-   look at factor loadings (top of output in the pattern matrix) for each question on each factor to see which items load most heavily onto which factors
-   this is difficult to interpret in the raw form, so use `parameters::model_parameters()` to sort items by their factor loadings and suppress factor loading sbelow a certain value

General form

```         
parameters::model_parameters(my_fa_object, sort = TRUE, threshold = "max")
```

-   `my_fa_object` is the factor analysis object containing the factor loadings.
-   `sort = "TRUE"` sorts items by their factor loadings
-   set `threshold` to a value above which we will show values. Default is maximum loading. To see all factor loadings, set `threshold = NULL`

```{r}
parameters::model_parameters(raq_fa, sort = TRUE, threshold = "0.2") |>
  knitr::kable(digits = 2)
```

-   Now we can see patterns in the questions that load onto the same factors
-   Items that load highly on `MR1` seem to be items that relate to fear of computers
    -   raq_05: I don’t understand statistics (also loads highly onto `MR4`)
    -   raq_06: I have little experience of computers
    -   raq_07: All computers hate me
    -   raq_10: Computers are useful only for playing games
    -   raq_13: I worry that I will cause irreparable damage because of my incompetence with computers
    -   raq_14: Computers have minds of their own and deliberately go wrong whenever I use them
    -   raq_15: Computers are out to get me
    -   raq_18: R always crashes when I try to use it
-   Items that load onto `MR2` relate to fear of peer/social evaluation
    -   raq_02: My friends will think I’m stupid for not being able to cope with R
    -   raq_09: My friends are better at statistics than me
    -   raq_19: Everybody looks at me when I use R
    -   raq_22: My friends are better at R than I am
    -   raq_23: If I am good at statistics people will think I am a nerd
-   Questions that load onto `MR4` relate to fear of statistics
    -   raq_01: Statistics make me cry
    -   raq_03: Standard deviations excite me
    -   raq_04: I dream that Pearson is attacking me with correlation coefficients
    -   raq_05: I don’t understand statistics
    -   raq_12: People try to tell you that R makes statistics easier to understand but it doesn’t
    -   raq_16: I weep openly at the mention of central tendency
    -   raq_20: I can’t sleep for thoughts of eigenvectors
    -   raq_21: I wake up under my duvet thinking that I am trapped under a normal distribution
-   questions that load onto `MR3` relate to fear of math
    -   raq_08: I have never been good at mathematics
    -   raq_11: I did badly at mathematics at school
    -   raq_17: I slip into a coma whenever I see an equation

Analysis seems to reveal that the questionnaire is composed of four subscales: fear of statistics, fear of computers, fear of maths, fear of negative peer evaluation.

-   two possibilities:
    -   the RAQ failed to measure what it set out to measure -\> R anxiety but instead measures related constructs
    -   these four constructs are subcomponents of R anxiety
-   However, the factor analysis does not indicate which of these is true.

## Reliability Analysis

### McDonald's $\omega_t$ and $\omega_h$

-   if items are all scored in the same direction, we can select the variables on a particular subscale and pipe them into `omega()` functions in the `psych` package

```         
my_omg <- psych::omega(
    my_tibble
    nfactors = 1,
    fm = "minres",
    key = c(1, 1, -1, 1, 1 … 1),
    rotate = "oblimin",
    poly = FALSE
  )
```

`my_omg`

:   name to give the object that stores the results

`my_tibble`

:   name of the tibble containing your data

### Code Example

-   need to recreate the original FA
-   arguments will be the same as they were set for the fa
-   the `key` argument allows us to reverse item scoring on the fly
-   supply the key argument with a vector of 1s and -1s that is the same length as the number of variables being fed into the `omega()` function
-   for RAQ, we have 23 items with the third being reverse coded
-   entering the items in order into `omega()` gives

```         
key = c(1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
```

or

```         
key = c(1, 1, -1, rep(1, 20))
```

```{r}
raq_omg <- psych::omega(raq_items_tib,
                        nfactors = 4,
                        fm = "minres",
                        key = c(1, 1, -1, rep(1, 20)),
                        poly = TRUE
                        )
raq_omg
```

-   note the table of factor loadings and that `raq_03-` is labeled with a minus to indicate it is reverse coded

-   column labeled `g`shows the loading of each item on the general factor

    -   if any items load poorly, then a common factor model isn't appropriate
    -   all items here have a factor loading far enough from zero that a general factor model is appropriate

-   Columns F1-F4 show the loadings of each item on the four factors we extracted

    -   these values differ from the original factor analysis because this one includes a general factor
    -   the patterns of item loadings to factors follow the same pattern as the original FA

-   at the top of the text output are reliability estimates for the general factor, including

    -   Chronbach's $\alpha=0.88$
    -   $\omega_h=0.68$
    -   $\omega_t=0.9$

-   $\omega_h$ is a measure of how much the items reflect a single construct, and a value of 0.68 suggests they do but there is still a lot of unexplained variance in the general factor

-   further down in the output, we see that only 48% of the variance in the common factor is explained by the items

-   $\omega_t$ is the total reliability and 0.90 is high, suggesting scores are reliable

-   next are two sets of model fit stats

-   the first set are for the model that has four factors and repeat the nformation from the original EFA

    -   `chi-square` is significant, $X^2= 267.21, p<0.001$, which is a bad thing but unsurprising given the sample size
    -   `RMSR = 0.01`
    -   `RMSEA = 0.02 90% CI[0.01-0.02]`

-   Next is the same information but for the model that contains only the general factor and not the four sub-factors

    -   the fit gets worse as shown by
        -   larger and more significant `chi-square, $X^2=6122.42, p<0.001$`
        -   larger `RMSR=0.10`
        -   larger `RMSEA=0.1090% CI[0.10, 0.10]`

-   all this tells us that the model that characterizes the RAQ in terms of four factors is a better fit of the data than a model that characterises it as a single factor

-   finally, under the Total, General, and Subset omega for each subset, we get the $\omega_t$ (labeled omega total) and $\omega_h$ (labeled omega general) for the general factor (column `g`) and for the subfactors.

-   values for the general factor repeat the information at tht e start of the output

-   the values for the subfactors are particularly relvant for $\omega_t$ because it represents the total reliability of the scores, so these values tell us the total reliability of the scores from the underlying subscales

    -   for anxiety related to computers (F1) we have $\omega_t=0.83$
    -   anxiety around peer or social evaluation (F2) \$\omega\_t=0.80
    -   anxiety around statistics (F3) $\omega_t=0.69$
    -   anxiety around maths (F4) $\omega_t=0.81$

-   scores from ach subscale are reliable, although somewhat less so for anxiety around statistics

## Cronbach's $\alpha$

-   Lots of reasons not to use Cronbach's $\alpha$ (see the book for details)
-   if Cronbach's $\alpha$ is needed, it must be computed on the individual subscales
    -   Anxiety related to computers: raq_06, raq_07, raq_10, raq_13, raq_14, raq_15 and raq_18
    -   Anxiety around peer or social evaluation: raq_02, raq_09, raq_19, raq_22, and raq_2
    -   Anxiety around statistics: raq_01, raq_03 (reverse scored), raq_04, raq_05, raq_12, raq_16, raq_20, and raq_21
    -   Anxiety around maths: raq_08, raq_11 and raq_17
-   pipe the variables for each subscale into `psych::alpha()` function

### Code example for `fear of computers`

-   example excludes `raq_05`, which makes more sense on the fear of statistics factor \[[See github issue](https://github.com/profandyfield/discovr/issues/19)\]

```{r}
raq_tib |> 
  dplyr::select(raq_06, raq_07, raq_10, raq_13, raq_14, raq_15, raq_18) |> 
  psych::alpha()
```

-   value at the top is Cronbach's $\alpha$, with the 95% CI below.

-   looking for between 0.70 and 0.80, in this case Cronbach's $\alpha$=0.77 \[0.75, 0.78\] indicating good reliability

-   next is a table of statistics for the scale if we deleted each item in turn

-   the values in the column `raw_alpha` are the values of the overall $\alpha$

-   we are looking for a change in Cronbach's $\alpha$ (0.77)

    -   if values are greater than 0.77, then reliability would have improved if the item were removed - not the case here

-   the table labeled `item statistics` shows, in `raw.r`, the correlations between each item and the total score from the scale - `item-total correlations`

    -   there is a problem with this statistic in that the item is included in the scale total, which inflates the overall correlation.
    -   we want these correlations to be computed without the item in question, and these values are in `r.drop`
    -   in a reliable scale all items should correlate with the total, so we're looking for items that don't correlate with the overall score from the subscale. If any values of `r.drop` are elss than 0.3, we have problems becasuse that means an item does not correlate well with the subscale
        -   0.3 is 'reasonable' - use your judgement

-   the final table tells us what percentage of people gave each response to each of the items, which is useful to make sure everyone in the sample is not giving the same response.

    -   usually, if everyone gives the same response, the item will have poor reliability statistics
    -   for this subscale, few people responded with a 1 on any of the items suggesting that no one is feeling the love for computers or that the items are doing a poor job of eliciting those extreme responses

### Code example for `fear of peer/social evaluation`

```{r}
raq_tib |> 
  dplyr::select(raq_02, raq_09, raq_19, raq_22, raq_23)  |> 
  psych::alpha()
```

-   good overall reliability ($\alpha=0.78 [o.77, 0.80]$)
-   no items improve the value if they are dropped
-   item correlations with the total are all good
-   again we have issues with the items not eliciting extremely low responses

### Code example for `fear of maths`

```{r}
raq_tib |> 
  dplyr::select(raq_08, raq_11, raq_17)  |> 
  psych::alpha()
```

-   fairly high reliability ($\alpha=0.77 [0.76, 0.79]$)
-   no items improve value if they are dropped
-   item correlations with the total subscale are all good
-   still issues with items not eliciting low responses

### Code example for `fear of statistics`

-   the `fear of statistics` subscale contains `raq_3`, which is reverse scored so we need to include the `keys` argument within the function
-   note that in the `omega()` function, the argument is `key`, but in the `alpha()` function, the argument is `keys`
-   

```{r}
raq_tib |> 
  dplyr::select(raq_01, raq_03, raq_04, raq_05, raq_12, raq_16, raq_20, raq_21)  |> 
  psych::alpha(keys = c(1, -1, 1, 1, 1, 1, 1, 1))
```

-   note that `raq_03-` has a minus to indicate reverse scoring
-   acceptable overall reliability ($\alpha=0.71 [0.69, 0.72]$)
-   no items improve the value if they are dropped
-   item correlations with the total subscale are ok (0.27-0.49)
-   issue with items not eliciting extreme responses