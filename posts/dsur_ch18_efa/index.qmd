---
title: "dsus_18 - Exploratory Factor Analysis"
author: "Colin Madland"
date: "2024-04-26"
categories: [EFA, dsur, dsus, R]
editor: visual
image: "ellie-snow.jpeg"
bibliography: references.bib
---

```{r}
library(tidyverse, knittr)
library(ggplot2, ggfortify, robust)
library(viridis)
raq_tib <- here::here("data/raq.csv") |>
  readr::read_csv()
```

Notes here are from

Field, A. P. (2018). *Discovering statistics using IBM SPSS statistics* (5th edition, North American edition). Sage Publications Inc. -\> `DSuS`

Code examples will be from:

Field, A. (2023). discovr: Interactive tutorials and data for “Discovering Statistics Using R and RStudio” \[Manual\]. -\> `discovr`

R Core Team. (2023). R: A language and environment for statistical computing \[Manual\]. R Foundation for Statistical Computing. https://www.R-project.org/

## When to use factor analysis (p. 571)

-   when attempting to measure latent variables
-   factor analysis and principal component analysis are used to identify clusters of variables
-   3 main uses
    -   understand the structure of a set of variables
    -   to construct a questionnaire to measure an underlying variable
    -   reduce the size of a data set while retaining as much of the original information as possible
        -   (FA can be used to solve the problem of multicollinearity by combining factors that are collinear)

::: callout

`multicollinearity`

:   exists when there is a strong correlation between two factors

:   multicollinearity makes it difficult or impossible to determine the amount of variance accounted for by one of two correlated factors

:   e.g., if a factor predicts the outcome variable with $R=0.80$ and a second variable accounts for *the same variance* (i.e., it is highly correlated to the first variable) then it is only contributing to a very small amount of the unique variance in outcome and we might see $R=0.82$. If the two predictor variables are uncorrelated, then the second variable is contributing more unique variance and we might see $R=0.95$

:   multicollinearity leads to interchangable predictors
:::

### Examples of factor analysis

-   extroversion, introversion, neuroticism traits
-   personality questionnaires
-   in economics to see whether productivity, profits, and workforce contribute to the underlying dimension of company growth

### EFA and PCA

-   they are not the same thing
-   but they both are used to reduce a set of variables into a smaller set of dimensions (factors in EFA, and components in PCA)

## Factors and Components

-   measuring several variables with several questions gives us data that can be arranged in a correlation matrix (R matrix) as below. ![Example of a correlation matrix](cor-matrix.png)

    *Note:* Created with the `ggplot2` R package [@wickham2016] with data from the `discovr` package [@field2023.]

-   factor analysis tries to explain the maximum amount of *common variance* in the matrix using the least number of explanatory constructs (latent variables), which represent clusters of variables that correlate highly with each other.

-   PCA differs in that it tries to explain the maximum amount of *total variance* in a correlation matrix by transforming the original variables into linear components

## Example - Induced Anxiety (`discovr_18` @field2023)

-   questionnaire developed to measure anxiety related to using R
-   questions developed from interviews with anxious and non-anxious students
-   23 questions; 5-point likert (strongly disagree -\> strongly agree)
    -   raq_01: Statistics make me cry
    -   raq_02: My friends will think I’m stupid for not being able to cope with R
    -   raq_03: Standard deviations excite me
    -   raq_04: I dream that Pearson is attacking me with correlation coefficients
    -   raq_05: I don’t understand statistics
    -   raq_06: I have little experience of computers
    -   raq_07: All computers hate me
    -   raq_08: I have never been good at mathematics
    -   raq_09: My friends are better at statistics than me
    -   raq_10: Computers are useful only for playing games
    -   raq_11: I did badly at mathematics at school
    -   raq_12: People try to tell you that R makes statistics easier to understand but it doesn’t
    -   raq_13: I worry that I will cause irreparable damage because of my incompetence with computers
    -   raq_14: Computers have minds of their own and deliberately go wrong whenever I use them
    -   raq_15: Computers are out to get me
    -   raq_16: I weep openly at the mention of central tendency
    -   raq_17: I slip into a coma whenever I see an equation
    -   raq_18: R always crashes when I try to use it
    -   raq_19: Everybody looks at me when I use R
    -   raq_20: I can’t sleep for thoughts of eigenvectors
    -   raq_21: I wake up under my duvet thinking that I am trapped under a normal distribution
    -   raq_22: My friends are better at R than I am
    -   raq_23: If I am good at statistics people will think I am a nerd

```{r}
raq_tib
```

-   24 variables including the ID
-   we don't need the `id` in analyses so create a new tib without it

```{r}
raq_items_tib <- raq_tib |> 
  dplyr::select(-id)
raq_items_tib
```

### Correlation matrix

```{r}
# feed the tibble with only the RAQ items into the correlation() function
correlation::correlation(raq_items_tib)
# pipe into summary() to get a condensed table of correlations:
correlation::correlation(raq_items_tib) |>
  summary() |> 
  knitr::kable(digits = 2)
```

-   because the items use Likert scales, should use `polychoric()` function from the `psych` package

```{r}
raq_poly <- psych::polychoric(raq_items_tib)
raq_poly
```

-   matrix of correlations is stored in in a variable called `rho`, accessible with `raq_poly$rho` but we can store it in an object

```{r}
raq_cor <- raq_poly$rho

```

```{r}
psych::cor.plot(raq_cor, upper = FALSE)

```

-   note items close to 0 - no correlation

-   note items between \$+/-\$0.3

-   note items greater than $+/-$ 0.9 as those may be collinear or singular

-   in this case, all questions correlate reasonably well and none are excessively large

## Bartlett's test and KMO test

### Bartlett's test of Sphericity

-   tests whether the correlation matrix is significantly different from an identity matrix (whether the correlations are all 0

    -   in FA, sample sizes are large, so the test will almost always be significant, but if it is not, then there is a problem

```{r}

psych::cortest.bartlett(raq_cor, n = 2571)
```

-   given large sample size, Bartlett's test is highly significant, indicating there is not a significant problem

### Kaiser-Meyer-Olkin (KMO)

-   check for sampling adequacy

-   KMO varies between 0 and 1 with 0 indicating FA is not appropriate

-   values closer to 1 indicate compact patterns of correlations and FA should reveal distinct and reliable factors

    -   Marvellous: values in the 0.90s

    -   Meritorious: values in the 0.80s

    -   Middling: values in the 0.70s

    -   Mediocre: values in the 0.60s

    -   Miserable: values in the 0.50s

```{r}
psych::KMO(raq_cor)

```

-   KMO statistic (Overall MSA) is 0.92 - well above the threshold of 0.5

-   MSA for each individual item ranges from 0.84 - 0.96

::: callout-note
If you find KMO values below 0.5, consider removing that variable, but be sure to run the KMO statistic again without the removed variable. Also run the analysis with and without the variable to compare
:::

## Parallel analysis

-   to determine how many factors to extract, run `psych::fa.parallel()`

-   most likely arguments

    -   `` n.obs() - need to tell the function the sample size (`n.obs = 2571`) ``

    -   `fm = “minres”` - `psych` packages uses minimum residual (minres) by default

        -   other options include principal axes (`pa`), alpha factoring (`alpha`), weighted least squares `wls`, minimum rank (`minrank`), or maximum likelihood (`ml`). Match this option to the one you’re going to use in the main factor analysis

    -   `fa = "both"` - by default the function will tell the number of factors to extract, but also the number of components for PCA.

        -   can change to `fa = "fa`" to see only the number of factors to extract. It is useful to look at both methods

    -   `use = “pairwise”` - by default, missing data are handled using all complete pairwise observations to calculate the correlation coefficients

    -   `cor` - default is the function assumes you are providing Pearson correlation coefficients, however, with ordinal variables (likert), use `cor = "poly"` (polychoric) and for binary, use tetrachoric `cor = "tet"`; with a mix of variable types, use `cor ="mixed"`

### Code example

```{r}
psych::fa.parallel(raq_items_tib, cor = "poly")

```

-   or since we already stored polychoric correlations in `raq_cor`, we can just apply the function to that correlation matrix and specify the sample size

```{r}
psych::fa.parallel(raq_cor, n.obs = 2571, fa = "fa")

```